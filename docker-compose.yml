networks:
  home:
    name: home
    driver: bridge

services:

  # Movie manager
  radarr:
    image: linuxserver/radarr:nightly-version-5.9.0.9004
    container_name: radarr
    hostname: radarr
    depends_on:
      qbittorrent:
        condition: service_healthy
    deploy:
      resources:
        limits:
          cpus: "1"
          memory: "1024M"
    environment:
      # Base config
      TZ: "${TIMEZONE:?Timezone missing}"
    healthcheck:
      interval: 30s
      retries: 3
      start_period: 30s
      test: [ "CMD", "wget", "--no-verbose", "--tries=1", "-O", "-", "http://127.0.0.1:3000/ping" ]
      timeout: 5s
    labels:
      # Uptime-Kuma monitor config
      kuma.{{container_name}}.http.name: "Radarr (Movie Manager)"
      kuma.{{container_name}}.http.url: "${RADARR_MONITOR_URL:?[radarr] Monitor URL missing}"
    networks:
      - home
    ports:
      - "4000:3000"
    restart: unless-stopped
    user: "${PUID_ROOT:?User ID missing}:${PGID_ROOT:?Group ID missing}"
    volumes:
      # Volume mounts from host system
      - "${DOWNLOADS_DIRECTORY}:/downloads"
      - "${MOVIE_DIRECTORY}:/movies"
      # Persistent volumes
      - ./storage/radarr/:/config

  # TV show manager
  sonarr:
    image: linuxserver/sonarr:develop-version-4.0.8.2008
    container_name: sonarr
    hostname: sonarr
    depends_on:
      qbittorrent:
        condition: service_healthy
    deploy:
      resources:
        limits:
          cpus: "2"
          memory: "2048M"
    environment:
      # Base config
      TZ: "${TIMEZONE:?Timezone missing}"
    healthcheck:
      interval: 30s
      retries: 3
      start_period: 30s
      test: [ "CMD", "wget", "--no-verbose", "--tries=1", "-O", "-", "http://127.0.0.1:3001/ping" ]
      timeout: 5s
    labels:
      # Uptime-Kuma monitor config
      kuma.{{container_name}}.http.name: "Sonarr (TV Manager)"
      kuma.{{container_name}}.http.url: "${SONARR_MONITOR_URL:?[sonarr] Monitor URL missing}"
    networks:
      - home
    ports:
      - "4001:3001"
    restart: unless-stopped
    user: "${PUID_ROOT:?User ID missing}:${PGID_ROOT:?Group ID missing}"
    volumes:
      # Volume mounts from host system
      - "${DOWNLOADS_DIRECTORY}:/downloads"
      - "${TV1_DIRECTORY}:/tv1"
      - "${TV2_DIRECTORY}:/tv2"
      - "${TV_TRASH_DIRECTORY}:/trash"
      - "${TV_KIDS_DIRECTORY}:/kids"
      # Persistent volumes
      - ./storage/sonarr/:/config

  # Quality profile manager for radarr/sonarr
  recyclarr:
    image: ghcr.io/recyclarr/recyclarr:7.2.0 # https://github.com/recyclarr/recyclarr/pkgs/container/recyclarr
    container_name: recyclarr
    hostname: recyclarr
    depends_on:
      radarr:
        condition: service_healthy
      sonarr:
        condition: service_healthy
    deploy:
      resources:
        limits:
          cpus: "1"
          memory: "1024M"
    environment:
      # Base config
      TZ: "${TIMEZONE:?Timezone missing}"
      # Scheduling config
      CRON_SCHEDULE: "@daily"
      # Radarr Config
      RADARR_API_KEY: "${RADARR_API_KEY}"
      RADARR_URL: "http://radarr:3000"
      # Sonarr Config
      SONARR_API_KEY: "${SONARR_API_KEY}"
      SONARR_URL: "http://sonarr:3001"
    healthcheck:
      interval: 30s
      retries: 3
      start_period: 30s
      test: [ "CMD", "recyclarr", "-v" ]
      timeout: 5s
    networks:
      - home
    restart: unless-stopped
    user: "${PUID_ROOT:?User ID missing}:${PGID_ROOT:?Group ID missing}"
    volumes:
      - ./storage/recyclarr:/config
      - ./docker/recyclarr/config/recyclarr.yml:/config/recyclarr.yml

  # Music manager
  lidarr:
    image: linuxserver/lidarr:nightly-version-2.5.1.4285
    container_name: lidarr
    hostname: lidarr
    depends_on:
      qbittorrent:
        condition: service_healthy
    deploy:
      resources:
        limits:
          cpus: "1"
          memory: "1024M"
    environment:
      # Base config
      TZ: "${TIMEZONE:?Timezone missing}"
    healthcheck:
      interval: 30s
      retries: 3
      start_period: 30s
      test: [ "CMD", "wget", "--no-verbose", "--tries=1", "-O", "-", "http://127.0.0.1:3002/ping" ]
      timeout: 5s
    labels:
      # Uptime-Kuma monitor config
      kuma.{{container_name}}.http.name: "Lidarr (Music Manager)"
      kuma.{{container_name}}.http.url: "${LIDARR_MONITOR_URL:?[lidarr] Monitor URL missing}"
    networks:
      - home
    ports:
      - "4002:3002"
    restart: unless-stopped
    user: "${PUID_ROOT:?User ID missing}:${PGID_ROOT:?Group ID missing}"
    volumes:
      # Volume mounts from host system
      - "${DOWNLOADS_DIRECTORY}:/downloads"
      - "${MUSIC_DIRECTORY}:/music"
      # Persistent volumes
      - ./storage/lidarr/:/config

  # Music Player
  navidrome:
    image: deluan/navidrome:0.52.5
    container_name: navidrome
    hostname: navidrome
    deploy:
      resources:
        limits:
          cpus: "1"
          memory: "1024M"
    environment:
      # Base config
      TZ: "${TIMEZONE:?Timezone missing}"
      # Authentication config
      ND_REVERSEPROXYUSERHEADER: "X-Authentik-Username"
      ND_REVERSEPROXYWHITELIST: "0.0.0.0/0"
      # External services config
      ND_LASTFM_APIKEY: "${LASTFM_API_KEY:?[navidrome] LastFM API key missing}"
      ND_LASTFM_SECRET: "${LASTFM_SECRET:?[navidrome] LastFM secret missing}"
      ND_SPOTIFY_ID: "${SPOTIFY_CLIENT_ID:?[navidrome] Spotify Client ID missing}"
      ND_SPOTIFY_SECRET: "${SPOTIFY_CLIENT_SECRET:?[navidrome] Spotify Client secret missing}"
      # Scheduling config
      ND_SCANSCHEDULE: "@every 24h"
      # Metadata config
      ND_ARTISTARTPRIORITY: "artist.*, external"
      ND_COVERARTPRIORITY: "album.*, embedded, external"
      # Logging config
      ND_LOGLEVEL: "info"
    labels:
      # Uptime-Kuma monitor config
      kuma.{{container_name}}.http.name: "Navidrome (Music Player)"
      kuma.{{container_name}}.http.url: "${NAVIDROME_MONITOR_URL:?[navidrome] Monitor URL missing}"
    networks:
      - home
    ports:
      - "4003:4533"
    restart: unless-stopped
    user: "${PUID_ROOT:?User ID missing}:${PGID_ROOT:?Group ID missing}"
    volumes:
      # Volume mounts from host system
      - "${MUSIC_DIRECTORY}:/music:ro"
      # Persistent volumes
      - ./storage/navidrome/:/data

  # eBook manager
  readarr:
    image: linuxserver/readarr:nightly-version-0.4.0.2593
    container_name: readarr
    hostname: readarr
    depends_on:
      qbittorrent:
        condition: service_healthy
    deploy:
      resources:
        limits:
          cpus: "1"
          memory: "512M"
    environment:
      # Base config
      TZ: "${TIMEZONE:?Timezone missing}"
    healthcheck:
      interval: 30s
      retries: 3
      start_period: 30s
      test: [ "CMD", "wget", "--no-verbose", "--tries=1", "-O", "-", "http://127.0.0.1:3004/ping" ]
      timeout: 5s
    labels:
      # Uptime-Kuma monitor config
      kuma.{{container_name}}.http.name: "Readarr (eBook Manager)"
      kuma.{{container_name}}.http.url: "${READARR_MONITOR_URL:?[readarr] Monitor URL missing}"
    networks:
      - home
    ports:
      - "4004:3004"
    restart: unless-stopped
    user: "${PUID_ROOT:?User ID missing}:${PGID_ROOT:?Group ID missing}"
    volumes:
      # Volume mounts from host system
      - "${BOOKS_DIRECTORY}:/books"
      - "${DOWNLOADS_DIRECTORY}:/downloads"
      # Persistent volumes
      - ./storage/readarr/:/config

  # eBook management
  kavita:
    image: jvmilazz0/kavita:nightly-0.8.2.1
    container_name: kavita
    hostname: kavita
    deploy:
      resources:
        limits:
          cpus: "0.5"
          memory: "2048M"
    environment:
      # Base config
      TZ: "${TIMEZONE:?Timezone missing}"
    labels:
      # Uptime-Kuma monitor config
      kuma.{{container_name}}.http.name: "Kavita (eBook Reader)"
      kuma.{{container_name}}.http.url: "${KAVITA_MONITOR_URL:?[kavita] Monitor URL missing}"
    networks:
      - home
    ports:
      - "4005:5000"
    restart: unless-stopped
    user: "${PUID_ROOT:?User ID missing}:${PGID_ROOT:?Group ID missing}"
    volumes:
      # Volume mounts from host system
      - "${BOOKS_DIRECTORY}:/books"
      # Persistent volumes
      - ./storage/kavita/:/kavita/config

  # Downloaded archive extracter
  unpackerr:
    image: golift/unpackerr:0.14.5
    container_name: unpackerr
    hostname: unpackerr
    depends_on:
      lidarr:
        condition: service_healthy
      radarr:
        condition: service_healthy
      readarr:
        condition: service_healthy
      sonarr:
        condition: service_healthy
    deploy:
      resources:
        limits:
          cpus: "2"
          memory: "512M"
    environment:
      # Base config
      TZ: "${TIMEZONE:?Timezone missing}"
      # Lidarr config
      UN_LIDARR_0_API_KEY: "${LIDARR_API_KEY:?[unpackerr] Lidarr API key missing}"
      UN_LIDARR_0_PATHS_0: "/downloads/complete/lidarr"
      UN_LIDARR_0_PROTOCOLS: "torrent,usenet"
      UN_LIDARR_0_URL: "http://lidarr:3002"
      # Radarr config
      UN_RADARR_0_API_KEY: "${RADARR_API_KEY:?[unpackerr] Radarr API key missing}"
      UN_RADARR_0_PATHS_0: "/downloads/complete/radarr"
      UN_RADARR_0_PROTOCOLS: "torrent,usenet"
      UN_RADARR_0_URL: "http://radarr:3000"
      # Readarr config
      UN_READARR_0_API_KEY: "${READARR_API_KEY:?[unpackerr] Readarr API key missing}"
      UN_READARR_0_PATHS_0: "/downloads/complete/readarr"
      UN_READARR_0_PROTOCOLS: "torrent,usenet"
      UN_READARR_0_URL: "http://readarr:3004"
      # Sonarr config
      UN_SONARR_0_API_KEY: "${SONARR_API_KEY:?[unpackerr] Sonarr API key missing}"
      UN_SONARR_0_PATHS_0: "/downloads/complete/sonarr"
      UN_SONARR_0_PROTOCOLS: "torrent,usenet"
      UN_SONARR_0_URL: "http://sonarr:3001"
      # Logging config
      UN_LOG_FILE: "/config/unpackerr.log"
    healthcheck:
      interval: 30s
      retries: 3
      start_period: 30s
      test: [ "CMD", "/unpackerr", "-v" ]
      timeout: 5s
    networks:
      - home
    restart: unless-stopped
    user: "${PUID_ROOT:?User ID missing}:${PGID_ROOT:?Group ID missing}"
    volumes:
      # Volume mounts from host system
      - "${DOWNLOADS_DIRECTORY}:/downloads"
      # Persistent volumes
      - ./storage/unpackerr/:/config

  # Subtitle downloader
  bazarr:
    image: linuxserver/bazarr:development-version-v1.4.4-beta.27
    container_name: bazarr
    hostname: bazarr
    deploy:
      resources:
        limits:
          cpus: "1"
          memory: "1024M"
    depends_on:
      radarr:
        condition: service_healthy
      sonarr:
        condition: service_healthy
    environment:
      # Base config
      TZ: "${TIMEZONE:?Timezone missing}"
    healthcheck:
      interval: 30s
      retries: 3
      start_period: 30s
      test: [ "CMD", "wget", "--no-verbose", "--tries=1", "-O", "-", "http://127.0.0.1:3009/api/system/status?apikey=${BAZARR_API_KEY}" ]
      timeout: 5s
    labels:
      # Uptime-Kuma monitor config
      kuma.{{container_name}}.http.name: "Bazarr (Subtitle Manager)"
      kuma.{{container_name}}.http.url: "${BAZARR_MONITOR_URL:?[bazarr] Monitor URL missing}"
    networks:
      - home
    ports:
      - "4009:3009"
    restart: unless-stopped
    user: "${PUID_ROOT:?User ID missing}:${PGID_ROOT:?Group ID missing}"
    volumes:
      # Volume mounts from host system
      - "${MOVIE_DIRECTORY}:/movies"
      - "${TV1_DIRECTORY}:/tv1"
      - "${TV2_DIRECTORY}:/tv2"
      - "${TV_TRASH_DIRECTORY}:/trash"
      - "${TV_KIDS_DIRECTORY}:/kids"
      # Persistent volumes
      - ./docker/bazarr/scripts/cleansubs.sh:/opt/cleansubs.sh
      - ./storage/bazarr/:/config

  # Automated Speech Recognition tool for subtitles
  whisper:
    image: onerahmet/openai-whisper-asr-webservice:v1.5.0
    container_name: whisper
    hostname: whisper
    deploy:
      resources:
        limits:
          cpus: "4"
          memory: "8192M"
    environment:
      # Model config
      ASR_MODEL: "small.en"
      ASR_ENGINE: "faster_whisper"
    healthcheck:
      interval: 60s
      retries: 3
      start_period: 30s
      test: [ "CMD", "wget", "--no-verbose", "--tries=1", "-O", "-", "http://127.0.0.1:9000/" ]
      timeout: 15s
    networks:
      - home
    restart: unless-stopped
    user: "${PUID_ROOT:?User ID missing}:${PGID_ROOT:?Group ID missing}"

  # Video Game ROM Manager
  romm:
    image: rommapp/romm:3.4.0
    container_name: romm
    hostname: romm
    depends_on:
      romm-db:
        condition: service_healthy
    deploy:
      resources:
        limits:
          cpus: "1"
          memory: "1024M"
    environment:
      # Base config
      TZ: "${TIMEZONE:?Timezone missing}"
      # Authentication config
      ROMM_AUTH_SECRET_KEY: "${ROMM_SECRET_KEY:?[romm] Authentication secret key missing}"
      # Database config
      ROMM_DB_DRIVER: "mariadb"
      DB_HOST: "romm-db"
      DB_NAME: "${ROMM_DB_NAME:?[romm] Database name missing}"
      DB_PORT: "${ROMM_DB_PORT:?[romm] Database port missing}"
      # Do not quote the following two, or leave a comment afterward, can cause issues, for some reason (I think the quotes are passed in)
      DB_PASSWD: ${ROMM_DB_PASSWORD:?[romm] Database password missing}
      DB_ROOT_PASSWD: ${ROMM_DB_ROOT_PASSWORD:?[romm] Database root password missing}
      DB_USER: "${ROMM_DB_USER:?[romm] Database user missing}"
      # External systems config
      IGDB_CLIENT_ID: "${TWITCH_CLIENT_ID:?[romm] Twitch client ID missing}"
      IGDB_CLIENT_SECRET: "${TWITCH_CLIENT_SECRET:?[romm] Twitch client secret missing}"
      MOBYGAMES_API_KEY: "${MOBYGAMES_API_KEY:?[romm] MobyGames API key missing}"
      STEAMGRIDDB_API_KEY: "${STEAMGRIDDB_API_KEY:?[romm] SteamGridDB API key missing}"
    healthcheck:
      interval: 30s
      retries: 3
      start_period: 30s
      test: [ "CMD", "wget", "--no-verbose", "--tries=1", "-O", "-", "http://127.0.0.1:8080/heartbeat" ]
      timeout: 5s
    labels:
      # Uptime-Kuma monitor config
      kuma.{{container_name}}.http.name: "RomM (Game ROM Manager)"
      kuma.{{container_name}}.http.url: "${ROMM_MONITOR_URL:?[romm] Monitor URL missing}"
    networks:
      - home
    ports:
      - "4006:8080"
    restart: unless-stopped
    user: "${PUID_ROOT:?User ID missing}:${PGID_ROOT:?Group ID missing}"
    volumes:
      # Volume mounts from host system
      - "${ROM_GAMES_AND_BIOS_PARENT_DIRECTORY}:/romm/library:ro"
      - "${ROM_SAVES_DIRECTORY}:/romm/assets"
      - "./docker/romm/config/:/romm/config"
      # Persistent volumes
      - ./storage/romm/resources:/romm/resources  # Resources fetched from IGDB (covers, screenshots, etc.)
      - ./storage/romm/redis:/redis-data          # Cached data for background tasks

  # Database for RomM
  romm-db:
    image: mariadb:11.4.2
    container_name: romm-db
    hostname: romm-db
    deploy:
      resources:
        limits:
          cpus: "0.5"
          memory: "512M"
    environment:
      # Base config
      TZ: "${TIMEZONE:?Timezone missing}"
      # Database config
      MYSQL_TCP_PORT: "${ROMM_DB_PORT:?[romm-db] Database port missing}"
      MYSQL_DATABASE: "${ROMM_DB_NAME:?[romm-db] Database name missing}"
      MYSQL_PASSWORD: "${ROMM_DB_PASSWORD:?[romm-db] Database password missing}"
      MYSQL_ROOT_PASSWORD: "${ROMM_DB_ROOT_PASSWORD:?[romm-db] Database root password missing}"
      MYSQL_USER: "${ROMM_DB_USER:?[romm-db] Database user missing}"
    expose:
      - "${ROMM_DB_PORT:?[romm-db] Database port missing}"
    healthcheck:
      interval: 30s
      retries: 3
      start_period: 30s
      test: [ "CMD", "healthcheck.sh", "--su-mysql", "--connect", "--innodb_initialized" ]
      timeout: 5s
    networks:
      - home
    restart: unless-stopped
    user: "${PUID_ROOT:?User ID missing}:${PGID_ROOT:?Group ID missing}"
    volumes:
      - romm-db_data:/var/lib/mysql

  # Indexer (Torrents + Usenet)
  prowlarr:
    image: linuxserver/prowlarr:nightly-version-1.21.2.4648
    container_name: prowlarr
    hostname: prowlarr
    depends_on:
      flaresolverr:
        condition: service_healthy
      lidarr:
        condition: service_healthy
      radarr:
        condition: service_healthy
      readarr:
        condition: service_healthy
      sonarr:
        condition: service_healthy
    deploy:
      resources:
        limits:
          cpus: "1"
          memory: "1024M" # High memory resourcing to allow stats page to load
    environment:
      # Base config
      TZ: "${TIMEZONE:?Timezone missing}"
    healthcheck:
      interval: 30s
      retries: 3
      start_period: 30s
      test: [ "CMD", "wget", "--no-verbose", "--tries=1", "-O", "-", "http://127.0.0.1:3016/ping" ]
      timeout: 5s
    labels:
      # Uptime-Kuma monitor config
      kuma.{{container_name}}.http.name: "Prowlarr (Torrent Indexer)"
      kuma.{{container_name}}.http.url: "${PROWLARR_MONITOR_URL:?[prowlarr] Monitor URL missing}"
    networks:
      - home
    ports:
      - "4016:3016"
    restart: unless-stopped
    user: "${PUID_ROOT:?User ID missing}:${PGID_ROOT:?Group ID missing}"
    volumes:
      # Persistent volumes
      - ./storage/prowlarr/:/config

  # Torrent downloader
  qbittorrent:
    build:
      context: .
      dockerfile: docker/qbittorrent/Dockerfile
      args:
        # Docker image versions
        QBITTORRENT_VERSION: "4.6.5"  # https://hub.docker.com/r/linuxserver/qbittorrent/tags
        # Build arguments
        VUETORRENT_VERSION: "2.11.1"  # https://github.com/WDaan/VueTorrent/releases/
    container_name: qbittorrent
    hostname: qbittorrent
    deploy:
      resources:
        limits:
          cpus: "1.5"
          memory: "4096M"
    environment:
      # Base config
      TZ: "${TIMEZONE:?Timezone missing}"
      # UI config
      WEBUI_PORT: "9000"
    healthcheck:
      interval: 30s
      retries: 3
      start_period: 30s
      test: [ "CMD", "wget", "--no-verbose", "--tries=1", "-O", "-", "http://127.0.0.1:9000/api/v2/app/version" ]
      timeout: 5s
    labels:
      # Uptime-Kuma monitor config
      kuma.{{container_name}}.http.name: "qBittorrent (Torrent Downloader)"
      kuma.{{container_name}}.http.url: "${QBITTORRENT_MONITOR_URL:?[qbittorrent] Monitor URL missing}"
    networks:
      - home
    ports:
      - "9000-9002:9000-9002"
    restart: unless-stopped
    user: "${PUID_ROOT:?User ID missing}:${PGID_ROOT:?Group ID missing}"
    volumes:
      # Volume mounts from host system
      - "${DOWNLOADS_DIRECTORY}:/downloads"
      # Persistent volumes
      - ./storage/qbittorrent/:/config

  # Discord bot for requests
  doplarr:
    image: linuxserver/doplarr:3.6.3
    container_name: doplarr
    hostname: doplarr
    depends_on:
      radarr:
        condition: service_healthy
      sonarr:
        condition: service_healthy
    deploy:
      resources:
        limits:
          cpus: "0.5"
          memory: "512M"
    environment:
      # Integration configuration
      RADARR__URL: "http://radarr:3000"
      SONARR__URL: "http://sonarr:3001"
      RADARR__API: "${RADARR_API_KEY:?[doplarr] Radarr API key missing}"
      SONARR__API: "${SONARR_API_KEY:?[doplarr] Sonarr API key missing}"
      # Discord configuration
      DISCORD__MAX_RESULTS: "5"
      DISCORD__TOKEN: "${DISCORD_BOT_TOKEN:?[doplarr] Discord bot token missing}"
    healthcheck:
      interval: 30s
      retries: 3
      start_period: 30s
      test: [ "CMD", "pidof", "java" ]
      timeout: 5s
    networks:
      - home
    restart: unless-stopped
    user: "${PUID_ROOT:?User ID missing}:${PGID_ROOT:?Group ID missing}"
    volumes:
      # Persistent volumes
      - ./storage/doplarr/:/config

  # Cloudflare rate-limiter solver
  flaresolverr:
    image: flaresolverr/flaresolverr:v3.3.21
    container_name: flaresolverr
    hostname: flaresolverr
    deploy:
      resources:
        limits:
          cpus: "0.5"
          memory: "1024M"
    environment:
      # Base config
      TZ: "${TIMEZONE:?Timezone missing}"
      # Logging config
      LOG_LEVEL: "info"
    healthcheck:
      interval: 30s
      retries: 3
      start_period: 30s
      test: [ "CMD", "curl", "--silent", "--fail", "http://127.0.0.1:8191/" ] # wget not installed
      timeout: 5s
    networks:
      - home
    restart: unless-stopped
    user: "${PUID_ROOT:?User ID missing}:${PGID_ROOT:?Group ID missing}"

  # Recipe manager
  tandoor:
    image: vabene1111/recipes:1.5.17
    container_name: tandoor
    hostname: tandoor
    depends_on:
      tandoor-db:
        condition: service_healthy
    deploy:
      resources:
        limits:
          cpus: "1"
          memory: "1024M"
    environment:
      # Base config
      TIMEZONE: "${TIMEZONE:?Timezone missing}"
      # Authentication config
      REMOTE_USER_AUTH: "1"
      SOCIAL_DEFAULT_ACCESS: "1"
      SOCIAL_DEFAULT_GROUP: "guest"
      SOCIAL_PROVIDERS: "allauth.socialaccount.providers.openid_connect"
      SOCIALACCOUNT_PROVIDERS: |
        {
          "openid_connect": {
            "SERVERS": [
              {
                "id": "authentik",
                "name": "Authentik",
                "server_url": "${TANDOOR_AUTHENTIK_WELL_KNOWN_CONFIGURATION_URL:?[tandoor] Authetik URL missing}",
                "token_auth_method": "client_secret_basic",
                "APP": {
                  "client_id": "${TANDOOR_AUTHENTIK_CLIENT_ID:?[tandoor] Authentik client ID missing}",
                  "secret": "${TANDOOR_AUTHENTIK_CLIENT_SECRET:?[tandoor] Authentik client secret missing}"
                }
              }
            ]
          }
        }
      # Database config
      DB_ENGINE: "django.db.backends.postgresql"
      POSTGRES_HOST: "tandoor-db"
      POSTGRES_PORT: "${TANDOOR_DB_PORT:?[tandoor] Database port missing}"
      POSTGRES_DB: "${TANDOOR_DB_NAME:?[tandoor] Database name missing}"
      POSTGRES_PASSWORD: "${TANDOOR_DB_PASSWORD:?[tandoor] Database password missing}"
      POSTGRES_USER: "${TANDOOR_DB_USER:?[tandoor] Database user missing}"
      # Secrets config
      SECRET_KEY: "${TANDOOR_SECRET_KEY:?[tandoor] Tandoor secret key missing}"
    healthcheck:
      interval: 30s
      retries: 3
      start_period: 30s
      test: [ "CMD", "wget", "--no-verbose", "--tries=1", "-O", "-", "http://127.0.0.1:8080/openapi" ]
      timeout: 5s
    labels:
      # Uptime-Kuma monitor config
      kuma.{{container_name}}.http.name: "Tandoor (Recipes)"
      kuma.{{container_name}}.http.url: "${TANDOOR_MONITOR_URL:?[tandoor] Monitor URL missing}"
      kuma.{{container_name}}.http.max_redirects: 1
    networks:
      - home
    restart: unless-stopped
    user: "${PUID_ROOT:?User ID missing}:${PGID_ROOT:?Group ID missing}"
    volumes:
      - ./storage/tandoor/nginx/conf.d/:/opt/recipes/nginx/conf.d
      - ./storage/tandoor/mediafiles/:/opt/recipes/mediafiles
      - ./storage/tandoor/staticfiles/:/opt/recipes/staticfiles

  # Recipe manager reverse-proxy/UI
  tandoor-ui:
    image: nginx:1.27.0-alpine
    container_name: tandoor-ui
    hostname: tandoor-ui
    depends_on:
      tandoor:
        condition: service_healthy
    deploy:
      resources:
        limits:
          cpus: "0.5"
          memory: "1024M"
    environment:
      # Base config
      TIMEZONE: "${TIMEZONE:?Timezone missing}"
      # Secrets config
      SECRET_KEY: "${TANDOOR_SECRET_KEY:?[tandoor-ui] Tandoor secret key missing}"
    healthcheck:
      interval: 30s
      retries: 3
      start_period: 30s
      test: [ "CMD", "wget", "--no-verbose", "--tries=1", "-O", "-", "http://127.0.0.1:80" ]
      timeout: 5s
    networks:
      - home
    ports:
      - "4010:80"
    restart: unless-stopped
    user: "${PUID_ROOT:?User ID missing}:${PGID_ROOT:?Group ID missing}"
    volumes:
      - ./storage/tandoor/nginx/conf.d/:/etc/nginx/conf.d:ro
      - ./storage/tandoor/mediafiles/:/media:ro
      - ./storage/tandoor/staticfiles/:/static:ro

  # Database for Tandoor
  tandoor-db:
    image: postgres:16.3-alpine
    container_name: tandoor-db
    hostname: tandoor-db
    deploy:
      resources:
        limits:
          cpus: "0.5"
          memory: "512M"
    environment:
      # Base config
      TIMEZONE: "${TIMEZONE:?Timezone missing}"
      # Database config
      PGPORT: "${TANDOOR_DB_PORT:?[tandoor-db] Database port missing}"
      POSTGRES_DB: "${TANDOOR_DB_NAME:?[tandoor-db] Database name missing}"
      POSTGRES_PASSWORD: "${TANDOOR_DB_PASSWORD:?[tandoor-db] Database password missing}"
      POSTGRES_USER: "${TANDOOR_DB_USER:?[tandoor-db] Database user missing}"
      # Next 3 variables needed to avoid "FATAL role 'root' does not exist" error
      PGDATABASE: "${TANDOOR_DB_NAME:?[tandoor-db] Database name missing}"
      PGPASSWORD: "${TANDOOR_DB_PASSWORD:?[tandoor-db] Database password missing}"
      PGUSER: "${TANDOOR_DB_USER:?[tandoor-db] Database user missing}"
    expose:
      - "${TANDOOR_DB_PORT:?[tandoor-db] Database port missing}"
    healthcheck:
      interval: 30s
      retries: 3
      start_period: 30s
      test: [ "CMD", "pg_isready", "--host", "localhost" ]
      timeout: 5s
    networks:
      - home
    restart: unless-stopped
    user: "${PUID_ROOT:?User ID missing}:${PGID_ROOT:?Group ID missing}"
    volumes:
      - ./storage/tandoor-db/data/:/var/lib/postgresql/data

  # Inventory management
  homebox:
    image: ghcr.io/sysadminsmedia/homebox:0.13.0 # https://github.com/sysadminsmedia/homebox/pkgs/container/homebox
    container_name: homebox
    hostname: homebox
    deploy:
      resources:
        limits:
          cpus: "0.5"
          memory: "512M"
    environment:
      # Base config
      TZ: "${TIMEZONE:?Timezone missing}"
      # Logging config
      HBOX_LOG_FORMAT: "text"
      HBOX_LOG_LEVEL: "info"
      # UI config
      HBOX_OPTIONS_ALLOW_REGISTRATION: "false"
      HBOX_WEB_MAX_UPLOAD_SIZE: "10"
    labels:
      # Uptime-Kuma monitor config
      kuma.{{container_name}}.http.name: "Homebox (House Inventory)"
      kuma.{{container_name}}.http.url: "${HOMEBOX_MONITOR_URL:?[homebox] Monitor URL missing}"
    networks:
      - home
    ports:
      - "4011:7745"
    restart: unless-stopped
    user: "${PUID_ROOT:?User ID missing}:${PGID_ROOT:?Group ID missing}"
    volumes:
      - ./storage/homebox/:/data/

  # Daily journal
  dailytxt:
    image: phitux/dailytxt:1.0.15
    container_name: dailytxt
    hostname: dailytxt
    deploy:
      resources:
        limits:
          cpus: "0.5"
          memory: "256M"
    environment:
      # Base config
      USER_GID: "${PGID_ROOT:?Group ID missing}"
      USER_UID: "${PUID_ROOT:?User ID missing}"
      TZ: "${TIMEZONE:?Timezone missing}"
      # Secrets config
      SECRET_KEY: "${DAILYTXT_SECRET_KEY:?[dailytxt] Secret key missing}"
      # UI config
      ALLOW_REGISTRATION: "false"
      DATA_INDENT: "2"
      ENABLE_UPDATE_CHECK: "false"
      JWT_EXP_DAYS: "60"
      PORT: "8765"
    healthcheck:
      interval: 30s
      retries: 3
      start_period: 30s
      test: [ "CMD", "wget", "--no-verbose", "--tries=1", "-O", "-", "http://127.0.0.1:8765" ]
      timeout: 5s
    labels:
      # Uptime-Kuma monitor config
      kuma.{{container_name}}.http.name: "DailyTxt (Daily Journal)"
      kuma.{{container_name}}.http.url: "${DAILYTXT_MONITOR_URL:?[dailytxt] Monitor URL missing}"
    networks:
      - home
    ports:
      - "4015:8765"
    restart: unless-stopped
    volumes:
      - ./storage/dailytxt/:/app/data/

  # Docker log viewer
  dozzle:
    image: amir20/dozzle:v8.1.3
    container_name: dozzle
    hostname: dozzle
    deploy:
      resources:
        limits:
          cpus: "0.5"
          memory: "256M"
    environment:
      # Base config
      TZ: "${TIMEZONE:?Timezone missing}"
      # Authentication config
      DOZZLE_AUTH_PROVIDER: "forward-proxy"
      DOZZLE_AUTH_HEADER_USER: "X-Authentik-Username"
      DOZZLE_AUTH_HEADER_EMAIL: "X-Authentik-Email"
      DOZZLE_AUTH_HEADER_NAME: "X-Authentik-Name"
      # Hosts config
      DOZZLE_HOSTNAME: "${MAIN_HOST_NAME:?[dozzle] Hostname missing}"
      DOZZLE_REMOTE_HOST: "${REMOTE_HOSTS}"
      # UI config
      DOZZLE_ENABLE_ACTIONS: "true"
      DOZZLE_NO_ANALYTICS: "true"
    healthcheck:
      interval: 30s
      retries: 3
      start_period: 30s
      test: [ "CMD", "/dozzle", "healthcheck" ]
      timeout: 5s
    labels:
      # Uptime-Kuma monitor config
      kuma.{{container_name}}.http.name: "Dozzle (Docker Log Viewer)"
      kuma.{{container_name}}.http.url: "${DOZZLE_MONITOR_URL:?[dozzle] Monitor URL missing}"
    networks:
      - home
    ports:
      - "4014:8080"
    restart: unless-stopped
    user: "${PUID_ROOT:?User ID missing}:${PGID_ROOT:?Group ID missing}"
    volumes:
      # Docker socket mount from host system
      - "${DOCKER_SOCKET}:/var/run/docker.sock"

  # Documents file browser
  filebrowser:
    image: filebrowser/filebrowser:v2.30.0
    container_name: filebrowser
    hostname: filebrowser
    deploy:
      resources:
        limits:
          cpus: "0.25"
          memory: "256M"
    environment:
      # Base config
      TZ: "${TIMEZONE:?Timezone missing}"
      # Authentication config
      FB_NOAUTH: noauth
    labels:
      # Uptime-Kuma monitor config
      kuma.{{container_name}}.http.name: "FileBrowser (Document Viewer)"
      kuma.{{container_name}}.http.url: "${FILEBROWSER_MONITOR_URL:?[filebrowser] Monitor URL missing}"
    networks:
      - home
    ports:
      - "4007:80"
    restart: unless-stopped
    security_opt:
      - no-new-privileges:true
    user: "${PUID_ROOT:?User ID missing}:${PGID_ROOT:?Group ID missing}"
    volumes:
      # Volume mounts from host system
      - "${DOCUMENTS_DIRECTORY}:/srv:ro"
      # Persistent volumes
      - ./storage/filebrowser/:/app/data/
      - ./storage/filebrowser/.filebrowser.json:/.filebrowser.json
      - ./storage/filebrowser/database.db:/database.db

  # Large Language Model with ChatGPT-style UI
  ollama:
    image: ghcr.io/open-webui/open-webui:v0.3.11-ollama # https://github.com/open-webui/open-webui/pkgs/container/open-webui
    container_name: ollama
    hostname: ollama
    deploy:
      resources:
        limits:
          cpus: "8"
          memory: "8192M"
    environment:
      # Base config
      TZ: "${TIMEZONE:?Timezone missing}"
      # UI config
      ENABLE_COMMUNITY_SHARING: "false"
      ENABLE_SIGNUP: "false"
      WEBUI_AUTH_TRUSTED_EMAIL_HEADER: "X-Authentik-Email"
      WEBUI_NAME: "${DOMAIN_NAME:?[ollama] Domain name missing} LLM"
      WEBUI_SECRET_KEY: "${OLLAMA_SECRET_KEY:?[ollama] Secret key missing}"
    labels:
      # Uptime-Kuma monitor config
      kuma.{{container_name}}.http.name: "Ollama (LLM)"
      kuma.{{container_name}}.http.url: "${OLLAMA_MONITOR_URL:?[ollama] Monitor URL missing}"
    networks:
      - home
    ports:
      - "4012:8080"
    restart: unless-stopped
    user: "${PUID_ROOT:?User ID missing}:${PGID_ROOT:?Group ID missing}"
    volumes:
      # Persistent volumes
      - ollama:/root/.ollama
      - ./storage/ollama/open-webui:/app/backend/data

  # Static code analysis tool
  sonarqube:
    image: sonarqube:10.6.0-community
    container_name: sonarqube
    hostname: sonarqube
    depends_on:
      sonarqube-db:
        condition: service_healthy
    deploy:
      resources:
        limits:
          cpus: "1"
          memory: "4096M"
    environment:
      SONAR_JDBC_URL: "jdbc:postgresql://sonarqube-db:5432/${SONARQUBE_DB_NAME:?[sonarqube] Database name missing}"
      SONAR_JDBC_USERNAME: "${SONARQUBE_DB_USER:?[sonarqube] Database user missing}"
      SONAR_JDBC_PASSWORD: "${SONARQUBE_DB_PASSWORD:?[sonarqube] Database password missing}"
    healthcheck:
      interval: 30s
      retries: 3
      start_period: 30s
      test: [ "CMD", "wget", "--no-verbose", "--tries=1", "-O", "-", "http://127.0.0.1:9000/api/system/status" ]
      timeout: 5s
    labels:
      # Uptime-Kuma monitor config
      kuma.{{container_name}}.http.name: "SonarQube (Code Analysis)"
      kuma.{{container_name}}.http.url: "${SONARQUBE_MONITOR_URL:?[sonarqube] Monitor URL missing}"
    networks:
      - home
    ports:
      - "4013:9000"
    restart: unless-stopped
    user: "${PUID_NON_ROOT:?User ID missing}:${PGID_ROOT:?Group ID missing}"
    volumes:
      - ./storage/sonarqube/data/:/opt/sonarqube/data
      - ./storage/sonarqube/extensions/:/opt/sonarqube/extensions
      - ./storage/sonarqube/logs/:/opt/sonarqube/logs

  # Database for SonarQube
  sonarqube-db:
    image: postgres:16.3-alpine
    container_name: sonarqube-db
    hostname: sonarqube-db
    deploy:
      resources:
        limits:
          cpus: "0.5"
          memory: "512M"
    environment:
      # Base config
      TIMEZONE: "${TIMEZONE:?Timezone missing}"
      # Database config
      PGPORT: "${SONARQUBE_DB_PORT:?[sonarqube-db] Database port missing}"
      POSTGRES_DB: "${SONARQUBE_DB_NAME:?[sonarqube-db] Database name missing}"
      POSTGRES_USER: "${SONARQUBE_DB_USER:?[sonarqube-db] Database user missing}"
      POSTGRES_PASSWORD: "${SONARQUBE_DB_PASSWORD:?[sonarqube-db] Database password missing}"
      # Next 3 variables needed to avoid "FATAL role 'root' does not exist" error
      PGDATABASE: "${SONARQUBE_DB_NAME:?[sonarqube-db] Database password missing}"
      PGPASSWORD: "${SONARQUBE_DB_PASSWORD:?[sonarqube-db] Database name missing}"
      PGUSER: "${SONARQUBE_DB_USER:?[sonarqube-db] Database user missing}"
    expose:
      - "${SONARQUBE_DB_PORT:?[sonarqube-db] Database port missing}"
    healthcheck:
      interval: 30s
      retries: 3
      start_period: 30s
      test: [ "CMD", "pg_isready", "--host", "localhost" ]
      timeout: 5s
    networks:
      - home
    restart: unless-stopped
    user: "${PUID_ROOT:?User ID missing}:${PGID_ROOT:?Group ID missing}"
    volumes:
      - ./storage/sonarqube-db/data/:/var/lib/postgresql/data

  # Statistics for Emby
  jellystat:
    image: cyfershepard/jellystat:1.1.0
    container_name: jellystat
    hostname: jellystat
    depends_on:
      jellystat-db:
        condition: service_healthy
    deploy:
      resources:
        limits:
          cpus: "0.5"
          memory: "1024M"
    environment:
      # Base config
      TZ: "${TIMEZONE:?Timezone missing}"
      # Database config
      POSTGRES_DATABASE: "${JELLYSTAT_DB_NAME:?[jellystat] Database name missing}"
      POSTGRES_USER: "${JELLYSTAT_DB_USER:?[jellystat] Database user missing}"
      POSTGRES_PASSWORD: "${JELLYSTAT_DB_PASSWORD:?[jellystat] Database password missing}"
      POSTGRES_IP: "jellystat-db"
      POSTGRES_PORT: "${JELLYSTAT_DB_PORT:?[jellystat] Database port missing}"
      # Secrets config
      JWT_SECRET: '${JELLYSTAT_SECRET_KEY:?[jellystat] Secret key missing}'
    # Healthcheck is temporary until https://github.com/CyferShepard/Jellystat/pull/220 is merged and released
    healthcheck:
      interval: 30s
      retries: 3
      start_period: 30s
      test: [ "CMD", "pidof", "node" ]
      timeout: 5s
    labels:
      # Uptime-Kuma monitor config
      kuma.{{container_name}}.http.name: "Jellystat (Emby Statistics)"
      kuma.{{container_name}}.http.url: "${JELLYSTAT_MONITOR_URL:?[jellystat] Monitor URL missing}"
    networks:
      - home
    ports:
      - "4008:3000"
    restart: unless-stopped
    user: "${PUID_ROOT:?User ID missing}:${PGID_ROOT:?Group ID missing}"
    volumes:
      - ./storage/jellystat/data/:/app/backend/backup-data

  # Database for Jellystat
  jellystat-db:
    image: postgres:16.3-alpine
    container_name: jellystat-db
    hostname: jellystat-db
    deploy:
      resources:
        limits:
          cpus: "0.5"
          memory: "512M"
    environment:
      # Base config
      TIMEZONE: "${TIMEZONE:?Timezone missing}"
      # Database config
      PGPORT: "${JELLYSTAT_DB_PORT:?[jellystat-db] Database port missing}"
      POSTGRES_DB: "${JELLYSTAT_DB_NAME:?[jellystat-db] Database name missing}"
      POSTGRES_PASSWORD: "${JELLYSTAT_DB_PASSWORD:?[jellystat-db] Database password missing}"
      POSTGRES_USER: "${JELLYSTAT_DB_USER:?[jellystat-db] Database user missing}"
      # Next 3 variables needed to avoid "FATAL role 'root' does not exist" error
      PGDATABASE: "${JELLYSTAT_DB_NAME:?[jellystat-db] Database name missing}"
      PGPASSWORD: "${JELLYSTAT_DB_PASSWORD:?[jellystat-db] Database password missing}"
      PGUSER: "${JELLYSTAT_DB_USER:?[jellystat-db] Database user missing}"
    expose:
      - "${JELLYSTAT_DB_PORT:?[jellystat-db] Database port missing}"
    healthcheck:
      interval: 30s
      retries: 3
      start_period: 30s
      test: [ "CMD", "pg_isready", "--host", "localhost" ]
      timeout: 5s
    networks:
      - home
    restart: unless-stopped
    user: "${PUID_ROOT:?User ID missing}:${PGID_ROOT:?Group ID missing}"
    volumes:
      - ./storage/jellystat-db/data/:/var/lib/postgresql/data

  # Bookmark manager
  linkwarden:
    image: ghcr.io/linkwarden/linkwarden:v2.6.2 # https://github.com/linkwarden/linkwarden/pkgs/container/linkwarden
    container_name: linkwarden
    hostname: linkwarden
    depends_on:
      linkwarden-db:
        condition: service_healthy
    deploy:
      resources:
        limits:
          cpus: "0.5"
          memory: "512M"
    environment:
      # Base config
      TIMEZONE: "${TIMEZONE:?Timezone missing}"
      # Auth config
      AUTHENTIK_CLIENT_ID: "${LINKWARDEN_AUTHENTIK_CLIENT_ID:?[linkwarden] Authentik client ID missing}"
      AUTHENTIK_CLIENT_SECRET: "${LINKWARDEN_AUTHENTIK_CLIENT_SECRET:?[linkwarden] Authentik client secret missing}"
      AUTHENTIK_CUSTOM_NAME: "Authentik"
      AUTHENTIK_ISSUER: "${LINKWARDEN_AUTHENTIK_OIDC_URI:?[linkwarden] Authentik OIDC URL missing}"
      NEXTAUTH_SECRET: "${LINKWARDEN_SECRET_KEY:?[linkwarden] Secret key missing}"
      NEXTAUTH_URL: "${LINKWARDEN_PUBLIC_URL:?[linkwarden] Public URL missing}/api/v1/auth"
      NEXT_PUBLIC_AUTHENTIK_ENABLED: "true"
      NEXT_PUBLIC_CREDENTIALS_ENABLED: "false"
      NEXT_PUBLIC_DISABLE_REGISTRATION: "true"
      # Database config
      DATABASE_URL: "postgresql://${LINKWARDEN_DB_USER:?[linkwarden] Database user missing}:${LINKWARDEN_DB_PASSWORD:?[linkwarden] Database password missing}@linkwarden-db:${LINKWARDEN_DB_PORT:?[linkwarden] Database port missing}/${LINKWARDEN_DB_NAME:?[linkwarden] Database name missing}"
      POSTGRES_PASSWORD: "${LINKWARDEN_DB_PASSWORD:?[linkwarden] Database password missing}"
    healthcheck:
      interval: 30s
      retries: 3
      start_period: 30s
      test: [ "CMD", "curl", "--silent", "--fail", "http://127.0.0.1:3000/" ] # wget not installed
      timeout: 5s
    labels:
      # Uptime-Kuma monitor config
      kuma.{{container_name}}.http.name: "Linkwarden (Bookmarks)"
      kuma.{{container_name}}.http.url: "${LINKWARDEN_MONITOR_URL:?[linkwarden] Monitor URL missing}"
    networks:
      - home
    ports:
      - "4017:3000"
    restart: unless-stopped
    user: "${PUID_ROOT:?User ID missing}:${PGID_ROOT:?Group ID missing}"
    volumes:
      - ./storage/linkwarden/data/:/data/data

  # Database for Linkwarden
  linkwarden-db:
    image: postgres:16.3-alpine
    container_name: linkwarden-db
    hostname: linkwarden-db
    deploy:
      resources:
        limits:
          cpus: "0.5"
          memory: "512M"
    environment:
      # Base config
      TIMEZONE: "${TIMEZONE:?Timezone missing}"
      # Database config
      PGPORT: "${LINKWARDEN_DB_PORT:?[linkwarden-db] Database port missing}"
      POSTGRES_DB: "${LINKWARDEN_DB_NAME:?[linkwarden-db] Database name missing}"
      POSTGRES_PASSWORD: "${LINKWARDEN_DB_PASSWORD:?[linkwarden-db] Database password missing}"
      POSTGRES_USER: "${LINKWARDEN_DB_USER:?[linkwarden-db] Database user missing}"
      # Next 3 variables needed to avoid "FATAL role 'root' does not exist" error
      PGDATABASE: "${LINKWARDEN_DB_NAME:?[linkwarden-db] Database name missing}"
      PGPASSWORD: "${LINKWARDEN_DB_PASSWORD:?[linkwarden-db] Database password missing}"
      PGUSER: "${LINKWARDEN_DB_USER:?[linkwarden-db] Database user missing}"
    expose:
      - "${LINKWARDEN_DB_PORT:?[linkwarden-db] Database port missing}"
    healthcheck:
      interval: 30s
      retries: 3
      start_period: 30s
      test: [ "CMD", "pg_isready", "--host", "localhost" ]
      timeout: 5s
    networks:
      - home
    restart: unless-stopped
    user: "${PUID_ROOT:?User ID missing}:${PGID_ROOT:?Group ID missing}"
    volumes:
      - ./storage/linkwarden-db/data/:/var/lib/postgresql/data

  # Automatic Uptime-Kuma monitor generator
  autokuma:
    build:
      context: .
      dockerfile: docker/autokuma/Dockerfile
      args:
        # Docker image versions
        AUTOKUMA_VERSION: "0.7.0" # https://github.com/BigBoot/AutoKuma/pkgs/container/autokuma
        # Build arguments
        # Host Monitor URLs
        EMBY_MONITOR_URL: "${EMBY_MONITOR_URL:?[autokuma] Emby static monitor URL missing}"
        # NAS URLs
        SYNOLOGY_MONITOR_URL: "${SYNOLOGY_MONITOR_URL:?[autokuma] Synology static monitor URL missing}"
    container_name: autokuma
    hostname: autokuma
    deploy:
      resources:
        limits:
          cpus: "0.5"
          memory: "256M"
    environment:
      # Base config
      TZ: "${TIMEZONE:?Timezone missing}"
      # Monitor config
      AUTOKUMA__DEFAULT_SETTINGS: |-
        http.accepted_statuscodes:  [ "200" ]
        http.expiry_notification: true
        http.interval: 60
        http.max_redirects: 1
        http.max_retries: 5
        http.retry_interval: 60
        http.timeout: 30
      AUTOKUMA__KUMA__URL: "${UPTIME_KUMA_URL:?[autokuma] UptimeKuma URL missing}"
      AUTOKUMA__KUMA__USERNAME: "${ADMIN_USERNAME:?[autokuma] UptimeKuma username missing}"
      AUTOKUMA__KUMA__PASSWORD: "${UPTIME_KUMA_PASSWORD:?[autokuma] UptimeKuma password missing}"
      AUTOKUMA__ON_DELETE: "keep" # Keep monitors even if the service goes down
      AUTOKUMA__SYNC_INTERVAL: 300 # Sync every 5 minutes
      AUTOKUMA__STATIC_MONITORS: "${AUTO_KUMA_STATIC_MONITOR_DIRECTORY:?[autokuma] Static monitor directory missing}"
      AUTOKUMA__TAG_NAME: "${AUTO_KUMA_TAG_NAME:?[autokuma] Tag name missing}"
      AUTOKUMA__TAG_COLOR: "${AUTO_KUMA_TAG_COLOUR:?[autokuma] Tag colour missing}"
      # Status page envs
      STATUS_PAGE_SLUG: "main"
    healthcheck:
      interval: 30s
      retries: 3
      start_period: 30s
      test: [ "CMD", "pidof", "autokuma" ]
      timeout: 5s
    networks:
      - home
    restart: no
    user: "${PUID_ROOT:?User ID missing}:${PGID_ROOT:?Group ID missing}"
    volumes:
      # Docker socket mount from host system
      - "${DOCKER_SOCKET}:/var/run/docker.sock"

  # Automatic Uptime-Kuma monitor generator
  # This is a near-duplicate of the main one above, but connects to the RaspberryPi Docker host.
  # It also uses the published docker image, rather than building one for any static monitors.
  autokuma-pi:
    image: ghcr.io/bigboot/autokuma:0.7.0 # https://github.com/BigBoot/AutoKuma/pkgs/container/autokuma
    container_name: autokuma-pi
    hostname: autokuma-pi
    environment:
      # Base config
      TZ: "${TIMEZONE:?Timezone missing}"
      # Monitor config
      AUTOKUMA__DEFAULT_SETTINGS: |-
        http.accepted_statuscodes:  [ "200" ]
        http.tags: [{"tag_id": 3 }]
        http.expiry_notification: true
        http.interval: 60
        http.max_redirects: 1
        http.max_retries: 5
        http.retry_interval: 60
        http.timeout: 30
      AUTOKUMA__KUMA__URL: "${UPTIME_KUMA_URL:?[autokuma] UptimeKuma URL missing}"
      AUTOKUMA__KUMA__USERNAME: "${ADMIN_USERNAME:?[autokuma] UptimeKuma username missing}"
      AUTOKUMA__KUMA__PASSWORD: "${UPTIME_KUMA_PASSWORD:?[autokuma] UptimeKuma password missing}"
      AUTOKUMA__ON_DELETE: "keep" # Keep monitors even if the service goes down
      AUTOKUMA__SYNC_INTERVAL: 600 # Sync every 10 minutes
      AUTOKUMA__TAG_NAME: "${AUTO_KUMA_TAG_NAME:?[autokuma] Tag name missing}"
      AUTOKUMA__TAG_COLOR: "${AUTO_KUMA_TAG_COLOUR:?[autokuma] Tag colour missing}"
      DOCKER_HOST: "${AUTO_KUMA_REMOTE_HOST}"
    healthcheck:
      interval: 30s
      retries: 3
      start_period: 30s
      test: [ "CMD", "pidof", "autokuma" ]
      timeout: 5s
    networks:
      - home
    restart: no
    user: "${PUID_ROOT:?User ID missing}:${PGID_ROOT:?Group ID missing}"
    volumes:
      # Docker socket mount from host system
      - "${DOCKER_SOCKET}:/var/run/docker.sock"

# These volumes are required by their service, due to issues when using a Windows mount
volumes:
  romm-db_data:
    name: romm-db_data
  ollama:
    name: ollama
