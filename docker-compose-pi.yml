networks:
  home_pi:
    name: home_pi
    driver: bridge

services:

  # Reverse proxy
  traefik:
    image: traefik:v3.5.2  # https://github.com/traefik/traefik/releases
    platform: linux/arm64
    container_name: traefik
    hostname: traefik
    command: --configFile=/config/traefik.yml
    depends_on:
      traefik-provider:
        condition: service_healthy
    environment:
      # Variables used for Go template substitution in the config files
      DOMAIN_NAME: "${DOMAIN_NAME}"
      STATUS_URL: "${STATUS_URL}"
      # Non-docker service IPs
      NETALERT_URL: "${NETALERT_URL}"
      TORRENT_URL: "${TORRENT_URL}"
    healthcheck:
      interval: 30s
      retries: 3
      start_period: 30s
      test: [ "CMD", "traefik", "healthcheck", "--ping" ]
      timeout: 5s
    labels:
      # Reverse proxy config
      traefik.enable: "true"
      traefik.http.routers.traefik.rule: "Host(`traefik.${DOMAIN_NAME}`)"
      traefik.http.routers.traefik.entrypoints: "websecure"
      traefik.http.routers.traefik.tls: "true"
      traefik.http.routers.traefik.middlewares: "authelia-forward-auth@file"
      traefik.http.routers.traefik.service: "api@internal"
      # Disable authentication for 'ping' path
      traefik.http.routers.traefik-ping.rule: "Host(`traefik.${DOMAIN_NAME}`) && Path(`/ping`)"
      traefik.http.routers.traefik-ping.entrypoints: "websecure"
      traefik.http.routers.traefik-ping.tls: "true"
      traefik.http.routers.traefik-ping.middlewares: "no-forward-auth@file"
      traefik.http.services.traefik-ping.loadbalancer.server.port: "8080"
      # Monitor config
      kuma.{{container_name}}.http.name: "Traefik (Reverse Proxy)"
      kuma.{{container_name}}.http.url: "${TRAEFIK_MONITOR_URL:?[traefik] Monitor URL missing}"
    networks:
      - home_pi
    ports:
      - "443:443"
      - "5555:8080"
    restart: unless-stopped
    user: "${PUID_ROOT:?User ID missing}:${PGID_ROOT:?Group ID missing}"
    volumes:
      - "${DOCKER_SOCKET}:/var/run/docker.sock"
      - ./docker/traefik/config/traefik.yml:/config/traefik.yml:ro
      - ./docker/traefik/config/dynamic_configs/:/dynamic_configs:ro
      - ./docker/traefik/certs/cloudflare_origin_cert.pem:/etc/certs/cloudflare_origin_cert.pem:ro
      - ./docker/traefik/certs/cloudflare_origin_key.pem:/etc/certs/cloudflare_origin_key.pem:ro

  # Reverse proxy connector for external hosts
  traefik-provider:
    image: redis:8.2.1-alpine  # https://hub.docker.com/_/redis
    platform: linux/arm64
    container_name: traefik-provider
    hostname: traefik-provider
    command: --save 60 1 --loglevel warning
    healthcheck:
      interval: 30s
      retries: 3
      start_period: 30s
      test: [ "CMD", "redis-cli", "ping" ]
      timeout: 5s
    networks:
      - home_pi
    ports:
      - "6379:6379"
    restart: unless-stopped
    user: "${PUID_ROOT:?User ID missing}:${PGID_ROOT:?Group ID missing}"
    volumes:
      # Persistent volumes
      - ./storage/traefik-provider/data/:/data

  # Docker log viewer
  dozzle:
    image: amir20/dozzle:v8.13.12  # https://github.com/amir20/dozzle/releases
    platform: linux/arm64
    container_name: dozzle
    hostname: dozzle
    environment:
      # Base config
      TZ: "${TIMEZONE:?Timezone missing}"
      # Authentication config
      DOZZLE_AUTH_PROVIDER: "forward-proxy"
      # Hosts config
      DOZZLE_HOSTNAME: "${REVERSE_PROXY_HOST_NAME:?[dozzle] Hostname missing}"
      DOZZLE_REMOTE_HOST: "${DOZZLE_REMOTE_HOSTS}"
      # UI config
      DOZZLE_ENABLE_ACTIONS: "true"
      DOZZLE_NO_ANALYTICS: "true"
    healthcheck:
      interval: 30s
      retries: 3
      start_period: 30s
      test: [ "CMD", "/dozzle", "healthcheck" ]
      timeout: 5s
    labels:
      # Reverse proxy config
      traefik.enable: "true"
      traefik.http.routers.dozzle.rule: "Host(`log.${DOMAIN_NAME}`)"
      traefik.http.routers.dozzle.entrypoints: "websecure"
      traefik.http.routers.dozzle.tls: "true"
      traefik.http.routers.dozzle.middlewares: "authelia-forward-auth@file"
      # Redirect from 'logs' subdomain to 'log' subdomain
      traefik.http.routers.dozzle-redirect.rule: "Host(`logs.${DOMAIN_NAME}`)"
      traefik.http.routers.dozzle-redirect.entryPoints: "websecure"
      traefik.http.routers.dozzle-redirect.tls: "true"
      traefik.http.routers.dozzle-redirect.middlewares: "dozzle-redirect-regex"
      traefik.http.middlewares.dozzle-redirect-regex.redirectregex.regex: "^https://logs.${DOMAIN_NAME}(.*)"
      traefik.http.middlewares.dozzle-redirect-regex.redirectregex.replacement: "https://log.${DOMAIN_NAME}$${1}"
      traefik.http.middlewares.dozzle-redirect-regex.redirectregex.permanent: "true"
      # Monitor config
      kuma.{{container_name}}.http.name: "Dozzle (Docker Log Viewer)"
      kuma.{{container_name}}.http.url: "${DOZZLE_MONITOR_URL:?[dozzle] Monitor URL missing}"
    networks:
      - home_pi
    ports:
      - "5005:8080"
    restart: unless-stopped
    user: "${PUID_ROOT:?User ID missing}:${PGID_ROOT:?Group ID missing}"
    volumes:
      # Docker socket mount from host system
      - "${DOCKER_SOCKET}:/var/run/docker.sock"

  # Homepage
  homarr:
    image: ghcr.io/homarr-labs/homarr:v1.37.0  # https://github.com/homarr-labs/homarr/releases
    platform: linux/arm64
    container_name: homarr
    hostname: homarr
    environment:
      # Base config
      TZ: "${TIMEZONE:?Timezone missing}"
      # Authentication config
      # AUTH_OIDC_ADMIN_GROUP: "homarr_admins" # Configured in UI at start-up
      AUTH_LOGOUT_REDIRECT_URL: "${HOMARR_OIDC_LOGOUT_URI:?[homarr] OIDC logout URL missing}"
      AUTH_PROVIDERS: "oidc"
      AUTH_OIDC_AUTO_LOGIN: "true"
      AUTH_OIDC_CLIENT_ID: "${HOMARR_OIDC_CLIENT_ID:?[homarr] OIDC client ID missing}"
      AUTH_OIDC_CLIENT_NAME: "Authelia"
      AUTH_OIDC_CLIENT_SECRET: "${HOMARR_OIDC_CLIENT_SECRET:?[homarr] OIDC client secret missing}"
      AUTH_OIDC_FORCE_USERINFO: "true"
      AUTH_OIDC_GROUPS_ATTRIBUTE: "groups"
      AUTH_OIDC_ISSUER: "${HOMARR_OIDC_URI:?[homarr] OIDC URI missing}"
      AUTH_OIDC_SCOPE_OVERWRITE: "openid email profile groups"
      # For debugging auth issues:
      # AUTH_PROVIDERS: "oidc,credentials"
      # AUTH_OIDC_AUTO_LOGIN: "false"
      BASE_URL: "${HOMARR_BASE_URL:?[homarr] Base URL missing}"
      NEXTAUTH_URL: "${HOMARR_BASE_URL:?[homarr] Base URL missing}"
      # Secrets config
      SECRET_ENCRYPTION_KEY: "${HOMARR_SECRET_KEY:?[homarr] Secret key missing}"
    healthcheck:
      interval: 30s
      retries: 3
      start_period: 30s
      test: [ "CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://127.0.0.1:7575/api/health/live" ]
      timeout: 5s
    labels:
      # Reverse proxy config
      traefik.enable: "true"
      traefik.http.routers.homarr.rule: "Host(`${DOMAIN_NAME}`)"
      traefik.http.routers.homarr.entrypoints: "websecure"
      traefik.http.routers.homarr.tls: "true"
      traefik.http.routers.homarr.middlewares: "no-forward-auth@file"
      # Monitor config
      kuma.{{container_name}}.http.name: "Homarr (HomePage)"
      kuma.{{container_name}}.http.url: "${HOMARR_MONITOR_URL:?[homarr] Monitor URL missing}"
    networks:
      - home_pi
    ports:
      - "5000:7575"
    restart: unless-stopped
    user: "${PUID_ROOT:?User ID missing}:${PGID_ROOT:?Group ID missing}"
    volumes:
      # Persistent volumes
      - ./storage/homarr/data:/appdata

  # Identity platform (authentication and SSO)
  authelia:
    image: authelia/authelia:4.39.9  # https://github.com/authelia/authelia/releases
    platform: linux/arm64
    container_name: authelia
    hostname: authelia
    depends_on:
      authelia-cache:
        condition: service_healthy
      authelia-db:
        condition: service_healthy
      lldap:
        condition: service_healthy
    environment:
      # Logging config
      LOG_LEVEL: "INFO"
      # LDAP configuration
      ADMIN_PASSWORD: "${LLDAP_ADMIN_PASSWORD:?[authelia] Missing LDAP admin password}"
      BASE_DN: "${LLDAP_BASE_DN:?[authelia] Missing base LDAP DN}"
      LDAP_HOSTNAME: "lldap"
      LDAP_PORT: "${LLDAP_PORT:?[authelia] Missing LDAP port}"
      LDAP_IMPL: "lldap"
      # Basic configuration
      DOMAIN_NAME: "${DOMAIN_NAME:?[authelia] Missing domain name}"
      SERVER_PORT: "${AUTHELIA_INTERNAL_PORT:-9091}"
      # Session configuration
      REDIS_HOSTNAME: "authelia-cache"
      REDIS_PORT: "6379"
      # SMTP configuration
      SMTP_EMAIL_ADDRESS: "${SMTP_EMAIL_ADDRESS:?[authelia] Missing SMTP email address}"
      SMTP_APP_PASSWORD: "${SMTP_APP_PASSWORD:?[authelia] Missing SMTP app password}"
      SMTP_FROM_EMAIL_ADDRESS: "${SMTP_EMAIL_ADDRESS:?[authelia] Missing SMTP from email address}"
      # Backend configuration
      BACKEND_HOSTNAME: "authelia-db"
      BACKEND_NAME: "${AUTHELIA_DB_NAME:?[authelia] Database name missing}"
      BACKEND_PASSWORD: "${AUTHELIA_DB_PASSWORD:?[authelia] Database password missing}"
      BACKEND_PORT: "${AUTHELIA_DB_PORT:?[authelia] Database port missing}"
      BACKEND_USER: "${AUTHELIA_DB_USER?[authelia] Database user missing}"
      # Enable the template filter for substitutions in configuration.yml
      X_AUTHELIA_CONFIG_FILTERS: "template"
      # 2FA config
      TOTP_DISABLE: "true"
      WEBAUTHN_DISABLE: "true"
      # Secrets
      RESET_PASSWORD_SECRET: "${AUTHELIA_RESET_PASSWORD_SECRET:?[authelia] Missing reset password secret}"
      OIDC_HMAC_SECRET: "${AUTHELIA_OIDC_HMAC_SECRET:?[authelia] Missing OIDC HMAC secret}"
      SESSION_SECRET: "${AUTHELIA_SESSION_SECRET:?[authelia] Missing session secret}"
      STORAGE_ENCRYPTION_KEY: "${AUTHELIA_STORAGE_ENCRYPTION_KEY:?[authelia] Missing storage encryption key}"
    labels:
      # Reverse proxy config for Authelia UI and OIDC
      traefik.enable: "true"
      traefik.http.routers.authelia.rule: "Host(`auth.${DOMAIN_NAME}`)"
      traefik.http.routers.authelia.entrypoints: "websecure"
      traefik.http.routers.authelia.tls: "true"
      # Monitor config
      kuma.{{container_name}}.http.name: "Authelia (Identity Platform)"
      kuma.{{container_name}}.http.url: "${AUTHELIA_MONITOR_URL:?[authelia] Monitor URL missing}"
    networks:
      - home_pi
    ports:
      - "${AUTHELIA_EXTERNAL_PORT:-9091}:${AUTHELIA_INTERNAL_PORT:-9091}"
    restart: unless-stopped
    user: "${PUID_NON_ROOT:?User ID missing}:${PGID_NON_ROOT:?Group ID missing}"
    volumes:
      # Persistent volumes
      - ./docker/authelia/config/configuration.yml:/config/configuration.yml:ro
      - ./docker/authelia/certs/authelia_private_key.pem:/etc/certs/authelia_private_key.pem:ro
      - ./storage/authelia/assets/:/assets
      - ./storage/authelia/data:/var/lib/authelia

  authelia-db:
    image: postgres:17.6-alpine  # https://hub.docker.com/_/postgres
    platform: linux/arm64
    container_name: authelia-db
    hostname: authelia-db
    environment:
      # Base config
      TIMEZONE: "${TIMEZONE:?Timezone missing}"
      # Database config
      PGPORT: "${AUTHELIA_DB_PORT:?[authentik-db] Database port missing}"
      POSTGRES_DB: "${AUTHELIA_DB_NAME:?[authentik-db] Database name missing}"
      POSTGRES_PASSWORD: "${AUTHELIA_DB_PASSWORD:?[authentik-db] Database password missing}"
      POSTGRES_USER: "${AUTHELIA_DB_USER?[authentik-db] Database user missing}"
      # Next 3 variables needed to avoid "FATAL role 'root' does not exist" error
      PGDATABASE: "${AUTHELIA_DB_NAME:?[authentik-db] Database name missing}"
      PGPASSWORD: "${AUTHELIA_DB_PASSWORD:?[authentik-db] Database password missing}"
      PGUSER: "${AUTHELIA_DB_USER?[authentik-db] Database user missing}"
    expose:
      - "${AUTHELIA_DB_PORT:?[authentik-db] Database port missing}"
    healthcheck:
      interval: 30s
      retries: 3
      start_period: 30s
      test: [ "CMD", "pg_isready", "--host", "localhost" ]
      timeout: 5s
    networks:
      - home_pi
    restart: unless-stopped
    user: "${PUID_ROOT:?User ID missing}:${PGID_ROOT:?Group ID missing}"
    volumes:
      # Persistent volumes
      - ./storage/authelia-db/data/:/var/lib/postgresql/data

  authelia-cache:
    image: redis:8.2.1-alpine  # https://hub.docker.com/_/redis
    platform: linux/arm64
    container_name: authelia-cache
    hostname: authelia-cache
    command: --save 60 1 --loglevel warning
    expose:
      - 6379
    healthcheck:
      interval: 30s
      retries: 3
      start_period: 30s
      test: [ "CMD", "redis-cli", "ping" ]
      timeout: 5s
    networks:
      - home_pi
    restart: unless-stopped
    user: "${PUID_ROOT:?User ID missing}:${PGID_ROOT:?Group ID missing}"
    volumes:
      # Persistent volumes
      - ./storage/authelia-cache/data/:/data

  # LDAP User-store
  lldap:
    image: lldap/lldap:2025-08-31-alpine  # https://hub.docker.com/r/lldap/lldap/tags?name=-alpine | https://github.com/lldap/lldap/releases
    platform: linux/arm64
    container_name: lldap
    hostname: lldap
    depends_on:
      lldap-db:
        condition: service_healthy
    environment:
      # Base config
      TZ: "${TIMEZONE:?Timezone missing}"
      # LDAP config
      LLDAP_DATABASE_URL: "postgres://${LLDAP_DB_USER:?[lldap] Missing DB user}:${LLDAP_DB_PASSWORD:?[lldap] Missing DB password}@lldap-db:${LLDAP_DB_PORT:?[lldap] Missing DB port}/${LLDAP_DB_NAME:?[lldap] Missing DB name}"
      LLDAP_LDAP_BASE_DN: "${LLDAP_BASE_DN:?[lldap] Missing base LDAP DN}"
      LLDAP_LDAP_USER_PASS: "${LLDAP_ADMIN_PASSWORD:?[lldap] Missing LDAP admin password}"
      # Secrets
      LLDAP_JWT_SECRET: "${LLDAP_JWT_SECRET:?[lldap] Missing LDAP JWT secret}"
      LLDAP_KEY_FILE: ""  # Empty on purpose so KEY_SEED is used
      LLDAP_KEY_SEED: "${LLDAP_KEY_SEED:?[lldap] Missing LDAP key seed}"
    labels:
      # Reverse proxy config for Authelia UI and OIDC
      traefik.enable: "true"
      traefik.http.routers.lldap.rule: "Host(`lldap.${DOMAIN_NAME}`)"
      traefik.http.routers.lldap.entrypoints: "websecure"
      traefik.http.routers.lldap.tls: "true"
      traefik.http.routers.lldap.service: "lldap"
      traefik.http.services.lldap.loadbalancer.server.port: "17170"
      # Monitor config
      kuma.{{container_name}}.http.name: "LLDAP (LDAP Server)"
      kuma.{{container_name}}.http.url: "${LLDAP_MONITOR_URL:?[lldap] Monitor URL missing}"
    networks:
      - home_pi
    ports:
      - "3890:3890"
      - "17170:17170"
    restart: unless-stopped
    user: "${PUID_ROOT:?User ID missing}:${PGID_ROOT:?Group ID missing}"
    volumes:
      # Persistent volumes
      - ./storage/lldap/:/data

  lldap-db:
    image: postgres:17.6-alpine  # https://hub.docker.com/_/postgres
    platform: linux/arm64
    container_name: lldap-db
    hostname: lldap-db
    environment:
      # Base config
      TIMEZONE: "${TIMEZONE:?Timezone missing}"
      # Database config
      PGPORT: "${LLDAP_DB_PORT:?[lldap-db] Missing DB port}"
      POSTGRES_DB: "${LLDAP_DB_NAME:?[lldap-db] Missing DB name}"
      POSTGRES_PASSWORD: "${LLDAP_DB_PASSWORD:?[lldap-db] Missing DB password}"
      POSTGRES_USER: "${LLDAP_DB_USER:?[lldap-db] Missing DB user}"
      # Next 3 variables needed to avoid "FATAL role 'root' does not exist" error
      PGDATABASE: "${LLDAP_DB_NAME:?[lldap-db] Missing DB name}"
      PGPASSWORD: "${LLDAP_DB_PASSWORD:?[lldap-db] Missing DB password}"
      PGUSER: "${LLDAP_DB_USER:?[lldap-db] Missing DB user}"
    expose:
      - "${LLDAP_DB_PORT:?[lldap-db] Missing DB port}"
    healthcheck:
      interval: 30s
      retries: 3
      start_period: 30s
      test: [ "CMD", "pg_isready", "--host", "localhost" ]
      timeout: 5s
    networks:
      - home_pi
    restart: unless-stopped
    user: "${PUID_ROOT:?User ID missing}:${PGID_ROOT:?Group ID missing}"
    volumes:
      - ./storage/lldap-db/data/:/var/lib/postgresql/data

  # Monitoring and alerting
  uptime-kuma:
    image: louislam/uptime-kuma:1.23.16-alpine  # https://github.com/louislam/uptime-kuma/releases
    platform: linux/arm64
    container_name: uptime-kuma
    hostname: uptime-kuma
    environment:
      # Base config
      TZ: "${TIMEZONE:?Timezone missing}"
      # Port config
      UPTIME_KUMA_PORT: "${UPTIME_KUMA_PORT:?[uptime-kuma] Port missing}"
    labels:
      # Reverse proxy config
      traefik.enable: "true"
      traefik.http.routers.uptime-kuma.rule: "Host(`monitor.${DOMAIN_NAME}`)"
      traefik.http.routers.uptime-kuma.entrypoints: "websecure"
      traefik.http.routers.uptime-kuma.tls: "true"
      traefik.http.routers.uptime-kuma.middlewares: "authelia-forward-auth@file"
      traefik.http.services.uptime-kuma.loadbalancer.server.port: "${UPTIME_KUMA_PORT:?[uptime-kuma] Port missing}"
      # Redirect from 'status' subdomain to '/status/main' path on 'monitor' subdomain
      traefik.http.routers.uptime-kuma-redirect.rule: "Host(`status.${DOMAIN_NAME}`)"
      traefik.http.routers.uptime-kuma-redirect.entryPoints: "websecure"
      traefik.http.routers.uptime-kuma-redirect.tls: "true"
      traefik.http.routers.uptime-kuma-redirect.middlewares: "redirect-to-status-page"
      traefik.http.middlewares.redirect-to-status-page.redirectRegex.regex: "^https?://status.${DOMAIN_NAME}(.*)"
      traefik.http.middlewares.redirect-to-status-page.redirectRegex.replacement: "https://monitor.${DOMAIN_NAME}/status/main"
      traefik.http.middlewares.redirect-to-status-page.redirectRegex.permanent: "true"
      # Monitor config
      kuma.{{container_name}}.http.name: "Uptime Kuma (Status Page)"
      kuma.{{container_name}}.http.url: "${UPTIME_KUMA_MONITOR_URL:?[uptime-kuma] Monitor URL missing}"
    networks:
      - home_pi
    ports:
      - "${UPTIME_KUMA_PORT:?[uptime-kuma] Port missing}:${UPTIME_KUMA_PORT:?[uptime-kuma] Port missing}"
    restart: unless-stopped
    user: "${PUID_ROOT:?User ID missing}:${PGID_ROOT:?Group ID missing}"
    volumes:
      # Persistent volumes
      - ./storage/uptime-kuma/:/app/data

  # WiFi/LAN network scanner
  netalert:
    build:
      context: .
      dockerfile: docker/netalert/Dockerfile
      args:
        # Docker image versions
        NETALERT_VERSION: "25.8.6" # https://registry.hub.docker.com/r/jokobsk/netalertx/tags | https://github.com/jokob-sk/NetAlertX/releases
        # Build arguments
        # Internal config
        TIMEZONE: "${TIMEZONE:?Timezone missing}"
        # Notification config
        DISCORD_NEW_NETWORK_DEVICE_WEBHOOK_URL: "${DISCORD_NEW_NETWORK_DEVICE_WEBHOOK_URL:?[netalert] Discord webhook URL missing}"
        # Scanning config
        NETALERT_PRIMARY_SUBNET: "${NETALERT_PRIMARY_SUBNET:?[netalert] Primary subnet missing}"
        # UI config
        DOMAIN_NAME: "${DOMAIN_NAME:?[netalert] Domain name missing}"
    platform: linux/arm64
    container_name: netalert
    hostname: netalert
    environment:
      # Base config
      TZ: "${TIMEZONE:?Timezone missing}"
      # UI config
      PORT: "${NETALERT_UI_PORT:?[netalert] Port missing}"
    labels:
      # Reverse proxy config
      #   Because NetAlert uses `network_mode: host`, we cannot use docker labels to configure the service with Traefik
      #   Instead, there is a dynamic config file netalert.yml to define this, and the home network IP address of this
      #   service is passed in to the Traefik as an environment variable
      #   We can configure the redirects here, but it makes sense to leave it all in the config file instead.
      # Monitor config
      kuma.{{container_name}}.http.name: "NetAlert (Network Scanner)"
      kuma.{{container_name}}.http.url: "${NETALERT_MONITOR_URL:?[netalert] Monitor URL missing}"
      kuma.{{container_name}}.http.max_redirects: "0"
    network_mode: host # Cannot be run in bridged mode, as it needs visibility of the network
    expose: # When using 'host' network_mode, published ports using 'ports' are discarded. We just 'expose' the new port in case we change the default
      - "${NETALERT_UI_PORT:?[netalert] Port missing}"
    restart: unless-stopped
    user: "${PUID_ROOT:?User ID missing}:${PGID_ROOT:?Group ID missing}"
    volumes:
      # Persistent volumes
      - ./storage/netalert/config/:/app/config
      - ./storage/netalert/db/:/app/db

  # Automated SpeedTest runner with graphs
  speedtest:
    image: linuxserver/speedtest-tracker:1.6.6  # https://github.com/alexjustesen/speedtest-tracker/releases
    platform: linux/arm64
    container_name: speedtest
    hostname: speedtest
    depends_on:
      speedtest-db:
        condition: service_healthy
    environment:
      # Base config
      APP_TIMEZONE: "${TIMEZONE:?Timezone missing}"
      PGID: "${PGID_NON_ROOT:?Group ID missing}"
      PUID: "${PUID_NON_ROOT:?User ID missing}"
      # Reporting config
      APP_KEY: "${SPEEDTEST_APP_KEY:?[speedtest] App key missing}"
      APP_URL: "${SPEEDTEST_APP_URL:?[speedtest] App URL missing}"
      PRUNE_RESULTS_OLDER_THAN: "30" # Days
      SPEEDTEST_SCHEDULE: "${SPEEDTEST_SCHEDULE?:[speedtest] Cron schedule missing}"
      SPEEDTEST_SERVERS: "${SPEEDTEST_SERVERS}"
      # Database config
      DB_CONNECTION: "pgsql"
      DB_HOST: "speedtest-db"
      DB_DATABASE: "${SPEEDTEST_DB_NAME:?[speedtest] Database name missing}"
      DB_PASSWORD: "${SPEEDTEST_DB_PASSWORD:?[speedtest] Database password missing}"
      DB_PORT: "${SPEEDTEST_DB_PORT:?[speedtest] Database port missing}"
      DB_USERNAME: "${SPEEDTEST_DB_USER:?[speedtest] Database user missing}"
    healthcheck:
      interval: 30s
      retries: 3
      start_period: 30s
      test: [ "CMD", "wget", "--no-verbose", "--tries=1", "-O", "-", "http://127.0.0.1:80/api/speedtest/latest" ]
      timeout: 5s
    labels:
      # Reverse proxy config
      traefik.enable: "true"
      traefik.http.routers.speedtest.rule: "Host(`speedtest.${DOMAIN_NAME}`)"
      traefik.http.routers.speedtest.entrypoints: "websecure"
      traefik.http.routers.speedtest.tls: "true"
      traefik.http.routers.speedtest.middlewares: "no-forward-auth@file"
      # Monitor config
      kuma.{{container_name}}.http.name: "SpeedTest Tracker (Network Speed Test)"
      kuma.{{container_name}}.http.url: "${SPEEDTEST_MONITOR_URL:?[speedtest] Monitor URL missing}"
    networks:
      - home_pi
    ports:
      - "5004:80"
    restart: unless-stopped
    volumes:
      - ./storage/speedtest/config/:/config

  # Database for SpeedTest
  speedtest-db:
    image: postgres:17.6-alpine  # https://hub.docker.com/_/postgres
    platform: linux/arm64
    container_name: speedtest-db
    hostname: speedtest-db
    environment:
      # Base config
      TIMEZONE: "${TIMEZONE:?Timezone missing}"
      # Database config
      PGPORT: "${SPEEDTEST_DB_PORT:?[speedtest-db] Database port missing}"
      POSTGRES_DB: "${SPEEDTEST_DB_NAME:?[speedtest-db] Database name missing}"
      POSTGRES_PASSWORD: "${SPEEDTEST_DB_PASSWORD:?[speedtest-db] Database password missing}"
      POSTGRES_USER: "${SPEEDTEST_DB_USER:?[speedtest-db] Database user missing}"
      # Next 3 variables needed to avoid "FATAL role 'root' does not exist" error
      PGDATABASE: "${SPEEDTEST_DB_NAME:?[speedtest-db] Database name missing}"
      PGPASSWORD: "${SPEEDTEST_DB_PASSWORD:?[speedtest-db] Database password missing}"
      PGUSER: "${SPEEDTEST_DB_USER:?[speedtest-db] Database user missing}"
    expose:
      - 5432
    healthcheck:
      interval: 30s
      retries: 3
      start_period: 30s
      test: [ "CMD", "pg_isready", "--host", "localhost" ]
      timeout: 5s
    networks:
      - home_pi
    restart: unless-stopped
    user: "${PUID_ROOT:?User ID missing}:${PGID_ROOT:?Group ID missing}"
    volumes:
      - ./storage/speedtest-db/data/:/var/lib/postgresql/data

  # Tool to import/export played state of media in Emby, instead of using Trakt
  watchstate:
    image: ghcr.io/arabcoders/watchstate:v0.4.1  # https://github.com/arabcoders/watchstate/releases
    platform: linux/arm64
    container_name: watchstate
    hostname: watchstate
    environment:
      # Base config
      TIMEZONE: "${TIMEZONE:?Timezone missing}"
      # Logging config
      WS_LOGGER_FILE_ENABLE: "false" # Disabled due to logs being too large, and not needing them
    healthcheck: # Default healthcheck fails at the start, before eventually passing which breaks scripts
      interval: 30s
      retries: 3
      start_period: 30s
      test: [ "CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://127.0.0.1:8080/v1/api/system/healthcheck" ]
      timeout: 5s
    networks:
      - home_pi
    ports:
      - "5006:8080"
    restart: unless-stopped
    user: "${PUID_ROOT:?User ID missing}:${PGID_ROOT:?Group ID missing}"
    volumes:
      - ./storage/watchstate/data:/config
