networks:
  home:
    name: home
    driver: bridge

services:

  # Media Server
  emby:
    image: emby/embyserver:4.9.0.41
    container_name: emby
    hostname: emby
    environment:
      # Base config
      TZ: "${TIMEZONE:?Timezone missing}"
    healthcheck:
      interval: 30s
      retries: 3
      start_period: 30s
      test: [ "CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://127.0.0.1:8096/emby/system/info/public?format=json" ]
      timeout: 5s
    labels:
      # Reverse proxy config
      traefik.enable: "true"
      traefik.http.routers.emby.rule: "Host(`emby.${DOMAIN_NAME}`)"
      traefik.http.routers.emby.entrypoints: "websecure"
      traefik.http.routers.emby.tls: "true"
      traefik.http.routers.emby.middlewares: "no-forward-auth@file"
      traefik.http.services.emby.loadbalancer.server.port: "8096"
      # Monitor config
      kuma.{{container_name}}.http.name: "Emby (Media Server)"
      kuma.{{container_name}}.http.url: "${EMBY_MONITOR_URL:?[emby] Monitor URL missing}"
      kuma.{{container_name}}.http.max_retries: "3"
      # TODO: Below are not working
      #      kuma.{{container_name}}.http.notification_id_list: '{ "1": true }'
      #      kuma.{{container_name}}.http.tags: '[{"tag_id": 2}]'
    networks:
      - home
    ports:
      - "8096:8096"
    restart: unless-stopped
    user: "${PUID_ROOT:?User ID missing}:${PGID_ROOT:?Group ID missing}"
    volumes:
      # Volume mounts from host system
      - "${AUDIO_DIRECTORY}:/media/audio"
      - "${BOOKS_DIRECTORY}:/media/books"
      - "${MOVIE_DIRECTORY}:/media/movies"
      - "${RECORDINGS_DIRECTORY}:/media/recordings"
      - "${TV1_DIRECTORY}:/media/tv1"
      - "${TV2_DIRECTORY}:/media/tv2"
      - "${TV_TRASH_DIRECTORY}:/media/trash"
      - "${TV_KIDS_DIRECTORY}:/media/kids"
      # Persistent volumes
      - ./storage/emby/config/:/config
      - ./storage/emby/backup/:/backup

  # Traefik connector
  traefik-kop:
    image: ghcr.io/jittering/traefik-kop:0.14
    container_name: traefik-kop
    hostname: traefik-kop
    environment:
      BIND_IP: "${MAIN_HOST_IP}" # IP address of this host
      REDIS_ADDR: "${REVERSE_PROXY_HOST_IP}:6379" # Port of the redis instance on the reverse proxy host
    healthcheck:
      interval: 30s
      retries: 3
      start_period: 30s
      test: [ "CMD", "/traefik-kop", "-V" ]
      timeout: 5s
    networks:
      - home
    restart: unless-stopped
    volumes:
      - "${DOCKER_SOCKET}:/var/run/docker.sock"

  # Movie manager
  radarr:
    image: linuxserver/radarr:nightly-version-5.20.0.9735
    container_name: radarr
    hostname: radarr
    depends_on:
      qbittorrent:
        condition: service_healthy
    deploy:
      resources:
        limits:
          cpus: "1"
          memory: "1024M"
    environment:
      # Base config
      TZ: "${TIMEZONE:?Timezone missing}"
    healthcheck:
      interval: 30s
      retries: 3
      start_period: 30s
      test: [ "CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://127.0.0.1:3000/ping" ]
      timeout: 5s
    labels:
      # Reverse proxy config
      traefik.enable: "true"
      traefik.http.routers.radarr.rule: "Host(`radarr.${DOMAIN_NAME}`)"
      traefik.http.routers.radarr.entrypoints: "websecure"
      traefik.http.routers.radarr.tls: "true"
      traefik.http.routers.radarr.middlewares: "authentik-forward-auth@file"
      traefik.http.services.radarr.loadbalancer.server.port: "4000"
      # Monitor config
      kuma.{{container_name}}.http.name: "Radarr (Movie Manager)"
      kuma.{{container_name}}.http.url: "${RADARR_MONITOR_URL:?[radarr] Monitor URL missing}"
    networks:
      - home
    ports:
      - "4000:3000"
    restart: unless-stopped
    user: "${PUID_ROOT:?User ID missing}:${PGID_ROOT:?Group ID missing}"
    volumes:
      # Volume mounts from host system
      - "${DOWNLOADS_DIRECTORY}:/downloads"
      - "${MOVIE_DIRECTORY}:/movies"
      # Persistent volumes
      - ./storage/radarr/:/config

  # TV show manager
  sonarr:
    image: linuxserver/sonarr:develop-version-4.0.13.2933
    container_name: sonarr
    hostname: sonarr
    depends_on:
      qbittorrent:
        condition: service_healthy
    deploy:
      resources:
        limits:
          cpus: "2"
          memory: "2048M"
    environment:
      # Base config
      TZ: "${TIMEZONE:?Timezone missing}"
    healthcheck:
      interval: 30s
      retries: 3
      start_period: 30s
      test: [ "CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://127.0.0.1:3001/ping" ]
      timeout: 5s
    labels:
      # Reverse proxy config
      traefik.enable: "true"
      traefik.http.routers.sonarr.rule: "Host(`sonarr.${DOMAIN_NAME}`)"
      traefik.http.routers.sonarr.entrypoints: "websecure"
      traefik.http.routers.sonarr.tls: "true"
      traefik.http.routers.sonarr.middlewares: "authentik-forward-auth@file"
      traefik.http.services.sonarr.loadbalancer.server.port: "4001"
      # Monitor config
      kuma.{{container_name}}.http.name: "Sonarr (TV Manager)"
      kuma.{{container_name}}.http.url: "${SONARR_MONITOR_URL:?[sonarr] Monitor URL missing}"
    networks:
      - home
    ports:
      - "4001:3001"
    restart: unless-stopped
    user: "${PUID_ROOT:?User ID missing}:${PGID_ROOT:?Group ID missing}"
    volumes:
      # Volume mounts from host system
      - "${DOWNLOADS_DIRECTORY}:/downloads"
      - "${TV1_DIRECTORY}:/tv1"
      - "${TV2_DIRECTORY}:/tv2"
      - "${TV_TRASH_DIRECTORY}:/trash"
      - "${TV_KIDS_DIRECTORY}:/kids"
      # Persistent volumes
      - ./storage/sonarr/:/config

  # Quality profile manager for radarr/sonarr
  recyclarr:
    image: ghcr.io/recyclarr/recyclarr:7.4.1
    container_name: recyclarr
    hostname: recyclarr
    depends_on:
      radarr:
        condition: service_healthy
      sonarr:
        condition: service_healthy
    deploy:
      resources:
        limits:
          cpus: "1"
          memory: "1024M"
    environment:
      # Base config
      TZ: "${TIMEZONE:?Timezone missing}"
      # Scheduling config
      CRON_SCHEDULE: "@daily"
      # Radarr Config
      RADARR_API_KEY: "${RADARR_API_KEY}"
      RADARR_URL: "http://radarr:3000"
      # Sonarr Config
      SONARR_API_KEY: "${SONARR_API_KEY}"
      SONARR_URL: "http://sonarr:3001"
    healthcheck:
      interval: 30s
      retries: 3
      start_period: 30s
      test: [ "CMD", "recyclarr", "-v" ]
      timeout: 5s
    networks:
      - home
    restart: unless-stopped
    user: "${PUID_ROOT:?User ID missing}:${PGID_ROOT:?Group ID missing}"
    volumes:
      - ./storage/recyclarr:/config
      - ./docker/recyclarr/config/recyclarr.yml:/config/recyclarr.yml

  # Music manager
  lidarr:
    image: linuxserver/lidarr:nightly-version-2.10.1.4576
    container_name: lidarr
    hostname: lidarr
    depends_on:
      qbittorrent:
        condition: service_healthy
    deploy:
      resources:
        limits:
          cpus: "1"
          memory: "1024M"
    environment:
      # Base config
      TZ: "${TIMEZONE:?Timezone missing}"
    healthcheck:
      interval: 30s
      retries: 3
      start_period: 30s
      test: [ "CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://127.0.0.1:3002/ping" ]
      timeout: 5s
    labels:
      # Reverse proxy config
      traefik.enable: "true"
      traefik.http.routers.lidarr.rule: "Host(`lidarr.${DOMAIN_NAME}`)"
      traefik.http.routers.lidarr.entrypoints: "websecure"
      traefik.http.routers.lidarr.tls: "true"
      traefik.http.routers.lidarr.middlewares: "authentik-forward-auth@file"
      traefik.http.services.lidarr.loadbalancer.server.port: "4002"
      # Monitor config
      kuma.{{container_name}}.http.name: "Lidarr (Music Manager)"
      kuma.{{container_name}}.http.url: "${LIDARR_MONITOR_URL:?[lidarr] Monitor URL missing}"
    networks:
      - home
    ports:
      - "4002:3002"
    restart: unless-stopped
    user: "${PUID_ROOT:?User ID missing}:${PGID_ROOT:?Group ID missing}"
    volumes:
      # Volume mounts from host system
      - "${DOWNLOADS_DIRECTORY}:/downloads"
      - "${MUSIC_DIRECTORY}:/music"
      # Persistent volumes
      - ./storage/lidarr/:/config

  # Music Player
  navidrome:
    image: deluan/navidrome:0.54.5
    container_name: navidrome
    hostname: navidrome
    deploy:
      resources:
        limits:
          cpus: "1"
          memory: "1024M"
    environment:
      # Base config
      TZ: "${TIMEZONE:?Timezone missing}"
      # Authentication config
      ND_REVERSEPROXYUSERHEADER: "X-Authentik-Username"
      ND_REVERSEPROXYWHITELIST: "0.0.0.0/0"
      # External services config
      ND_LASTFM_APIKEY: "${LASTFM_API_KEY:?[navidrome] LastFM API key missing}"
      ND_LASTFM_SECRET: "${LASTFM_SECRET:?[navidrome] LastFM secret missing}"
      ND_SPOTIFY_ID: "${SPOTIFY_CLIENT_ID:?[navidrome] Spotify Client ID missing}"
      ND_SPOTIFY_SECRET: "${SPOTIFY_CLIENT_SECRET:?[navidrome] Spotify Client secret missing}"
      # Scheduling config
      ND_SCANSCHEDULE: "@every 24h"
      # Metadata config
      ND_ARTISTARTPRIORITY: "artist.*, external"
      ND_COVERARTPRIORITY: "album.*, embedded, external"
      # Logging config
      ND_LOGLEVEL: "info"
      # System config
      ND_ENABLEINSIGHTSCOLLECTOR: "false"
    labels:
      # Reverse proxy config
      traefik.enable: "true"
      traefik.http.routers.navidrome.rule: "Host(`music.${DOMAIN_NAME}`)"
      traefik.http.routers.navidrome.entrypoints: "websecure"
      traefik.http.routers.navidrome.tls: "true"
      traefik.http.routers.navidrome.middlewares: "authentik-forward-auth@file"
      traefik.http.services.navidrome.loadbalancer.server.port: "4003"
      # Monitor config
      kuma.{{container_name}}.http.name: "Navidrome (Music Player)"
      kuma.{{container_name}}.http.url: "${NAVIDROME_MONITOR_URL:?[navidrome] Monitor URL missing}"
    networks:
      - home
    ports:
      - "4003:4533"
    restart: unless-stopped
    user: "${PUID_ROOT:?User ID missing}:${PGID_ROOT:?Group ID missing}"
    volumes:
      # Volume mounts from host system
      - "${MUSIC_DIRECTORY}:/music:ro"
      # Persistent volumes
      - ./storage/navidrome/:/data

  # eBook manager
  readarr:
    image: linuxserver/readarr:nightly-version-0.4.11.2747
    container_name: readarr
    hostname: readarr
    depends_on:
      qbittorrent:
        condition: service_healthy
    deploy:
      resources:
        limits:
          cpus: "1"
          memory: "512M"
    environment:
      # Base config
      TZ: "${TIMEZONE:?Timezone missing}"
    healthcheck:
      interval: 30s
      retries: 3
      start_period: 30s
      test: [ "CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://127.0.0.1:3004/ping" ]
      timeout: 5s
    labels:
      # Reverse proxy config
      traefik.enable: "true"
      traefik.http.routers.readarr.rule: "Host(`readarr.${DOMAIN_NAME}`)"
      traefik.http.routers.readarr.entrypoints: "websecure"
      traefik.http.routers.readarr.tls: "true"
      traefik.http.routers.readarr.middlewares: "authentik-forward-auth@file"
      traefik.http.services.readarr.loadbalancer.server.port: "4004"
      # Monitor config
      kuma.{{container_name}}.http.name: "Readarr (eBook Manager)"
      kuma.{{container_name}}.http.url: "${READARR_MONITOR_URL:?[readarr] Monitor URL missing}"
    networks:
      - home
    ports:
      - "4004:3004"
    restart: unless-stopped
    user: "${PUID_ROOT:?User ID missing}:${PGID_ROOT:?Group ID missing}"
    volumes:
      # Volume mounts from host system
      - "${BOOKS_DIRECTORY}:/books"
      - "${DOWNLOADS_DIRECTORY}:/downloads"
      # Persistent volumes
      - ./storage/readarr/:/config

  # eBook & Audiobook management
  audiobookshelf:
    image: ghcr.io/advplyr/audiobookshelf:2.19.5
    container_name: audiobookshelf
    hostname: audiobookshelf
    deploy:
      resources:
        limits:
          cpus: "1"
          memory: "2048M"
    environment:
      # Base config
      TZ: "${TIMEZONE:?Timezone missing}"
    healthcheck:
      interval: 30s
      retries: 3
      start_period: 30s
      test: [ "CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://127.0.0.1:80/healthcheck" ]
      timeout: 5s
    labels:
      # Reverse proxy config
      traefik.enable: "true"
      traefik.http.routers.audiobookshelf.rule: "Host(`book.${DOMAIN_NAME}`)"
      traefik.http.routers.audiobookshelf.entrypoints: "websecure"
      traefik.http.routers.audiobookshelf.tls: "true"
      traefik.http.routers.audiobookshelf.middlewares: "no-forward-auth@file"
      traefik.http.services.audiobookshelf.loadbalancer.server.port: "4005"
      # Redirect from 'books' subdomain to 'book' subdomain
      traefik.http.routers.audiobookshelf-redirect.rule: "Host(`books.${DOMAIN_NAME}`)"
      traefik.http.routers.audiobookshelf-redirect.entryPoints: "websecure"
      traefik.http.routers.audiobookshelf-redirect.tls: "true"
      traefik.http.routers.audiobookshelf-redirect.middlewares: "audiobookshelf-redirect-regex"
      traefik.http.middlewares.audiobookshelf-redirect-regex.redirectregex.regex: "^https://books.${DOMAIN_NAME}(.*)"
      traefik.http.middlewares.audiobookshelf-redirect-regex.redirectregex.replacement: "https://book.${DOMAIN_NAME}$${1}"
      traefik.http.middlewares.audiobookshelf-redirect-regex.redirectregex.permanent: "true"
      # Monitor config
      kuma.{{container_name}}.http.name: "AudioBookShelf (eBook/Audiobook Reader)"
      kuma.{{container_name}}.http.url: "${AUDIOBOOKSHELF_MONITOR_URL:?[audiobookshelf] Monitor URL missing}"
    networks:
      - home
    ports:
      - "4005:80"
    restart: unless-stopped
    user: "${PUID_ROOT:?User ID missing}:${PGID_ROOT:?Group ID missing}"
    volumes:
      # Volume mounts from host system
      - "${BOOKS_DIRECTORY}:/ebooks:ro"
      # Persistent volumes
      - ./storage/audiobookshelf/config:/config
      - ./storage/audiobookshelf/metadata:/metadata

  # Downloaded archive extractor
  unpackerr:
    image: golift/unpackerr:0.14.5
    container_name: unpackerr
    hostname: unpackerr
    depends_on:
      lidarr:
        condition: service_healthy
      radarr:
        condition: service_healthy
      readarr:
        condition: service_healthy
      sonarr:
        condition: service_healthy
    deploy:
      resources:
        limits:
          cpus: "2"
          memory: "512M"
    environment:
      # Base config
      TZ: "${TIMEZONE:?Timezone missing}"
      # Lidarr config
      UN_LIDARR_0_API_KEY: "${LIDARR_API_KEY:?[unpackerr] Lidarr API key missing}"
      UN_LIDARR_0_PATHS_0: "/downloads/complete/lidarr"
      UN_LIDARR_0_PROTOCOLS: "torrent,usenet"
      UN_LIDARR_0_URL: "http://lidarr:3002"
      # Radarr config
      UN_RADARR_0_API_KEY: "${RADARR_API_KEY:?[unpackerr] Radarr API key missing}"
      UN_RADARR_0_PATHS_0: "/downloads/complete/radarr"
      UN_RADARR_0_PROTOCOLS: "torrent,usenet"
      UN_RADARR_0_URL: "http://radarr:3000"
      # Readarr config
      UN_READARR_0_API_KEY: "${READARR_API_KEY:?[unpackerr] Readarr API key missing}"
      UN_READARR_0_PATHS_0: "/downloads/complete/readarr"
      UN_READARR_0_PROTOCOLS: "torrent,usenet"
      UN_READARR_0_URL: "http://readarr:3004"
      # Sonarr config
      UN_SONARR_0_API_KEY: "${SONARR_API_KEY:?[unpackerr] Sonarr API key missing}"
      UN_SONARR_0_PATHS_0: "/downloads/complete/sonarr"
      UN_SONARR_0_PROTOCOLS: "torrent,usenet"
      UN_SONARR_0_URL: "http://sonarr:3001"
      # Logging config
      UN_LOG_FILE: "/config/unpackerr.log"
    healthcheck:
      interval: 30s
      retries: 3
      start_period: 30s
      test: [ "CMD", "/unpackerr", "-v" ]
      timeout: 5s
    networks:
      - home
    restart: unless-stopped
    user: "${PUID_ROOT:?User ID missing}:${PGID_ROOT:?Group ID missing}"
    volumes:
      # Volume mounts from host system
      - "${DOWNLOADS_DIRECTORY}:/downloads"
      # Persistent volumes
      - ./storage/unpackerr/:/config

  # Subtitle downloader
  bazarr:
    image: linuxserver/bazarr:development-version-v1.5.2-beta.20
    container_name: bazarr
    hostname: bazarr
    depends_on:
      radarr:
        condition: service_healthy
      sonarr:
        condition: service_healthy
    deploy:
      resources:
        limits:
          cpus: "1"
          memory: "1024M"
    environment:
      # Base config
      TZ: "${TIMEZONE:?Timezone missing}"
    healthcheck:
      interval: 30s
      retries: 3
      start_period: 30s
      test: [ "CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://127.0.0.1:6767/api/system/status?apikey=${BAZARR_API_KEY}" ]
      timeout: 5s
    labels:
      # Reverse proxy config
      traefik.enable: "true"
      traefik.http.routers.bazarr.rule: "Host(`bazarr.${DOMAIN_NAME}`)"
      traefik.http.routers.bazarr.entrypoints: "websecure"
      traefik.http.routers.bazarr.tls: "true"
      traefik.http.routers.bazarr.middlewares: "authentik-forward-auth@file"
      traefik.http.services.bazarr.loadbalancer.server.port: "4009"
      # Monitor config
      kuma.{{container_name}}.http.name: "Bazarr (Subtitle Manager)"
      kuma.{{container_name}}.http.url: "${BAZARR_MONITOR_URL:?[bazarr] Monitor URL missing}"
    networks:
      - home
    ports:
      - "4009:6767"
    restart: unless-stopped
    user: "${PUID_ROOT:?User ID missing}:${PGID_ROOT:?Group ID missing}"
    volumes:
      # Volume mounts from host system
      - "${MOVIE_DIRECTORY}:/movies"
      - "${TV1_DIRECTORY}:/tv1"
      - "${TV2_DIRECTORY}:/tv2"
      - "${TV_TRASH_DIRECTORY}:/trash"
      - "${TV_KIDS_DIRECTORY}:/kids"
      # Persistent volumes
      - ./storage/bazarr/:/config

  # Automated Speech Recognition tool for subtitles
  whisper:
    image: onerahmet/openai-whisper-asr-webservice:v1.8.2
    container_name: whisper
    hostname: whisper
    deploy:
      resources:
        limits:
          cpus: "4"
          memory: "8192M"
    environment:
      # Model config
      ASR_MODEL: "small.en"
      ASR_ENGINE: "faster_whisper"
    healthcheck:
      interval: 60s
      retries: 3
      start_period: 30s
      test: [ "CMD", "wget", "--no-verbose", "--tries=1", "-O", "-", "http://127.0.0.1:9000/" ] # '--spider' fails
      timeout: 15s
    networks:
      - home
    restart: unless-stopped
    user: "${PUID_ROOT:?User ID missing}:${PGID_ROOT:?Group ID missing}"

  # Video Game ROM Manager
  romm:
    image: rommapp/romm:3.7.3
    container_name: romm
    hostname: romm
    depends_on:
      romm-db:
        condition: service_healthy
    deploy:
      resources:
        limits:
          cpus: "1"
          memory: "1024M"
    environment:
      # Base config
      TZ: "${TIMEZONE:?Timezone missing}"
      # Authentication config
      DISABLE_USERPASS_LOGIN: "true"
      OIDC_CLIENT_ID: "${ROMM_AUTHENTIK_CLIENT_ID:?[romm] Authentik client ID missing}"
      OIDC_CLIENT_SECRET: "${ROMM_AUTHENTIK_CLIENT_SECRET:?[romm] Authentik client secret missing}"
      OIDC_ENABLED: "true"
      OIDC_PROVIDER: "Authentik"
      OIDC_REDIRECT_URI: "${ROMM_AUTHENTIK_REDIRECT_URL:?[romm] Authentik redirect URL missing}"
      OIDC_SERVER_APPLICATION_URL: "${ROMM_AUTHENTIK_OIDC_URL:?[romm] Authentik OIDC URL missing}"
      LOGLEVEL: "DEBUG"
      # Background task config
      ENABLE_RESCAN_ON_FILESYSTEM_CHANGE: "true"
      ENABLE_SCHEDULED_RESCAN: "true"
      # Database config
      ROMM_DB_DRIVER: "mariadb"
      DB_HOST: "romm-db"
      DB_NAME: "${ROMM_DB_NAME:?[romm] Database name missing}"
      DB_PORT: "${ROMM_DB_PORT:?[romm] Database port missing}"
      DB_PASSWD: "${ROMM_DB_PASSWORD:?[romm] Database password missing}"
      DB_ROOT_PASSWD: "${ROMM_DB_ROOT_PASSWORD:?[romm] Database root password missing}"
      DB_USER: "${ROMM_DB_USER:?[romm] Database user missing}"
      # External systems config
      IGDB_CLIENT_ID: "${TWITCH_CLIENT_ID:?[romm] Twitch client ID missing}"
      IGDB_CLIENT_SECRET: "${TWITCH_CLIENT_SECRET:?[romm] Twitch client secret missing}"
      MOBYGAMES_API_KEY: "${MOBYGAMES_API_KEY:?[romm] MobyGames API key missing}"
      STEAMGRIDDB_API_KEY: "${STEAMGRIDDB_API_KEY:?[romm] SteamGridDB API key missing}"
      # Secrets config
      ROMM_AUTH_SECRET_KEY: "${ROMM_SECRET_KEY:?[romm] Authentication secret key missing}"
    healthcheck:
      interval: 30s
      retries: 3
      start_period: 30s
      test: [ "CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://127.0.0.1:8080/heartbeat" ]
      timeout: 5s
    labels:
      # Reverse proxy config
      traefik.enable: "true"
      traefik.http.routers.romm.rule: "Host(`game.${DOMAIN_NAME}`)"
      traefik.http.routers.romm.entrypoints: "websecure"
      traefik.http.routers.romm.tls: "true"
      traefik.http.routers.romm.middlewares: "no-forward-auth@file"
      traefik.http.services.romm.loadbalancer.server.port: "4006"
      # Redirect from 'games' subdomain to 'game' subdomain
      traefik.http.routers.romm-redirect.rule: "Host(`games.${DOMAIN_NAME}`)"
      traefik.http.routers.romm-redirect.entryPoints: "websecure"
      traefik.http.routers.romm-redirect.tls: "true"
      traefik.http.routers.romm-redirect.middlewares: "romm-redirect-regex"
      traefik.http.middlewares.romm-redirect-regex.redirectregex.regex: "^https://games.${DOMAIN_NAME}(.*)"
      traefik.http.middlewares.romm-redirect-regex.redirectregex.replacement: "https://game.${DOMAIN_NAME}$${1}"
      traefik.http.middlewares.romm-redirect-regex.redirectregex.permanent: "true"
      # Monitor config
      kuma.{{container_name}}.http.name: "RomM (Game ROM Manager)"
      kuma.{{container_name}}.http.url: "${ROMM_MONITOR_URL:?[romm] Monitor URL missing}"
    networks:
      - home
    ports:
      - "4006:8080"
    restart: unless-stopped
    user: "${PUID_ROOT:?User ID missing}:${PGID_ROOT:?Group ID missing}"
    volumes:
      # Volume mounts from host system
      - "${ROM_GAMES_AND_BIOS_PARENT_DIRECTORY}:/romm/library:ro"
      - "${ROM_SAVES_DIRECTORY}:/romm/assets"
      - "./docker/romm/config/:/romm/config"
      # Persistent volumes
      - ./storage/romm/resources:/romm/resources  # Resources (covers, screenshots, etc.) that are fetched externally
      - ./storage/romm/redis:/redis-data          # Cached data for background tasks

  # Database for RomM
  romm-db:
    image: mariadb:11.7.2
    container_name: romm-db
    hostname: romm-db
    deploy:
      resources:
        limits:
          cpus: "0.5"
          memory: "512M"
    environment:
      # Base config
      TZ: "${TIMEZONE:?Timezone missing}"
      # Database config
      MYSQL_TCP_PORT: "${ROMM_DB_PORT:?[romm-db] Database port missing}"
      MYSQL_DATABASE: "${ROMM_DB_NAME:?[romm-db] Database name missing}"
      MYSQL_PASSWORD: "${ROMM_DB_PASSWORD:?[romm-db] Database password missing}"
      MYSQL_ROOT_PASSWORD: "${ROMM_DB_ROOT_PASSWORD:?[romm-db] Database root password missing}"
      MYSQL_USER: "${ROMM_DB_USER:?[romm-db] Database user missing}"
    expose:
      - "${ROMM_DB_PORT:?[romm-db] Database port missing}"
    healthcheck:
      interval: 30s
      retries: 3
      start_period: 30s
      test: [ "CMD", "mariadb-admin", "-proot", "ping", "-h", "localhost", "--protocol=tcp" ]
      timeout: 5s
    networks:
      - home
    restart: unless-stopped
    user: "${PUID_ROOT:?User ID missing}:${PGID_ROOT:?Group ID missing}"
    volumes:
      - ./storage/romm-db/:/var/lib/mysql

  # Indexer (Torrents + Usenet)
  prowlarr:
    image: linuxserver/prowlarr:nightly-version-1.32.0.4976
    container_name: prowlarr
    hostname: prowlarr
    depends_on:
      flaresolverr:
        condition: service_healthy
      lidarr:
        condition: service_healthy
      radarr:
        condition: service_healthy
      readarr:
        condition: service_healthy
      sonarr:
        condition: service_healthy
    deploy:
      resources:
        limits:
          cpus: "1"
          memory: "1024M" # High memory resourcing to allow stats page to load
    environment:
      # Base config
      TZ: "${TIMEZONE:?Timezone missing}"
    healthcheck:
      interval: 30s
      retries: 3
      start_period: 30s
      test: [ "CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://127.0.0.1:3016/ping" ]
      timeout: 5s
    labels:
      # Reverse proxy config
      traefik.enable: "true"
      traefik.http.routers.prowlarr.rule: "Host(`prowlarr.${DOMAIN_NAME}`)"
      traefik.http.routers.prowlarr.entrypoints: "websecure"
      traefik.http.routers.prowlarr.tls: "true"
      traefik.http.routers.prowlarr.middlewares: "authentik-forward-auth@file"
      traefik.http.services.prowlarr.loadbalancer.server.port: "4016"
      # Monitor config
      kuma.{{container_name}}.http.name: "Prowlarr (Torrent Indexer)"
      kuma.{{container_name}}.http.url: "${PROWLARR_MONITOR_URL:?[prowlarr] Monitor URL missing}"
    networks:
      - home
    ports:
      - "4016:3016"
    restart: unless-stopped
    user: "${PUID_ROOT:?User ID missing}:${PGID_ROOT:?Group ID missing}"
    volumes:
      # Persistent volumes
      - ./storage/prowlarr/:/config

  # Torrent downloader
  qbittorrent:
    image: linuxserver/qbittorrent:5.0.4-libtorrentv1
    container_name: qbittorrent
    hostname: qbittorrent
    deploy:
      resources:
        limits:
          cpus: "1.5"
          memory: "4096M"
    environment:
      # Base config
      TZ: "${TIMEZONE:?Timezone missing}"
      # UI config
      WEBUI_PORT: "9000"
    healthcheck:
      interval: 30s
      retries: 3
      start_period: 30s
      test: [ "CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://127.0.0.1:9000/api/v2/app/version" ]
      timeout: 5s
    labels:
      # Reverse proxy config
      traefik.enable: "true"
      traefik.http.routers.qbittorrent.rule: "Host(`qbittorrent.${DOMAIN_NAME}`)"
      traefik.http.routers.qbittorrent.entrypoints: "websecure"
      traefik.http.routers.qbittorrent.tls: "true"
      traefik.http.routers.qbittorrent.middlewares: "no-forward-auth@file"
      traefik.http.services.qbittorrent.loadbalancer.server.port: "9000"
      # Monitor config
      kuma.{{container_name}}.http.name: "qBittorrent (Torrent Downloader)"
      kuma.{{container_name}}.http.url: "${QBITTORRENT_MONITOR_URL:?[qbittorrent] Monitor URL missing}"
    networks:
      - home
    ports:
      - "9000:9000"
    restart: unless-stopped
    user: "${PUID_ROOT:?User ID missing}:${PGID_ROOT:?Group ID missing}"
    volumes:
      # Volume mounts from host system
      - "${DOWNLOADS_DIRECTORY}:/downloads"
      # Persistent volumes
      - ./storage/qbittorrent/:/config

  # Discord bot for requests
  doplarr:
    image: linuxserver/doplarr:3.6.3
    container_name: doplarr
    hostname: doplarr
    depends_on:
      radarr:
        condition: service_healthy
      sonarr:
        condition: service_healthy
    deploy:
      resources:
        limits:
          cpus: "0.5"
          memory: "512M"
    environment:
      # Integration configuration
      RADARR__URL: "http://radarr:3000"
      SONARR__URL: "http://sonarr:3001"
      RADARR__API: "${RADARR_API_KEY:?[doplarr] Radarr API key missing}"
      SONARR__API: "${SONARR_API_KEY:?[doplarr] Sonarr API key missing}"
      # Discord configuration
      DISCORD__MAX_RESULTS: "5"
      DISCORD__TOKEN: "${DISCORD_BOT_TOKEN:?[doplarr] Discord bot token missing}"
    healthcheck:
      interval: 30s
      retries: 3
      start_period: 30s
      test: [ "CMD", "pidof", "java" ]
      timeout: 5s
    networks:
      - home
    restart: unless-stopped
    user: "${PUID_ROOT:?User ID missing}:${PGID_ROOT:?Group ID missing}"
    volumes:
      # Persistent volumes
      - ./storage/doplarr/:/config

  # Cloudflare rate-limiter solver
  flaresolverr:
    image: flaresolverr/flaresolverr:v3.3.21
    container_name: flaresolverr
    hostname: flaresolverr
    deploy:
      resources:
        limits:
          cpus: "0.5"
          memory: "1024M"
    environment:
      # Base config
      TZ: "${TIMEZONE:?Timezone missing}"
      # Logging config
      LOG_LEVEL: "info"
    healthcheck:
      interval: 30s
      retries: 3
      start_period: 30s
      test: [ "CMD", "curl", "--silent", "--fail", "http://127.0.0.1:8191/" ] # wget not installed
      timeout: 5s
    networks:
      - home
    restart: unless-stopped
    user: "${PUID_ROOT:?User ID missing}:${PGID_ROOT:?Group ID missing}"

  # Recipe manager
  tandoor:
    image: vabene1111/recipes:1.5.31
    container_name: tandoor
    hostname: tandoor
    depends_on:
      tandoor-db:
        condition: service_healthy
    deploy:
      resources:
        limits:
          cpus: "1"
          memory: "1024M"
    environment:
      # Base config
      TZ: "${TIMEZONE:?Timezone missing}"
      # Authentication config
      REMOTE_USER_AUTH: "1"
      SOCIAL_DEFAULT_ACCESS: "1"
      SOCIAL_DEFAULT_GROUP: "guest"
      SOCIAL_PROVIDERS: "allauth.socialaccount.providers.openid_connect"
      SOCIALACCOUNT_PROVIDERS: |
        {
          "openid_connect": {
            "SERVERS": [
              {
                "id": "authentik",
                "name": "Authentik",
                "server_url": "${TANDOOR_AUTHENTIK_WELL_KNOWN_CONFIGURATION_URL:?[tandoor] Authetik URL missing}",
                "token_auth_method": "client_secret_basic",
                "APP": {
                  "client_id": "${TANDOOR_AUTHENTIK_CLIENT_ID:?[tandoor] Authentik client ID missing}",
                  "secret": "${TANDOOR_AUTHENTIK_CLIENT_SECRET:?[tandoor] Authentik client secret missing}"
                }
              }
            ]
          }
        }
      # Database config
      DB_ENGINE: "django.db.backends.postgresql"
      POSTGRES_HOST: "tandoor-db"
      POSTGRES_PORT: "${TANDOOR_DB_PORT:?[tandoor] Database port missing}"
      POSTGRES_DB: "${TANDOOR_DB_NAME:?[tandoor] Database name missing}"
      POSTGRES_PASSWORD: "${TANDOOR_DB_PASSWORD:?[tandoor] Database password missing}"
      POSTGRES_USER: "${TANDOOR_DB_USER:?[tandoor] Database user missing}"
      # Secrets config
      SECRET_KEY: "${TANDOOR_SECRET_KEY:?[tandoor] Tandoor secret key missing}"
      # System config
      ENABLE_PDF_EXPORT: "1"
      DEBUG: "0"
    healthcheck:
      interval: 30s
      retries: 3
      start_period: 30s
      test: [ "CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://127.0.0.1:8080/openapi" ]
      timeout: 5s
    networks:
      - home
    restart: unless-stopped
    user: "${PUID_ROOT:?User ID missing}:${PGID_ROOT:?Group ID missing}"
    volumes:
      - ./storage/tandoor/nginx/conf.d/:/opt/recipes/nginx/conf.d
      - ./storage/tandoor/mediafiles/:/opt/recipes/mediafiles
      - ./storage/tandoor/staticfiles/:/opt/recipes/staticfiles

  # Recipe manager reverse-proxy/UI
  tandoor-ui:
    image: nginx:1.27.4-alpine
    container_name: tandoor-ui
    hostname: tandoor-ui
    depends_on:
      tandoor:
        condition: service_healthy
    deploy:
      resources:
        limits:
          cpus: "0.5"
          memory: "1024M"
    environment:
      # Base config
      TIMEZONE: "${TIMEZONE:?Timezone missing}"
      # Secrets config
      SECRET_KEY: "${TANDOOR_SECRET_KEY:?[tandoor-ui] Tandoor secret key missing}"
    healthcheck:
      interval: 30s
      retries: 3
      start_period: 30s
      test: [ "CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://127.0.0.1:80" ]
      timeout: 5s
    labels:
      # Reverse proxy config
      traefik.enable: "true"
      traefik.http.routers.tandoor.rule: "Host(`recipe.${DOMAIN_NAME}`)"
      traefik.http.routers.tandoor.entrypoints: "websecure"
      traefik.http.routers.tandoor.tls: "true"
      traefik.http.routers.tandoor.middlewares: "no-forward-auth@file"
      traefik.http.services.tandoor.loadbalancer.server.port: "4010"
      # Redirect from 'recipes' subdomain to 'recipe' subdomain
      traefik.http.routers.tandoor-redirect.rule: "Host(`recipes.${DOMAIN_NAME}`)"
      traefik.http.routers.tandoor-redirect.entryPoints: "websecure"
      traefik.http.routers.tandoor-redirect.tls: "true"
      traefik.http.routers.tandoor-redirect.middlewares: "tandoor-redirect-regex"
      traefik.http.middlewares.tandoor-redirect-regex.redirectregex.regex: "^https://recipes.${DOMAIN_NAME}(.*)"
      traefik.http.middlewares.tandoor-redirect-regex.redirectregex.replacement: "https://recipe.${DOMAIN_NAME}$${1}"
      traefik.http.middlewares.tandoor-redirect-regex.redirectregex.permanent: "true"
      # Monitor config
      kuma.tandoor.http.name: "Tandoor (Recipes)"
      kuma.tandoor.http.url: "${TANDOOR_MONITOR_URL:?[tandoor] Monitor URL missing}"
    networks:
      - home
    ports:
      - "4010:80"
    restart: unless-stopped
    user: "${PUID_ROOT:?User ID missing}:${PGID_ROOT:?Group ID missing}"
    volumes:
      - ./storage/tandoor/nginx/conf.d/:/etc/nginx/conf.d:ro
      - ./storage/tandoor/mediafiles/:/media:ro
      - ./storage/tandoor/staticfiles/:/static:ro

  # Database for Tandoor
  tandoor-db:
    image: postgres:17.4-alpine
    container_name: tandoor-db
    hostname: tandoor-db
    deploy:
      resources:
        limits:
          cpus: "0.5"
          memory: "512M"
    environment:
      # Base config
      TIMEZONE: "${TIMEZONE:?Timezone missing}"
      # Database config
      PGPORT: "${TANDOOR_DB_PORT:?[tandoor-db] Database port missing}"
      POSTGRES_DB: "${TANDOOR_DB_NAME:?[tandoor-db] Database name missing}"
      POSTGRES_PASSWORD: "${TANDOOR_DB_PASSWORD:?[tandoor-db] Database password missing}"
      POSTGRES_USER: "${TANDOOR_DB_USER:?[tandoor-db] Database user missing}"
      # Next 3 variables needed to avoid "FATAL role 'root' does not exist" error
      PGDATABASE: "${TANDOOR_DB_NAME:?[tandoor-db] Database name missing}"
      PGPASSWORD: "${TANDOOR_DB_PASSWORD:?[tandoor-db] Database password missing}"
      PGUSER: "${TANDOOR_DB_USER:?[tandoor-db] Database user missing}"
    expose:
      - "${TANDOOR_DB_PORT:?[tandoor-db] Database port missing}"
    healthcheck:
      interval: 30s
      retries: 3
      start_period: 30s
      test: [ "CMD", "pg_isready", "--host", "localhost" ]
      timeout: 5s
    networks:
      - home
    restart: unless-stopped
    user: "${PUID_ROOT:?User ID missing}:${PGID_ROOT:?Group ID missing}"
    volumes:
      - ./storage/tandoor-db/data/:/var/lib/postgresql/data

  # Inventory management
  homebox:
    image: ghcr.io/sysadminsmedia/homebox:0.17.2
    container_name: homebox
    hostname: homebox
    deploy:
      resources:
        limits:
          cpus: "0.5"
          memory: "512M"
    environment:
      # Base config
      TZ: "${TIMEZONE:?Timezone missing}"
      # Logging config
      HBOX_LOG_FORMAT: "text"
      HBOX_LOG_LEVEL: "info"
      # UI config
      HBOX_OPTIONS_ALLOW_REGISTRATION: "false"
      HBOX_WEB_MAX_UPLOAD_SIZE: "10"
    labels:
      # Reverse proxy config
      traefik.enable: "true"
      traefik.http.routers.homebox.rule: "Host(`inventory.${DOMAIN_NAME}`)"
      traefik.http.routers.homebox.entrypoints: "websecure"
      traefik.http.routers.homebox.tls: "true"
      traefik.http.routers.homebox.middlewares: "no-forward-auth@file"
      traefik.http.services.homebox.loadbalancer.server.port: "4011"
      # Monitor config
      kuma.{{container_name}}.http.name: "Homebox (House Inventory)"
      kuma.{{container_name}}.http.url: "${HOMEBOX_MONITOR_URL:?[homebox] Monitor URL missing}"
    networks:
      - home
    ports:
      - "4011:7745"
    restart: unless-stopped
    user: "${PUID_ROOT:?User ID missing}:${PGID_ROOT:?Group ID missing}"
    volumes:
      - ./storage/homebox/:/data/

  # Daily journal
  dailytxt:
    image: phitux/dailytxt:1.0.15
    container_name: dailytxt
    hostname: dailytxt
    deploy:
      resources:
        limits:
          cpus: "0.5"
          memory: "256M"
    environment:
      # Base config
      USER_GID: "${PGID_ROOT:?Group ID missing}"
      USER_UID: "${PUID_ROOT:?User ID missing}"
      TZ: "${TIMEZONE:?Timezone missing}"
      # Secrets config
      SECRET_KEY: "${DAILYTXT_SECRET_KEY:?[dailytxt] Secret key missing}"
      # UI config
      ALLOW_REGISTRATION: "false"
      DATA_INDENT: "2"
      ENABLE_UPDATE_CHECK: "false"
      JWT_EXP_DAYS: "60"
      PORT: "8765"
    healthcheck:
      interval: 30s
      retries: 3
      start_period: 30s
      test: [ "CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://127.0.0.1:8765" ]
      timeout: 5s
    labels:
      # Reverse proxy config
      traefik.enable: "true"
      traefik.http.routers.dailytxt.rule: "Host(`journal.${DOMAIN_NAME}`)"
      traefik.http.routers.dailytxt.entrypoints: "websecure"
      traefik.http.routers.dailytxt.tls: "true"
      traefik.http.routers.dailytxt.middlewares: "no-forward-auth@file"
      traefik.http.services.dailytxt.loadbalancer.server.port: "4015"
      # Monitor config
      kuma.{{container_name}}.http.name: "DailyTxt (Daily Journal)"
      kuma.{{container_name}}.http.url: "${DAILYTXT_MONITOR_URL:?[dailytxt] Monitor URL missing}"
    networks:
      - home
    ports:
      - "4015:8765"
    restart: unless-stopped
    volumes:
      - ./storage/dailytxt/:/app/data/

  # Documents file browser
  filebrowser:
    image: filebrowser/filebrowser:v2.32.0
    container_name: filebrowser
    hostname: filebrowser
    deploy:
      resources:
        limits:
          cpus: "0.25"
          memory: "256M"
    environment:
      # Base config
      TZ: "${TIMEZONE:?Timezone missing}"
      # Authentication config
      FB_NOAUTH: noauth
    labels:
      # Reverse proxy config
      traefik.enable: "true"
      traefik.http.routers.filebrowser.rule: "Host(`file.${DOMAIN_NAME}`)"
      traefik.http.routers.filebrowser.entrypoints: "websecure"
      traefik.http.routers.filebrowser.tls: "true"
      traefik.http.routers.filebrowser.middlewares: "authentik-forward-auth@file"
      traefik.http.services.filebrowser.loadbalancer.server.port: "4007"
      # Redirect from 'files' subdomain to 'file' subdomain
      traefik.http.routers.filebrowser-redirect.rule: "Host(`files.${DOMAIN_NAME}`)"
      traefik.http.routers.filebrowser-redirect.entryPoints: "websecure"
      traefik.http.routers.filebrowser-redirect.tls: "true"
      traefik.http.routers.filebrowser-redirect.middlewares: "filebrowser-redirect-regex"
      traefik.http.middlewares.filebrowser-redirect-regex.redirectregex.regex: "^https://files.${DOMAIN_NAME}(.*)"
      traefik.http.middlewares.filebrowser-redirect-regex.redirectregex.replacement: "https://file.${DOMAIN_NAME}$${1}"
      traefik.http.middlewares.filebrowser-redirect-regex.redirectregex.permanent: "true"
      # Monitor config
      kuma.{{container_name}}.http.name: "FileBrowser (Document Viewer)"
      kuma.{{container_name}}.http.url: "${FILEBROWSER_MONITOR_URL:?[filebrowser] Monitor URL missing}"
    networks:
      - home
    ports:
      - "4007:80"
    restart: unless-stopped
    security_opt:
      - no-new-privileges:true
    user: "${PUID_ROOT:?User ID missing}:${PGID_ROOT:?Group ID missing}"
    volumes:
      # Volume mounts from host system
      - "${DOCUMENTS_DIRECTORY}:/srv:ro"
      # Persistent volumes
      - ./storage/filebrowser/:/app/data/
      - ./storage/filebrowser/.filebrowser.json:/.filebrowser.json
      - ./storage/filebrowser/database.db:/database.db

  # Large Language Model with ChatGPT-style UI
  ollama:
    image: ghcr.io/open-webui/open-webui:v0.5.18-ollama
    container_name: ollama
    hostname: ollama
    deploy:
      resources:
        limits:
          cpus: "8"
          memory: "8192M"
    environment:
      # Base config
      TZ: "${TIMEZONE:?Timezone missing}"
      # UI config
      OLLAMA_BASE_URL: "https://ai.${DOMAIN_NAME}"
      ENABLE_COMMUNITY_SHARING: "false"
      ENABLE_SIGNUP: "false"
      WEBUI_AUTH_TRUSTED_EMAIL_HEADER: "X-Authentik-Email"
      WEBUI_NAME: "${DOMAIN_NAME:?[ollama] Domain name missing} LLM"
      WEBUI_SECRET_KEY: "${OLLAMA_SECRET_KEY:?[ollama] Secret key missing}"
    labels:
      # Reverse proxy config
      traefik.enable: "true"
      traefik.http.routers.ollama.rule: "Host(`ai.${DOMAIN_NAME}`)"
      traefik.http.routers.ollama.entrypoints: "websecure"
      traefik.http.routers.ollama.tls: "true"
      traefik.http.routers.ollama.middlewares: "authentik-forward-auth@file"
      traefik.http.services.ollama.loadbalancer.server.port: "4012"
      # Monitor config
      kuma.{{container_name}}.http.name: "Ollama (LLM)"
      kuma.{{container_name}}.http.url: "${OLLAMA_MONITOR_URL:?[ollama] Monitor URL missing}"
    networks:
      - home
    ports:
      - "4012:8080"
    restart: unless-stopped
    user: "${PUID_ROOT:?User ID missing}:${PGID_ROOT:?Group ID missing}"
    volumes:
      # Persistent volumes
      - ./storage/ollama/open-webui:/app/backend/data

  # Static code analysis tool
  sonarqube:
    image: sonarqube:25.2.0.102705-community
    container_name: sonarqube
    hostname: sonarqube
    depends_on:
      sonarqube-db:
        condition: service_healthy
    deploy:
      resources:
        limits:
          cpus: "1"
          memory: "4096M"
    environment:
      # Database config
      SONAR_JDBC_URL: "jdbc:postgresql://sonarqube-db:${SONARQUBE_DB_PORT:?[sonarqube-db] Database port missing}/${SONARQUBE_DB_NAME:?[sonarqube] Database name missing}"
      SONAR_JDBC_USERNAME: "${SONARQUBE_DB_USER:?[sonarqube] Database user missing}"
      SONAR_JDBC_PASSWORD: "${SONARQUBE_DB_PASSWORD:?[sonarqube] Database password missing}"
    healthcheck:
      interval: 30s
      retries: 3
      start_period: 30s
      test: [ "CMD", "wget", "--no-verbose", "--tries=1", "-O", "-", "http://127.0.0.1:9000/api/system/status" ] # '--spider' fails
      timeout: 5s
    labels:
      # Reverse proxy config
      traefik.enable: "true"
      traefik.http.routers.sonarqube.rule: "Host(`sonarqube.${DOMAIN_NAME}`)"
      traefik.http.routers.sonarqube.entrypoints: "websecure"
      traefik.http.routers.sonarqube.tls: "true"
      traefik.http.routers.sonarqube.middlewares: "no-forward-auth@file"
      traefik.http.services.sonarqube.loadbalancer.server.port: "4013"
      # Monitor config
      kuma.{{container_name}}.http.name: "SonarQube (Code Analysis)"
      kuma.{{container_name}}.http.url: "${SONARQUBE_MONITOR_URL:?[sonarqube] Monitor URL missing}"
    networks:
      - home
    ports:
      - "4013:9000"
    restart: unless-stopped
    user: "${PUID_NON_ROOT:?User ID missing}:${PGID_ROOT:?Group ID missing}"
    volumes:
      - ./storage/sonarqube/data/:/opt/sonarqube/data
      - ./storage/sonarqube/extensions/:/opt/sonarqube/extensions
      - ./storage/sonarqube/logs/:/opt/sonarqube/logs
      - ./storage/sonarqube/temp/:/opt/sonarqube/temp

  # Database for SonarQube
  sonarqube-db:
    image: postgres:17.4-alpine
    container_name: sonarqube-db
    hostname: sonarqube-db
    deploy:
      resources:
        limits:
          cpus: "0.5"
          memory: "512M"
    environment:
      # Base config
      TIMEZONE: "${TIMEZONE:?Timezone missing}"
      # Database config
      PGPORT: "${SONARQUBE_DB_PORT:?[sonarqube-db] Database port missing}"
      POSTGRES_DB: "${SONARQUBE_DB_NAME:?[sonarqube-db] Database name missing}"
      POSTGRES_USER: "${SONARQUBE_DB_USER:?[sonarqube-db] Database user missing}"
      POSTGRES_PASSWORD: "${SONARQUBE_DB_PASSWORD:?[sonarqube-db] Database password missing}"
      # Next 3 variables needed to avoid "FATAL role 'root' does not exist" error
      PGDATABASE: "${SONARQUBE_DB_NAME:?[sonarqube-db] Database password missing}"
      PGPASSWORD: "${SONARQUBE_DB_PASSWORD:?[sonarqube-db] Database name missing}"
      PGUSER: "${SONARQUBE_DB_USER:?[sonarqube-db] Database user missing}"
    expose:
      - "${SONARQUBE_DB_PORT:?[sonarqube-db] Database port missing}"
    healthcheck:
      interval: 30s
      retries: 3
      start_period: 30s
      test: [ "CMD", "pg_isready", "--host", "localhost" ]
      timeout: 5s
    networks:
      - home
    restart: unless-stopped
    user: "${PUID_ROOT:?User ID missing}:${PGID_ROOT:?Group ID missing}"
    volumes:
      - ./storage/sonarqube-db/data/:/var/lib/postgresql/data

  # Statistics for Emby
  jellystat:
    image: cyfershepard/jellystat:1.1.3
    container_name: jellystat
    hostname: jellystat
    depends_on:
      jellystat-db:
        condition: service_healthy
    deploy:
      resources:
        limits:
          cpus: "0.5"
          memory: "1024M"
    environment:
      # Base config
      TZ: "${TIMEZONE:?Timezone missing}"
      # Database config
      POSTGRES_DATABASE: "${JELLYSTAT_DB_NAME:?[jellystat] Database name missing}"
      POSTGRES_USER: "${JELLYSTAT_DB_USER:?[jellystat] Database user missing}"
      POSTGRES_PASSWORD: "${JELLYSTAT_DB_PASSWORD:?[jellystat] Database password missing}"
      POSTGRES_IP: "jellystat-db"
      POSTGRES_PORT: "${JELLYSTAT_DB_PORT:?[jellystat] Database port missing}"
      # Secrets config
      JWT_SECRET: '${JELLYSTAT_SECRET_KEY:?[jellystat] Secret key missing}'
      # System config
      MINIMUM_SECONDS_TO_INCLUDE_PLAYBACK: "10"
    labels:
      # Reverse proxy config
      traefik.enable: "true"
      traefik.http.routers.jellystat.rule: "Host(`stat.${DOMAIN_NAME}`)"
      traefik.http.routers.jellystat.entrypoints: "websecure"
      traefik.http.routers.jellystat.tls: "true"
      traefik.http.routers.jellystat.middlewares: "authentik-forward-auth@file"
      traefik.http.services.jellystat.loadbalancer.server.port: "4008"
      # Redirect from 'stats' subdomain to 'stat' subdomain
      traefik.http.routers.jellystat-redirect.rule: "Host(`stats.${DOMAIN_NAME}`)"
      traefik.http.routers.jellystat-redirect.entryPoints: "websecure"
      traefik.http.routers.jellystat-redirect.tls: "true"
      traefik.http.routers.jellystat-redirect.middlewares: "jellystat-redirect-regex"
      traefik.http.middlewares.jellystat-redirect-regex.redirectregex.regex: "^https://stats.${DOMAIN_NAME}(.*)"
      traefik.http.middlewares.jellystat-redirect-regex.redirectregex.replacement: "https://stat.${DOMAIN_NAME}$${1}"
      traefik.http.middlewares.jellystat-redirect-regex.redirectregex.permanent: "true"
      # Monitor config
      kuma.{{container_name}}.http.name: "Jellystat (Emby Statistics)"
      kuma.{{container_name}}.http.url: "${JELLYSTAT_MONITOR_URL:?[jellystat] Monitor URL missing}"
    networks:
      - home
    ports:
      - "4008:3000"
    restart: unless-stopped
    user: "${PUID_ROOT:?User ID missing}:${PGID_ROOT:?Group ID missing}"
    volumes:
      - ./storage/jellystat/data/:/app/backend/backup-data

  # Database for Jellystat
  jellystat-db:
    image: postgres:17.4-alpine
    container_name: jellystat-db
    hostname: jellystat-db
    deploy:
      resources:
        limits:
          cpus: "0.5"
          memory: "512M"
    environment:
      # Base config
      TIMEZONE: "${TIMEZONE:?Timezone missing}"
      # Database config
      PGPORT: "${JELLYSTAT_DB_PORT:?[jellystat-db] Database port missing}"
      POSTGRES_DB: "${JELLYSTAT_DB_NAME:?[jellystat-db] Database name missing}"
      POSTGRES_PASSWORD: "${JELLYSTAT_DB_PASSWORD:?[jellystat-db] Database password missing}"
      POSTGRES_USER: "${JELLYSTAT_DB_USER:?[jellystat-db] Database user missing}"
      # Next 3 variables needed to avoid "FATAL role 'root' does not exist" error
      PGDATABASE: "${JELLYSTAT_DB_NAME:?[jellystat-db] Database name missing}"
      PGPASSWORD: "${JELLYSTAT_DB_PASSWORD:?[jellystat-db] Database password missing}"
      PGUSER: "${JELLYSTAT_DB_USER:?[jellystat-db] Database user missing}"
    expose:
      - "${JELLYSTAT_DB_PORT:?[jellystat-db] Database port missing}"
    healthcheck:
      interval: 30s
      retries: 3
      start_period: 30s
      test: [ "CMD", "pg_isready", "--host", "localhost" ]
      timeout: 5s
    networks:
      - home
    restart: unless-stopped
    user: "${PUID_ROOT:?User ID missing}:${PGID_ROOT:?Group ID missing}"
    volumes:
      - ./storage/jellystat-db/data/:/var/lib/postgresql/data

  # Bookmark manager
  linkwarden:
    image: ghcr.io/linkwarden/linkwarden:v2.9.3
    container_name: linkwarden
    hostname: linkwarden
    depends_on:
      linkwarden-db:
        condition: service_healthy
    deploy:
      resources:
        limits:
          cpus: "0.75"
          memory: "1024M"
    environment:
      # Base config
      TIMEZONE: "${TIMEZONE:?Timezone missing}"
      # Auth config
      AUTHENTIK_CLIENT_ID: "${LINKWARDEN_AUTHENTIK_CLIENT_ID:?[linkwarden] Authentik client ID missing}"
      AUTHENTIK_CLIENT_SECRET: "${LINKWARDEN_AUTHENTIK_CLIENT_SECRET:?[linkwarden] Authentik client secret missing}"
      AUTHENTIK_CUSTOM_NAME: "Authentik"
      AUTHENTIK_ISSUER: "${LINKWARDEN_AUTHENTIK_OIDC_URI:?[linkwarden] Authentik OIDC URL missing}"
      NEXTAUTH_SECRET: "${LINKWARDEN_SECRET_KEY:?[linkwarden] Secret key missing}"
      NEXTAUTH_URL: "${LINKWARDEN_PUBLIC_URL:?[linkwarden] Public URL missing}/api/v1/auth"
      NEXT_PUBLIC_AUTHENTIK_ENABLED: "true"
      NEXT_PUBLIC_CREDENTIALS_ENABLED: "false"
      NEXT_PUBLIC_DISABLE_REGISTRATION: "true"
      DISABLE_NEW_SSO_USERS: "true"
      # Database config
      DATABASE_URL: "postgresql://${LINKWARDEN_DB_USER:?[linkwarden] Database user missing}:${LINKWARDEN_DB_PASSWORD:?[linkwarden] Database password missing}@linkwarden-db:${LINKWARDEN_DB_PORT:?[linkwarden] Database port missing}/${LINKWARDEN_DB_NAME:?[linkwarden] Database name missing}"
      POSTGRES_PASSWORD: "${LINKWARDEN_DB_PASSWORD:?[linkwarden] Database password missing}"
    labels:
      # Reverse proxy config
      traefik.enable: "true"
      traefik.http.routers.linkwarden.rule: "Host(`bookmark.${DOMAIN_NAME}`)"
      traefik.http.routers.linkwarden.entrypoints: "websecure"
      traefik.http.routers.linkwarden.tls: "true"
      traefik.http.routers.linkwarden.middlewares: "no-forward-auth@file"
      traefik.http.services.linkwarden.loadbalancer.server.port: "4017"
      # Redirect from 'bookmarks' subdomain to 'bookmark' subdomain
      traefik.http.routers.linkwarden-redirect.rule: "Host(`bookmarks.${DOMAIN_NAME}`)"
      traefik.http.routers.linkwarden-redirect.entryPoints: "websecure"
      traefik.http.routers.linkwarden-redirect.tls: "true"
      traefik.http.routers.linkwarden-redirect.middlewares: "linkwarden-redirect-regex"
      traefik.http.middlewares.linkwarden-redirect-regex.redirectregex.regex: "^https://bookmarks.${DOMAIN_NAME}(.*)"
      traefik.http.middlewares.linkwarden-redirect-regex.redirectregex.replacement: "https://bookmark.${DOMAIN_NAME}$${1}"
      traefik.http.middlewares.linkwarden-redirect-regex.redirectregex.permanent: "true"
      # Monitor config
      kuma.{{container_name}}.http.name: "Linkwarden (Bookmarks)"
      kuma.{{container_name}}.http.url: "${LINKWARDEN_MONITOR_URL:?[linkwarden] Monitor URL missing}"
    networks:
      - home
    ports:
      - "4017:3000"
    restart: unless-stopped
    user: "${PUID_ROOT:?User ID missing}:${PGID_ROOT:?Group ID missing}"
    volumes:
      - ./storage/linkwarden/data/:/data/data

  # Database for Linkwarden
  linkwarden-db:
    image: postgres:17.4-alpine
    container_name: linkwarden-db
    hostname: linkwarden-db
    deploy:
      resources:
        limits:
          cpus: "0.5"
          memory: "512M"
    environment:
      # Base config
      TIMEZONE: "${TIMEZONE:?Timezone missing}"
      # Database config
      PGPORT: "${LINKWARDEN_DB_PORT:?[linkwarden-db] Database port missing}"
      POSTGRES_DB: "${LINKWARDEN_DB_NAME:?[linkwarden-db] Database name missing}"
      POSTGRES_PASSWORD: "${LINKWARDEN_DB_PASSWORD:?[linkwarden-db] Database password missing}"
      POSTGRES_USER: "${LINKWARDEN_DB_USER:?[linkwarden-db] Database user missing}"
      # Next 3 variables needed to avoid "FATAL role 'root' does not exist" error
      PGDATABASE: "${LINKWARDEN_DB_NAME:?[linkwarden-db] Database name missing}"
      PGPASSWORD: "${LINKWARDEN_DB_PASSWORD:?[linkwarden-db] Database password missing}"
      PGUSER: "${LINKWARDEN_DB_USER:?[linkwarden-db] Database user missing}"
    expose:
      - "${LINKWARDEN_DB_PORT:?[linkwarden-db] Database port missing}"
    healthcheck:
      interval: 30s
      retries: 3
      start_period: 30s
      test: [ "CMD", "pg_isready", "--host", "localhost" ]
      timeout: 5s
    networks:
      - home
    restart: unless-stopped
    user: "${PUID_ROOT:?User ID missing}:${PGID_ROOT:?Group ID missing}"
    volumes:
      - ./storage/linkwarden-db/data/:/var/lib/postgresql/data

  # Automatic Uptime-Kuma monitor generator
  autokuma:
    build:
      context: .
      dockerfile: docker/autokuma/Dockerfile
      args:
        # Docker image versions
        AUTOKUMA_VERSION: "0.8.0" # https://github.com/BigBoot/AutoKuma/pkgs/container/autokuma
        # Build arguments
        # Host Monitor URLs
        QBITTORRENT_MONITOR_URL: "${QBITTORRENT_NATIVE_MONITOR_URL:?[autokuma] qBittorrent static monitor URL missing}"
        # NAS URLs
        SYNOLOGY_MONITOR_URL: "${SYNOLOGY_MONITOR_URL:?[autokuma] Synology static monitor URL missing}"
    container_name: autokuma
    hostname: autokuma
    deploy:
      resources:
        limits:
          cpus: "0.5"
          memory: "256M"
    environment:
      # Base config
      TZ: "${TIMEZONE:?Timezone missing}"
      # Monitor config
      AUTOKUMA__DEFAULT_SETTINGS: |-
        http.accepted_statuscodes:  [ "200" ]
        http.expiry_notification: true
        http.interval: 60
        http.max_redirects: 1
        http.max_retries: 5
        http.retry_interval: 60
        http.timeout: 30
      AUTOKUMA__KUMA__URL: "${UPTIME_KUMA_URL:?[autokuma] UptimeKuma URL missing}"
      AUTOKUMA__KUMA__USERNAME: "${ADMIN_USERNAME:?[autokuma] UptimeKuma username missing}"
      AUTOKUMA__KUMA__PASSWORD: "${UPTIME_KUMA_PASSWORD:?[autokuma] UptimeKuma password missing}"
      AUTOKUMA__ON_DELETE: "keep" # Keep monitors even if the service goes down
      AUTOKUMA__SYNC_INTERVAL: 300 # Sync every 5 minutes
      AUTOKUMA__STATIC_MONITORS: "${AUTO_KUMA_STATIC_MONITOR_DIRECTORY:?[autokuma] Static monitor directory missing}"
      AUTOKUMA__TAG_NAME: "${AUTO_KUMA_TAG_NAME:?[autokuma] Tag name missing}"
      AUTOKUMA__TAG_COLOR: "${AUTO_KUMA_TAG_COLOUR:?[autokuma] Tag colour missing}"
      # Status page environment variables
      STATUS_PAGE_SLUG: "main"
    healthcheck:
      interval: 30s
      retries: 3
      start_period: 30s
      test: [ "CMD", "pidof", "autokuma" ]
      timeout: 5s
    networks:
      - home
    restart: no
    user: "${PUID_ROOT:?User ID missing}:${PGID_ROOT:?Group ID missing}"
    volumes:
      # Docker socket mount from host system
      - "${DOCKER_SOCKET}:/var/run/docker.sock"

  # Automatic Uptime-Kuma monitor generator
  # This is a near-duplicate of the main one above, but connects to the RaspberryPi Docker host.
  # It also uses the published docker image, rather than building one for any static monitors.
  autokuma-pi:
    image: ghcr.io/bigboot/autokuma:0.8.0
    container_name: autokuma-pi
    hostname: autokuma-pi
    deploy:
      resources:
        limits:
          cpus: "0.5"
          memory: "256M"
    environment:
      # Base config
      TZ: "${TIMEZONE:?Timezone missing}"
      # Monitor config
      AUTOKUMA__DEFAULT_SETTINGS: |-
        http.accepted_statuscodes:  [ "200" ]
        http.tags: [{"tag_id": 3 }]
        http.expiry_notification: true
        http.interval: 60
        http.max_redirects: 1
        http.max_retries: 5
        http.retry_interval: 60
        http.timeout: 30
      AUTOKUMA__KUMA__URL: "${UPTIME_KUMA_URL:?[autokuma] UptimeKuma URL missing}"
      AUTOKUMA__KUMA__USERNAME: "${ADMIN_USERNAME:?[autokuma] UptimeKuma username missing}"
      AUTOKUMA__KUMA__PASSWORD: "${UPTIME_KUMA_PASSWORD:?[autokuma] UptimeKuma password missing}"
      AUTOKUMA__ON_DELETE: "keep" # Keep monitors even if the service goes down
      AUTOKUMA__SYNC_INTERVAL: 600 # Sync every 10 minutes
      AUTOKUMA__TAG_NAME: "${AUTO_KUMA_TAG_NAME:?[autokuma] Tag name missing}"
      AUTOKUMA__TAG_COLOR: "${AUTO_KUMA_TAG_COLOUR:?[autokuma] Tag colour missing}"
      DOCKER_HOST: "${AUTO_KUMA_REMOTE_HOST}"
    healthcheck:
      interval: 30s
      retries: 3
      start_period: 30s
      test: [ "CMD", "pidof", "autokuma" ]
      timeout: 5s
    networks:
      - home
    restart: no
    user: "${PUID_ROOT:?User ID missing}:${PGID_ROOT:?Group ID missing}"
