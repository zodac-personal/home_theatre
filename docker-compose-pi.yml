networks:
  home_pi:
    name: home_pi
    driver: bridge

services:

  # Reverse proxy
  traefik:
    image: traefik:v3.2.0
    container_name: traefik
    hostname: traefik
    command: --configFile=/config/traefik.yml
    environment:
      # Variables used for Go template substitution in the config files
      DOMAIN_NAME: "${PUBLIC_DOMAIN_NAME}"
      # Non-docker service IPs
      EMBY_URL: "${EMBY_URL}"
      STATUS_URL: "${STATUS_URL}"
      TRAEFIK_URL: "${TRAEFIK_URL}"
    healthcheck:
      interval: 30s
      retries: 3
      start_period: 30s
      test: [ "CMD", "traefik", "healthcheck", "--ping" ]
      timeout: 5s
    networks:
      - home_pi
    ports:
      - "443:443"
      - "5555:8080"
    restart: unless-stopped
    user: "${PUID_ROOT:?User ID missing}:${PGID_ROOT:?Group ID missing}"
    volumes:
      - "${DOCKER_SOCKET}:/var/run/docker.sock"
      - ./docker/traefik/config/traefik.yml:/config/traefik.yml:ro
      - ./docker/traefik/config/file_configs/:/file_configs:ro
      - ./docker/traefik/certs/cloudflare_origin_cert.pem:/etc/certs/cloudflare_origin_cert.pem:ro
      - ./docker/traefik/certs/cloudflare_origin_key.pem:/etc/certs/cloudflare_origin_key.pem:ro

  # Reverse proxy connector for external hosts
  traefik-provider:
    image: redis:7.4.1-alpine
    container_name: traefik-provider
    hostname: traefik-provider
    command: --save 60 1 --loglevel warning
    healthcheck:
      interval: 30s
      retries: 3
      start_period: 30s
      test: [ "CMD", "redis-cli", "ping" ]
      timeout: 5s
    networks:
      - home_pi
    ports:
      - "6379:6379"
    restart: unless-stopped
    user: "${PUID_ROOT:?User ID missing}:${PGID_ROOT:?Group ID missing}"
    volumes:
      # Persistent volumes
      - ./storage/traefik-provider/data/:/data

  # Reverse proxy
  apache:
    build: 
      context: .
      dockerfile: docker/apache/Dockerfile
      args:
        # Docker image version
        APACHE_VERSION: "2.4.62" # https://hub.docker.com/_/httpd
        # Build arguments
        ADMIN_EMAIL_ADDRESS: "${ADMIN_EMAIL_ADDRESS?[apache] Admin email address missing}"
    container_name: apache
    hostname: apache
    deploy:
      placement:
        constraints: [ node.labels.role == manager ]
    environment:
      # Base config
      TZ: "${TIMEZONE:?Timezone missing}"
    healthcheck:
      interval: 30s
      retries: 3
      start_period: 30s
      test: [ "CMD", "wget", "--no-verbose", "--tries=1", "-O", "-", "http://localhost/" ]
      timeout: 5s
    networks:
      - home_pi
    ports:
      - "80:80"
    restart: unless-stopped
    user: "${PUID_ROOT:?User ID missing}:${PGID_ROOT:?Group ID missing}"

  # Homepage
  homarr:
    image: ghcr.io/ajnart/homarr:0.15.7 # https://github.com/ajnart/homarr/pkgs/container/homarr
    container_name: homarr
    hostname: homarr
    deploy:
      placement:
        constraints: [ node.labels.role == manager ]
    environment:
      # Base config
      TZ: "${TIMEZONE:?Timezone missing}"
      # Authentication config
      AUTH_OIDC_ADMIN_GROUP: "HomarrAdmins"
      AUTH_OIDC_AUTO_LOGIN: "true"
      AUTH_OIDC_CLIENT_ID: "${HOMARR_AUTHENTIK_CLIENT_ID:?[homarr] Authentik client ID missing}"
      AUTH_OIDC_CLIENT_NAME: "Authentik"
      AUTH_OIDC_CLIENT_SECRET: "${HOMARR_AUTHENTIK_CLIENT_SECRET:?[homarr] Authentik client secret missing}"
      AUTH_OIDC_URI: "${HOMARR_AUTHENTIK_OIDC_URI:?[homarr] Authentik OIDC URI missing}"
      AUTH_PROVIDER: "oidc"
      BASE_URL: "${HOMARR_BASE_URL:?[homarr] Base URL missing}"
      NEXTAUTH_URL: "${HOMARR_BASE_URL:?[homarr] Base URL missing}"
    labels:
      # Uptime-Kuma monitor config
      kuma.{{container_name}}.http.name: "Homarr (HomePage)"
      kuma.{{container_name}}.http.url: "${HOMARR_MONITOR_URL:?[homarr] Monitor URL missing}"
    networks:
      - home_pi
    ports:
      - "5000:7575"
    restart: unless-stopped
    user: "${PUID_ROOT:?User ID missing}:${PGID_ROOT:?Group ID missing}"
    volumes:
      # Persistent volumes
      - ./storage/homarr/configs:/app/data/configs
      - ./storage/homarr/data:/data
      - ./storage/homarr/icons:/app/public/icons

  # Identity platform (authentication and SSO)
  authentik:
    image: ghcr.io/goauthentik/server:2024.10.1 # https://github.com/goauthentik/authentik/pkgs/container/server
    container_name: authentik
    hostname: authentik
    depends_on:
      authentik-cache:
          condition: service_healthy
      authentik-db:
          condition: service_healthy
    command: server
    deploy:
      placement:
        constraints: [ node.labels.role == manager ]
    environment:
      # Base config
      TZ: "${TIMEZONE:?Timezone missing}"
      # Database/cache config
      AUTHENTIK_REDIS__HOST: authentik-cache
      AUTHENTIK_POSTGRESQL__HOST: authentik-db
      AUTHENTIK_POSTGRESQL__NAME: "${AUTHENTIK_DB_NAME:?[authentik] Database name missing}"
      AUTHENTIK_POSTGRESQL__PASSWORD: "${AUTHENTIK_DB_PASSWORD:?[authentik] Database password missing}"
      AUTHENTIK_POSTGRESQL__PORT: "${AUTHENTIK_DB_PORT:?[authentik] Database port missing}"
      AUTHENTIK_POSTGRESQL__USER: "${AUTHENTIK_DB_USER?[authentik] Database user missing}"
      # Authentik config
      AUTHENTIK_BOOTSTRAP_PASSWORD: "admin"
      AUTHENTIK_SECRET_KEY: "${AUTHENTIK_SECRET_KEY:?[authentik] Authentik secret key missing}"
    labels:
      traefik.enable: "true"
      # Main router for the service
      traefik.http.routers.authentik.rule: "Host(`auth.${PUBLIC_DOMAIN_NAME}`)"
      traefik.http.routers.authentik.entrypoints: "websecure"
      traefik.http.routers.authentik.tls: "true"
      # Uptime-Kuma monitor config
      kuma.{{container_name}}.http.accepted_statuscodes:  '[ "200" ]'
      kuma.{{container_name}}.http.name: "Authentik (Identity Platform)"
      kuma.{{container_name}}.http.url: "${AUTHENTIK_MONITOR_URL:?[authentik] Monitor URL missing}"
    networks:
      - home_pi
    ports:
      - "${AUTHENTIK_UI_PORT:?[authentik] UI port missing}:9000"
    restart: unless-stopped
    user: "${PUID_ROOT:?User ID missing}:${PGID_ROOT:?Group ID missing}"
    volumes:
      # Persistent volumes
      - ./storage/authentik/media/:/media
      - ./storage/authentik/certs/:/certs
      - ./storage/authentik/templates/:/templates

  # Worker node for Authentik
  authentik-worker:
    image: ghcr.io/goauthentik/server:2024.10.1 # https://github.com/goauthentik/authentik/pkgs/container/server
    container_name: authentik-worker
    hostname: authentik-worker
    command: worker
    depends_on:
      authentik:
          condition: service_healthy
      authentik-cache:
          condition: service_healthy
      authentik-db:
          condition: service_healthy
    deploy:
      placement:
        constraints: [ node.labels.role == manager ]
    environment:
      # Base config
      TZ: "${TIMEZONE:?Timezone missing}"
      # Database/cache config
      AUTHENTIK_REDIS__HOST: authentik-cache
      AUTHENTIK_POSTGRESQL__HOST: authentik-db
      AUTHENTIK_POSTGRESQL__NAME: "${AUTHENTIK_DB_NAME:?[authentik-worker] Database name missing}"
      AUTHENTIK_POSTGRESQL__PASSWORD: "${AUTHENTIK_DB_PASSWORD:?[authentik-worker] Database password missing}"
      AUTHENTIK_POSTGRESQL__PORT: "${AUTHENTIK_DB_PORT:?[authentik-worker] Database port missing}"
      AUTHENTIK_POSTGRESQL__USER: "${AUTHENTIK_DB_USER?[authentik-worker] Database user missing}"
      # Authentik config
      AUTHENTIK_SECRET_KEY: "${AUTHENTIK_SECRET_KEY:?[authentik-worker] Authentik secret key missing}"
    networks:
      - home_pi
    restart: unless-stopped
    user: "${PUID_ROOT:?User ID missing}:${PGID_ROOT:?Group ID missing}"
    volumes:
      # Persistent volumes
      - ./storage/authentik/media/:/media
      - ./storage/authentik/certs/:/certs
      - ./storage/authentik/templates/:/templates

  # Cache for Authentik
  authentik-cache:
    image: redis:7.4.1-alpine
    container_name: authentik-cache
    hostname: authentik-cache
    command: --save 60 1 --loglevel warning
    deploy:
      placement:
        constraints: [ node.labels.role == manager ]
    healthcheck:
      interval: 30s
      retries: 3
      start_period: 30s
      test: ["CMD", "redis-cli", "ping"]
      timeout: 5s
    networks:
      - home_pi
    restart: unless-stopped
    user: "${PUID_ROOT:?User ID missing}:${PGID_ROOT:?Group ID missing}"
    volumes:
      # Persistent volumes
      - ./storage/authentik-cache/data/:/data

  # Database for Authentik
  authentik-db:
    image: postgres:17.0-alpine
    container_name: authentik-db
    hostname: authentik-db
    deploy:
      placement:
        constraints: [ node.labels.role == manager ]
    environment:
      # Base config
      TIMEZONE: "${TIMEZONE:?Timezone missing}"
      # Database config
      PGPORT: "${AUTHENTIK_DB_PORT:?[authentik-db] Database port missing}"
      POSTGRES_DB: "${AUTHENTIK_DB_NAME:?[authentik-db] Database name missing}"
      POSTGRES_PASSWORD: "${AUTHENTIK_DB_PASSWORD:?[authentik-db] Database password missing}"
      POSTGRES_USER: "${AUTHENTIK_DB_USER?[authentik-db] Database user missing}"
      # Next 3 variables needed to avoid "FATAL role 'root' does not exist" error
      PGDATABASE: "${AUTHENTIK_DB_NAME:?[authentik-db] Database name missing}"
      PGPASSWORD: "${AUTHENTIK_DB_PASSWORD:?[authentik-db] Database password missing}"
      PGUSER: "${AUTHENTIK_DB_USER?[authentik-db] Database user missing}"
    expose:
      - "${AUTHENTIK_DB_PORT:?[authentik-db] Database port missing}"
    healthcheck:
      interval: 30s
      retries: 3
      start_period: 30s
      test: [ "CMD", "pg_isready", "--host", "localhost" ]
      timeout: 5s
    networks:
      - home_pi
    restart: unless-stopped
    user: "${PUID_ROOT:?User ID missing}:${PGID_ROOT:?Group ID missing}"
    volumes:
      # Persistent volumes
      - ./storage/authentik-db/data/:/var/lib/postgresql/data

  # Monitoring and alerting
  uptime-kuma:
    image: louislam/uptime-kuma:1.23.15-alpine
    container_name: uptime-kuma
    hostname: uptime-kuma
    deploy:
      placement:
        constraints: [ node.labels.role == manager ]
    environment:
      # Base config
      TZ: "${TIMEZONE:?Timezone missing}"
      # Port config
      UPTIME_KUMA_PORT: "${UPTIME_KUMA_PORT:?[uptime-kuma] Port missing}"
    labels:
      # Uptime-Kuma monitor config
      kuma.{{container_name}}.http.name: "Uptime Kuma (Status Page)"
      kuma.{{container_name}}.http.url: "${UPTIME_KUMA_MONITOR_URL:?[uptime-kuma] Monitor URL missing}"
    networks:
      - home_pi
    ports:
      - "${UPTIME_KUMA_PORT:?[uptime-kuma] Port missing}:${UPTIME_KUMA_PORT:?[uptime-kuma] Port missing}"
    restart: unless-stopped
    user: "${PUID_ROOT:?User ID missing}:${PGID_ROOT:?Group ID missing}"
    volumes:
      # Persistent volumes
      - ./storage/uptime-kuma/:/app/data

  # WiFi/LAN network scanner
  netalert:
    build:
      context: .
      dockerfile: docker/netalert/Dockerfile
      args:
        # Docker image versions
        NETALERT_VERSION: "24.10.31" # https://registry.hub.docker.com/r/jokobsk/netalertx/tags
        # Build arguments
        # Internal config
        TIMEZONE: "${TIMEZONE:?Timezone missing}"
        # Notification config
        DISCORD_NEW_NETWORK_DEVICE_WEBHOOK_URL: "${DISCORD_NEW_NETWORK_DEVICE_WEBHOOK_URL:?[netalert] Discord webhook URL missing}"
        # Scanning config
        NETALERT_PRIMARY_SUBNET: "${NETALERT_PRIMARY_SUBNET:?[netalert] Primary subnet missing}"
        # UI config
        DOMAIN_NAME: "${DOMAIN_NAME:?[netalert] Domain name missing}"
    container_name: netalert
    hostname: netalert
    deploy:
      placement:
        constraints: [ node.labels.role == manager ]
    environment:
      # Base config
      TZ: "${TIMEZONE:?Timezone missing}"
      # UI config
      PORT: "${NETALERT_UI_PORT:?[netalert] Port missing}"
    labels:
      # Uptime-Kuma monitor config
      kuma.{{container_name}}.http.name: "NetAlert (Network Scanner)"
      kuma.{{container_name}}.http.url: "${NETALERT_MONITOR_URL:?[netalert] Monitor URL missing}"
    network_mode: host # Cannot be run in bridged mode, as it needs visibility of the network
    expose: # When using 'host' network_mode, published ports using 'ports' are discarded. We just 'expose' the new port in case we change the default
      - "${NETALERT_UI_PORT:?[netalert] Port missing}"
    restart: unless-stopped
    user: "${PUID_ROOT:?User ID missing}:${PGID_ROOT:?Group ID missing}"
    volumes:
      # Persistent volumes
      - ./storage/netalert/config/:/app/config
      - ./storage/netalert/db/:/app/db

  speedtest:
    image: linuxserver/speedtest-tracker:0.21.4
    container_name: speedtest
    hostname: speedtest
    depends_on:
      speedtest-db:
        condition: service_healthy
    deploy:
      placement:
        constraints: [ node.labels.role == manager ]
    environment:
      # Base config
      APP_TIMEZONE: "${TIMEZONE:?Timezone missing}"
      PGID: "${PGID_NON_ROOT:?Group ID missing}"
      PUID: "${PUID_NON_ROOT:?User ID missing}"
      # Reporting config
      APP_KEY: "${SPEEDTEST_APP_KEY:?[speedtest] App key missing}"
      APP_URL: "${SPEEDTEST_APP_URL:?[speedtest] App URL missing}"
      PRUNE_RESULTS_OLDER_THAN: "30" # Days
      SPEEDTEST_SCHEDULE: "${SPEEDTEST_SCHEDULE?:[speedtest] Cron schedule missing}"
      SPEEDTEST_SERVERS: "${SPEEDTEST_SERVERS}"
      # Database config
      DB_CONNECTION: "pgsql"
      DB_HOST: "speedtest-db"
      DB_DATABASE: "${SPEEDTEST_DB_NAME:?[speedtest] Database name missing}"
      DB_PASSWORD: "${SPEEDTEST_DB_PASSWORD:?[speedtest] Database password missing}"
      DB_PORT: "${SPEEDTEST_DB_PORT:?[speedtest] Database port missing}"
      DB_USERNAME: "${SPEEDTEST_DB_USER:?[speedtest] Database user missing}"
    healthcheck:
      interval: 30s
      retries: 3
      start_period: 30s
      test: [ "CMD", "wget", "--no-verbose", "--tries=1", "-O", "-", "http://127.0.0.1:80/api/speedtest/latest" ]
      timeout: 5s
    labels:
      traefik.enable: "true"
      # Main router for the service
      traefik.http.routers.speedtest.rule: "Host(`speedtest.${PUBLIC_DOMAIN_NAME}`)"
      traefik.http.routers.speedtest.entrypoints: "websecure"
      traefik.http.routers.speedtest.tls: "true"
      # Uptime-Kuma monitor config
      kuma.{{container_name}}.http.name: "SpeedTest Tracker (Network Speed Test)"
      kuma.{{container_name}}.http.url: "${SPEEDTEST_MONITOR_URL:?[speedtest] Monitor URL missing}"
    networks:
      - home_pi
    ports:
      - "5004:80"
    restart: unless-stopped
    volumes:
      - ./storage/speedtest/config/:/config

  speedtest-db:
    image: postgres:17.0-alpine
    container_name: speedtest-db
    hostname: speedtest-db
    deploy:
      placement:
        constraints: [ node.labels.role == manager ]
    environment:
      # Base config
      TIMEZONE: "${TIMEZONE:?Timezone missing}"
      # Database config
      PGPORT: "${SPEEDTEST_DB_PORT:?[speedtest-db] Database port missing}"
      POSTGRES_DB: "${SPEEDTEST_DB_NAME:?[speedtest-db] Database name missing}"
      POSTGRES_PASSWORD: "${SPEEDTEST_DB_PASSWORD:?[speedtest-db] Database password missing}"
      POSTGRES_USER: "${SPEEDTEST_DB_USER:?[speedtest-db] Database user missing}"
      # Next 3 variables needed to avoid "FATAL role 'root' does not exist" error
      PGDATABASE: "${SPEEDTEST_DB_NAME:?[speedtest-db] Database name missing}"
      PGPASSWORD: "${SPEEDTEST_DB_PASSWORD:?[speedtest-db] Database password missing}"
      PGUSER: "${SPEEDTEST_DB_USER:?[speedtest-db] Database user missing}"
    expose:
      - 5432
    healthcheck:
      interval: 30s
      retries: 3
      start_period: 30s
      test: [ "CMD", "pg_isready", "--host", "localhost" ]
      timeout: 5s
    networks:
      - home_pi
    restart: unless-stopped
    user: "${PUID_ROOT:?User ID missing}:${PUID_ROOT:?Group ID missing}"
    volumes:
      - ./storage/speedtest-db/data/:/var/lib/postgresql/data
