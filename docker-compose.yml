networks:
  home:
    name: home
    driver: bridge

services:

  # Homepage
  homarr:
    image: ghcr.io/ajnart/homarr:0.15.3 # https://github.com/ajnart/homarr/pkgs/container/homarr
    container_name: homarr
    hostname: homarr
    depends_on:
      qbittorrent:
        condition: service_healthy
      lidarr:
        condition: service_healthy
      radarr:
        condition: service_healthy
      readarr:
        condition: service_healthy
      sonarr:
        condition: service_healthy
      tandoor-ui:
        condition: service_healthy
    deploy:
      resources:
        limits:
          cpus: "1"
          memory: "1024M"
    environment:
      # Base config
      PGID: "${PGID:?Group ID missing}"
      PUID: "${PUID:?User ID missing}"
      TZ: "${TIMEZONE:?Timezone missing}"
    labels:
      # Uptime-Kuma monitor config
      kuma.{{container_name}}.http.name: "Homarr (HomePage)"
      kuma.{{container_name}}.http.url: "${HOMARR_MONITOR_URL:?[homarr] Monitor URL missing}"
    networks:
      - home
    ports:
      - "4100:7575"
    restart: unless-stopped
    volumes:
      # Docker socket mount from host system
      - "${DOCKER_SOCKET}:/var/run/docker.sock"
      # Persistent volumes
      - ./storage/homarr/configs:/app/data/configs
      - ./storage/homarr/data:/data
      - ./storage/homarr/icons:/app/public/icons

  # Movie manager
  radarr:
    image: linuxserver/radarr:nightly-version-5.7.0.8851
    container_name: radarr
    hostname: radarr
    depends_on:
      qbittorrent:
        condition: service_healthy
    deploy:
      resources:
        limits:
          cpus: "1"
          memory: "1024M"
    environment:
      # Base config
      PGID: "${PGID:?Group ID missing}"
      PUID: "${PUID:?User ID missing}"
      TZ: "${TIMEZONE:?Timezone missing}"
    healthcheck:
      interval: 30s
      retries: 3
      start_period: 30s
      test: "curl --silent --fail http://localhost:3000/ping | grep -q 'OK' || exit 1"
      timeout: 5s
    labels:
      # Uptime-Kuma monitor config
      kuma.{{container_name}}.http.name: "Radarr (Movie Manager)"
      kuma.{{container_name}}.http.url: "${RADARR_MONITOR_URL:?[radarr] Monitor URL missing}"
    networks:
      - home
    ports:
      - "4000:3000"
    restart: unless-stopped
    volumes:
      # Volume mounts from host system
      - "${DOWNLOADS_DIRECTORY}:/downloads"
      - "${MOVIE_DIRECTORY}:/movies"
      # Persistent volumes
      - ./storage/radarr/:/config

  # TV show manager
  sonarr:
    image: linuxserver/sonarr:develop-version-4.0.4.1695
    container_name: sonarr
    hostname: sonarr
    depends_on:
      qbittorrent:
        condition: service_healthy
    deploy:
      resources:
        limits:
          cpus: "2"
          memory: "2048M"
    environment:
      # Base config
      PGID: "${PGID:?Group ID missing}"
      PUID: "${PUID:?User ID missing}"
      TZ: "${TIMEZONE:?Timezone missing}"
    healthcheck:
      interval: 30s
      retries: 3
      start_period: 30s
      test: "curl --silent --fail http://localhost:3001/ping | grep -q 'OK' || exit 1"
      timeout: 5s
    labels:
      # Uptime-Kuma monitor config
      kuma.{{container_name}}.http.name: "Sonarr (TV Manager)"
      kuma.{{container_name}}.http.url: "${SONARR_MONITOR_URL:?[sonarr] Monitor URL missing}"
    networks:
      - home
    ports:
      - "4001:3001"
    restart: unless-stopped
    volumes:
      # Volume mounts from host system
      - "${DOWNLOADS_DIRECTORY}:/downloads"
      - "${TV1_DIRECTORY}:/tv1"
      - "${TV2_DIRECTORY}:/tv2"
      - "${TV_TRASH_DIRECTORY}:/trash"
      - "${TV_KIDS_DIRECTORY}:/kids"
      # Persistent volumes
      - ./storage/sonarr/:/config

  # Quality profile manager for radarr/sonarr
  recyclarr:
    image: ghcr.io/recyclarr/recyclarr:6.0.2 # https://github.com/recyclarr/recyclarr/pkgs/container/recyclarr
    container_name: recyclarr
    hostname: recyclarr
    depends_on:
      radarr:
        condition: service_healthy
      sonarr:
        condition: service_healthy
    deploy:
      resources:
        limits:
          cpus: "1"
          memory: "1024M"
    environment:
      # Base config
      PUID: "${PUID:?User ID missing}"
      PGID: "${PGID:?Group ID missing}"
      TZ: "${TIMEZONE:?Timezone missing}"
      # Scheduling config
      CRON_SCHEDULE: "@daily"
      # Radarr Config
      RADARR_API_KEY: "${RADARR_API_KEY}"
      RADARR_URL: "http://radarr:3000"
      # Sonarr Config
      SONARR_API_KEY: "${SONARR_API_KEY}"
      SONARR_URL: "http://sonarr:3001"
    healthcheck:
      interval: 30s
      retries: 3
      start_period: 30s
      test: "recyclarr -v || exit 1"
      timeout: 5s
    networks:
      - home
    restart: unless-stopped
    volumes:
      - ./storage/recyclarr:/config
      - ./docker/recyclarr/config/recyclarr.yml:/config/recyclarr.yml

  # Music manager
  lidarr:
    image: linuxserver/lidarr:nightly-version-2.4.0.4211
    container_name: lidarr
    hostname: lidarr
    depends_on:
      qbittorrent:
        condition: service_healthy
    deploy:
      resources:
        limits:
          cpus: "1"
          memory: "1024M"
    environment:
      # Base config
      PGID: "${PGID:?Group ID missing}"
      PUID: "${PUID:?User ID missing}"
      TZ: "${TIMEZONE:?Timezone missing}"
    healthcheck:
      interval: 30s
      retries: 3
      start_period: 30s
      test: "curl --silent --fail http://localhost:3002/ping | grep -q 'OK' || exit 1"
      timeout: 5s
    labels:
      # Uptime-Kuma monitor config
      kuma.{{container_name}}.http.name: "Lidarr (Music Manager)"
      kuma.{{container_name}}.http.url: "${LIDARR_MONITOR_URL:?[lidarr] Monitor URL missing}"
    networks:
      - home
    ports:
      - "4002:3002"
    restart: unless-stopped
    volumes:
      # Volume mounts from host system
      - "${DOWNLOADS_DIRECTORY}:/downloads"
      - "${MUSIC_DIRECTORY}:/music"
      # Persistent volumes
      - ./storage/lidarr/:/config

  # Music Player
  navidrome:
    image: deluan/navidrome:0.52.5
    container_name: navidrome
    hostname: navidrome
    deploy:
      resources:
        limits:
          cpus: "1"
          memory: "1024M"
    environment:
      # Base config
      PGID: "${PGID:?Group ID missing}"
      PUID: "${PUID:?User ID missing}"
      TZ: "${TIMEZONE:?Timezone missing}"
      # Auth config
      ND_REVERSEPROXYUSERHEADER: "X-Authentik-Username"
      ND_REVERSEPROXYWHITELIST: "0.0.0.0/0"
      # External services config
      ND_LASTFM_APIKEY: "${LASTFM_API_KEY:?[navidrome] LastFM API key missing}"
      ND_LASTFM_SECRET: "${LASTFM_SECRET:?[navidrome] LastFM secret missing}"
      ND_SPOTIFY_ID: "${SPOTIFY_CLIENT_ID:?[navidrome] Spotify Client ID missing}"
      ND_SPOTIFY_SECRET: "${SPOTIFY_CLIENT_SECRET:?[navidrome] Spotify Client secret missing}"
      # Scheduling config
      ND_SCANSCHEDULE: "@every 24h"
      # Metadata config
      ND_ARTISTARTPRIORITY: "artist.*, external"
      ND_COVERARTPRIORITY: "album.*, embedded, external"
      # Logging config
      ND_LOGLEVEL: "info"
    labels:
      # Uptime-Kuma monitor config
      kuma.{{container_name}}.http.name: "Navidrome (Music Player)"
      kuma.{{container_name}}.http.url: "${NAVIDROME_MONITOR_URL:?[navidrome] Monitor URL missing}"
    networks:
      - home
    ports:
      - "4003:4533"
    restart: unless-stopped
    volumes:
      # Volume mounts from host system
      - "${MUSIC_DIRECTORY}:/music:ro"
      # Persistent volumes
      - ./storage/navidrome/:/data

  # eBook manager
  readarr:
    image: linuxserver/readarr:develop-version-0.3.26.2526
    container_name: readarr
    hostname: readarr
    depends_on:
      qbittorrent:
        condition: service_healthy
    deploy:
      resources:
        limits:
          cpus: "1"
          memory: "512M"
    environment:
      # Base config
      PGID: "${PGID:?Group ID missing}"
      PUID: "${PUID:?User ID missing}"
      TZ: "${TIMEZONE:?Timezone missing}"
    healthcheck:
      interval: 30s
      retries: 3
      start_period: 30s
      test: "curl --silent --fail http://localhost:3004/ping | grep -q 'OK' || exit 1"
      timeout: 5s
    labels:
      # Uptime-Kuma monitor config
      kuma.{{container_name}}.http.name: "Readarr (eBook Manager)"
      kuma.{{container_name}}.http.url: "${READARR_MONITOR_URL:?[readarr] Monitor URL missing}"
    networks:
      - home
    ports:
      - "4004:3004"
    restart: unless-stopped
    volumes:
      # Volume mounts from host system
      - "${BOOKS_DIRECTORY}:/books"
      - "${DOWNLOADS_DIRECTORY}:/downloads"
      # Persistent volumes
      - ./storage/readarr/:/config

  # eBook management
  kavita:
    image: jvmilazz0/kavita:nightly-0.8.1.5
    container_name: kavita
    hostname: kavita
    deploy:
      resources:
        limits:
          cpus: "0.5"
          memory: "2048M"
    environment:
      # Base config
      PGID: "${PGID:?Group ID missing}"
      PUID: "${PUID:?User ID missing}"
      TZ: "${TIMEZONE:?Timezone missing}"
    labels:
      # Uptime-Kuma monitor config
      kuma.{{container_name}}.http.name: "Kavita (eBook Reader)"
      kuma.{{container_name}}.http.url: "${KAVITA_MONITOR_URL:?[kavita] Monitor URL missing}"
    networks:
      - home
    ports:
      - "4005:5000"
    restart: unless-stopped
    volumes:
      # Volume mounts from host system
      - "${BOOKS_DIRECTORY}:/books"
      # Persistent volumes
      - ./storage/kavita/:/kavita/config

  # Downloaded archive extracter
  unpackerr:
    image: golift/unpackerr:0.13.1
    container_name: unpackerr
    hostname: unpackerr
    depends_on:
      lidarr:
        condition: service_healthy
      radarr:
        condition: service_healthy
      readarr:
        condition: service_healthy
      sonarr:
        condition: service_healthy
    deploy:
      resources:
        limits:
          cpus: "2"
          memory: "512M"
    environment:
      # Base config
      PGID: "${PGID:?Group ID missing}"
      PUID: "${PUID:?User ID missing}"
      TZ: "${TIMEZONE:?Timezone missing}"
      # Lidarr config
      UN_LIDARR_0_API_KEY: "${LIDARR_API_KEY:?[unpackerr] Lidarr API key missing}"
      UN_LIDARR_0_PATHS_0: "/downloads/complete/lidarr"
      UN_LIDARR_0_PROTOCOLS: "torrent,usenet"
      UN_LIDARR_0_URL: "http://lidarr:3002"
      # Radarr config
      UN_RADARR_0_API_KEY: "${RADARR_API_KEY:?[unpackerr] Radarr API key missing}"
      UN_RADARR_0_PATHS_0: "/downloads/complete/radarr"
      UN_RADARR_0_PROTOCOLS: "torrent,usenet"
      UN_RADARR_0_URL: "http://radarr:3000"
      # Readarr config
      UN_READARR_0_API_KEY: "${READARR_API_KEY:?[unpackerr] Readarr API key missing}"
      UN_READARR_0_PATHS_0: "/downloads/complete/readarr"
      UN_READARR_0_PROTOCOLS: "torrent,usenet"
      UN_READARR_0_URL: "http://readarr:3004"
      # Sonarr config
      UN_SONARR_0_API_KEY: "${SONARR_API_KEY:?[unpackerr] Sonarr API key missing}"
      UN_SONARR_0_PATHS_0: "/downloads/complete/sonarr"
      UN_SONARR_0_PROTOCOLS: "torrent,usenet"
      UN_SONARR_0_URL: "http://sonarr:3001"
    healthcheck:
      interval: 30s
      retries: 3
      start_period: 30s
      test: [ "CMD", "/unpackerr", "-v" ]
      timeout: 5s
    networks:
      - home
    restart: unless-stopped
    security_opt:
      - no-new-privileges:true
    user: "${PUID:?User ID missing}:${PGID:?Group ID missing}"
    volumes:
      # Volume mounts from host system
      - "${DOWNLOADS_DIRECTORY}:/downloads"
      # Persistent volumes
      - ./storage/unpackerr/:/config

  # Subtitle downloader
  bazarr:
    build:
      context: .
      dockerfile: docker/bazarr/Dockerfile
      args:
        # Docker image versions
        BAZARR_VERSION: "development-version-v1.4.3-beta.38" # https://hub.docker.com/r/linuxserver/bazarr/tags
    container_name: bazarr
    hostname: bazarr
    deploy:
      resources:
        limits:
          cpus: "1"
          memory: "1024M"
    depends_on:
      radarr:
        condition: service_healthy
      sonarr:
        condition: service_healthy
      whisper:
        condition: service_healthy
    environment:
      # Base config
      PGID: "${PGID:?Group ID missing}"
      PUID: "${PUID:?User ID missing}"
      TZ: "${TIMEZONE:?Timezone missing}"
    healthcheck:
      interval: 30s
      retries: 3
      start_period: 30s
      test: "curl --silent --insecure --fail --output /dev/null 'http://localhost:3009/api/system/status?apikey=${BAZARR_API_KEY}' || exit 1"
      timeout: 5s
    labels:
      # Uptime-Kuma monitor config
      kuma.{{container_name}}.http.name: "Bazarr (Subtitle Manager)"
      kuma.{{container_name}}.http.url: "${BAZARR_MONITOR_URL:?[bazarr] Monitor URL missing}"
    networks:
      - home
    ports:
      - "4009:3009"
    restart: unless-stopped
    volumes:
      # Volume mounts from host system
      - "${MOVIE_DIRECTORY}:/movies"
      - "${TV1_DIRECTORY}:/tv1"
      - "${TV2_DIRECTORY}:/tv2"
      - "${TV_TRASH_DIRECTORY}:/trash"
      - "${TV_KIDS_DIRECTORY}:/kids"
      # Persistent volumes
      - ./storage/bazarr/:/config

  # Automated Speech Recognition tool for subtitles
  whisper:
    image: onerahmet/openai-whisper-asr-webservice:v1.4.1
    container_name: whisper
    hostname: whisper
    deploy:
      resources:
        limits:
          cpus: "1"
          memory: "2048M"
    environment:
      # Model config
      ASR_MODEL: "small.en"
      ASR_ENGINE: "faster_whisper"
    healthcheck:
      interval: 30s
      retries: 3
      start_period: 30s
      test: "curl --silent --fail http://localhost:9000/ || exit 1"
      timeout: 5s
    networks:
      - home
    restart: unless-stopped

  # Video Game ROM Manager
  romm:
    image: rommapp/romm:3.2.0
    container_name: romm
    hostname: romm
    depends_on:
      romm-db:
        condition: service_healthy
    deploy:
      resources:
        limits:
          cpus: "1"
          memory: "1024M"
    environment:
      # Authentication config
      DISABLE_CSRF_PROTECTION: "true"
      ROMM_AUTH_SECRET_KEY: "${ROMM_SECRET_KEY:?[romm] Authentication secret key missing}"
      ROMM_AUTH_PASSWORD: "${ROMM_ADMIN_PASSWORD:?[romm] Admin password missing}"
      ROMM_AUTH_USERNAME: "${ADMIN_USERNAME:?[romm] Admin username missing}"
      # Database config
      ROMM_DB_DRIVER: "mariadb"
      DB_HOST: "romm-db"
      DB_NAME: "${ROMM_DB_NAME:?[romm] Database name missing}"
      DB_PORT: "${ROMM_DB_PORT:?[romm] Database port missing}"
      # Do not quote the following two, or leave a comment afterward, can cause issues, for some reason (I think the quotes are passed in)
      DB_PASSWD: ${ROMM_DB_PASSWORD:?[romm] Database password missing}
      DB_ROOT_PASSWD: ${ROMM_DB_ROOT_PASSWORD:?[romm] Database root password missing}
      DB_USER: "${ROMM_DB_USER:?[romm] Database user missing}"
      # External systems config
      IGDB_CLIENT_ID: "${TWITCH_CLIENT_ID:?[romm] Twitch client ID missing}"
      IGDB_CLIENT_SECRET: "${TWITCH_CLIENT_SECRET:?[romm] Twitch client secret missing}"
      MOBYGAMES_API_KEY: "${MOBYGAMES_API_KEY:?[romm] MobyGames API key missing}"
      STEAMGRIDDB_API_KEY: "${STEAMGRIDDB_API_KEY:?[romm] SteamGridDB API key missing}"
    healthcheck:
      interval: 30s
      retries: 3
      start_period: 30s
      test: "curl --silent --fail http://localhost:8080/api/heartbeat | grep -q 'VERSION' || exit 1"
      timeout: 5s
    labels:
      # Uptime-Kuma monitor config
      kuma.{{container_name}}.http.name: "RomM (Game ROM Manager)"
      kuma.{{container_name}}.http.url: "${ROMM_MONITOR_URL:?[romm] Monitor URL missing}"
    networks:
      - home
    ports:
      - "4006:8080"
    restart: unless-stopped
    volumes:
      # Volume mounts from host system
      - "${ROM_GAMES_AND_BIOS_PARENT_DIRECTORY}:/romm/library:ro"
      - "${ROM_SAVES_DIRECTORY}:/romm/assets"
      - "./docker/romm/config/:/romm/config"
      # Persistent volumes
      - ./storage/romm/resources:/romm/resources  # Resources fetched from IGDB (covers, screenshots, etc.)
      - ./storage/romm/redis:/redis-data          # Cached data for background tasks

  # Database for RomM
  romm-db:
    image: mariadb:11.3.2
    container_name: romm-db
    hostname: romm-db
    deploy:
      resources:
        limits:
          cpus: "1"
          memory: "1024M"
    environment:
      # Database config
      MYSQL_TCP_PORT: "${ROMM_DB_PORT:?[romm-db] Database port missing}"
      MYSQL_DATABASE: "${ROMM_DB_NAME:?[romm-db] Database name missing}"
      MYSQL_PASSWORD: "${ROMM_DB_PASSWORD:?[romm-db] Database password missing}"
      MYSQL_ROOT_PASSWORD: "${ROMM_DB_ROOT_PASSWORD:?[romm-db] Database root password missing}"
      MYSQL_USER: "${ROMM_DB_USER:?[romm-db] Database user missing}"
    expose:
      - "${ROMM_DB_PORT}"
    healthcheck:
      interval: 30s
      retries: 3
      start_period: 30s
      test: "healthcheck.sh --su-mysql --connect --innodb_initialized || exit 1"
      timeout: 5s
    networks:
      - home
    restart: unless-stopped
    volumes:
      - romm-db_data:/var/lib/mysql

  # Indexer (Torrents + Usenet)
  prowlarr:
    image: linuxserver/prowlarr:nightly-version-1.18.0.4522
    container_name: prowlarr
    hostname: prowlarr
    depends_on:
      flaresolverr:
        condition: service_healthy
      lidarr:
        condition: service_healthy
      radarr:
        condition: service_healthy
      readarr:
        condition: service_healthy
      sonarr:
        condition: service_healthy
    deploy:
      resources:
        limits:
          cpus: "1"
          memory: "1024M" # High memory resourcing to allow stats page to load
    environment:
      # Base config
      PGID: "${PGID:?Group ID missing}"
      PUID: "${PUID:?User ID missing}"
      TZ: "${TIMEZONE:?Timezone missing}"
    healthcheck:
      interval: 30s
      retries: 3
      start_period: 30s
      test: "curl --silent --fail http://localhost:9696/ping | grep -q 'OK' || exit 1"
      timeout: 5s
    labels:
      # Uptime-Kuma monitor config
      kuma.{{container_name}}.http.name: "Prowlarr (Torrent Indexer)"
      kuma.{{container_name}}.http.url: "${PROWLARR_MONITOR_URL:?[prowlarr] Monitor URL missing}"
    networks:
      - home
    ports:
      - "9696:9696"
    restart: unless-stopped
    volumes:
      # Persistent volumes
      - ./storage/prowlarr/:/config

  # Torrent downloader
  qbittorrent:
    build:
      context: .
      dockerfile: docker/qbittorrent/Dockerfile
      args:
        # Docker image versions
        QBITTORRENT_VERSION: "4.6.4"  # https://hub.docker.com/r/linuxserver/qbittorrent/tags
        # Build arguments
        VUETORRENT_VERSION: "2.8.1"   # https://github.com/WDaan/VueTorrent/releases/
    container_name: qbittorrent
    hostname: qbittorrent
    deploy:
      resources:
        limits:
          cpus: "1.5"
          memory: "4096M"
    environment:
      # Base config
      PGID: "${PGID:?Group ID missing}"
      PUID: "${PUID:?User ID missing}"
      TZ: "${TIMEZONE:?Timezone missing}"
      # UI config
      WEBUI_PORT: "9000"
    healthcheck:
      interval: 30s
      retries: 3
      start_period: 30s
      test: "curl --silent --fail http://localhost:9000/api/v2/app/version || exit 1"
      timeout: 5s
    labels:
      # Uptime-Kuma monitor config
      kuma.{{container_name}}.http.name: "qBittorrent (Torrent Downloader)"
      kuma.{{container_name}}.http.url: "${QBITTORRENT_MONITOR_URL:?[qbittorrent] Monitor URL missing}"
    networks:
      - home
    ports:
      - "9000-9002:9000-9002"
    restart: unless-stopped
    volumes:
      # Volume mounts from host system
      - "${DOWNLOADS_DIRECTORY}:/downloads"
      # Persistent volumes
      - ./storage/qbittorrent/:/config

  # Discord bot for requests
  doplarr:
    image: linuxserver/doplarr:3.6.3
    container_name: doplarr
    hostname: doplarr
    depends_on:
      radarr:
        condition: service_healthy
      sonarr:
        condition: service_healthy
    deploy:
      resources:
        limits:
          cpus: "0.5"
          memory: "512M"
    environment:
      # Integration configuration
      RADARR__URL: "http://radarr:3000"
      SONARR__URL: "http://sonarr:3001"
      RADARR__API: "${RADARR_API_KEY:?[doplarr] Radarr API key missing}"
      SONARR__API: "${SONARR_API_KEY:?[doplarr] Sonarr API key missing}"
      # Discord configuration
      DISCORD__MAX_RESULTS: "5"
      DISCORD__TOKEN: "${DISCORD_BOT_TOKEN:?[doplarr] Discord bot token missing}"
    healthcheck:
      interval: 30s
      retries: 3
      start_period: 30s
      test: [ "CMD", "pidof", "java" ]
      timeout: 5s
    networks:
      - home
    restart: unless-stopped
    volumes:
      # Persistent volumes
      - ./storage/doplarr/:/config

  # Cloudflare rate-limiter solver
  flaresolverr:
    image: flaresolverr/flaresolverr:v3.3.18
    container_name: flaresolverr
    hostname: flaresolverr
    deploy:
      resources:
        limits:
          cpus: "0.5"
          memory: "1024M"
    environment:
      # Base config
      TZ: "${TIMEZONE:?Timezone missing}"
      # Logging config
      LOG_LEVEL: "info"
    healthcheck:
      interval: 30s
      retries: 3
      start_period: 30s
      test: "curl --silent --fail http://localhost:8191/ | grep -q 'FlareSolverr is ready' || exit 1"
      timeout: 5s
    networks:
      - home
    restart: unless-stopped

  # Recipe manager
  tandoor:
    image: vabene1111/recipes:1.5.17
    container_name: tandoor
    hostname: tandoor
    depends_on:
      tandoor-db:
        condition: service_healthy
    deploy:
      resources:
        limits:
          cpus: "1"
          memory: "1024M"
    environment:
      # Base config
      TZ: "${TIMEZONE:?Timezone missing}"
      # Database config
      DB_ENGINE: "django.db.backends.postgresql"
      POSTGRES_HOST: "tandoor-db"
      POSTGRES_PORT: "${TANDOOR_DB_PORT:?[tandoor] Database port missing}"
      POSTGRES_DB: "${TANDOOR_DB_NAME:?[tandoor] Database name missing}"
      POSTGRES_PASSWORD: "${TANDOOR_DB_PASSWORD:?[tandoor] Database password missing}"
      POSTGRES_USER: "${TANDOOR_DB_USER:?[tandoor] Database user missing}"
      # Logging config
      DEBUG: "0"
      SQL_DEBUG: "0"
      # Secrets config
      SECRET_KEY: "${TANDOOR_SECRET_KEY:?[tandoor] Tandoor secret key missing}"
    healthcheck:
      interval: 30s
      retries: 3
      start_period: 30s
      test: "wget -q --spider http://localhost:8080/api 2>&1 | grep -q '404\\|403' || exit 1"
      timeout: 5s
    labels:
      # Uptime-Kuma monitor config
      kuma.{{container_name}}.http.name: "Tandoor (Recipes)"
      kuma.{{container_name}}.http.url: "${TANDOOR_MONITOR_URL:?[tandoor] Monitor URL missing}"
    networks:
      - home
    restart: unless-stopped
    volumes:
      - ./storage/tandoor/nginx/conf.d/:/opt/recipes/nginx/conf.d
      - ./storage/tandoor/mediafiles/:/opt/recipes/mediafiles
      - ./storage/tandoor/staticfiles/:/opt/recipes/staticfiles

  # Recipe manager reverse-proxy/UI
  tandoor-ui:
    image: nginx:1.25.5-alpine
    container_name: tandoor-ui
    hostname: tandoor-ui
    depends_on:
      tandoor:
        condition: service_healthy
    deploy:
      resources:
        limits:
          cpus: "0.5"
          memory: "1024M"
    environment:
      # Base config
      TIMEZONE: "${TIMEZONE:?Timezone missing}"
      # Secrets config
      SECRET_KEY: "${TANDOOR_SECRET_KEY:?[tandoor-ui] Tandoor secret key missing}"
    healthcheck:
      interval: 30s
      retries: 3
      start_period: 30s
      test: "curl --silent --fail http://localhost || exit 1"
      timeout: 5s
    networks:
      - home
    ports:
      - "4010:80"
    restart: unless-stopped
    volumes:
      - ./storage/tandoor/nginx/conf.d/:/etc/nginx/conf.d:ro
      - ./storage/tandoor/mediafiles/:/media:ro
      - ./storage/tandoor/staticfiles/:/static:ro

  # Database for Tandoor
  tandoor-db:
    image: postgres:15.7-alpine
    container_name: tandoor-db
    hostname: tandoor-db
    deploy:
      resources:
        limits:
          cpus: "1"
          memory: "1024M"
    environment:
      # Base config
      TIMEZONE: "${TIMEZONE:?Timezone missing}"
      # Database config
      PGPORT: "${TANDOOR_DB_PORT:?[tandoor-db] Database port missing}"
      POSTGRES_DB: "${TANDOOR_DB_NAME:?[tandoor-db] Database name missing}"
      POSTGRES_PASSWORD: "${TANDOOR_DB_PASSWORD:?[tandoor-db] Database password missing}"
      POSTGRES_USER: "${TANDOOR_DB_USER:?[tandoor-db] Database user missing}"
      # Next 3 variables needed to avoid "FATAL role 'root' does not exist" error
      PGDATABASE: "${TANDOOR_DB_NAME:?[tandoor-db] Database name missing}"
      PGPASSWORD: "${TANDOOR_DB_PASSWORD:?[tandoor-db] Database password missing}"
      PGUSER: "${TANDOOR_DB_USER:?[tandoor-db] Database user missing}"
    expose:
      - "${TANDOOR_DB_PORT}"
    healthcheck:
      interval: 30s
      retries: 3
      start_period: 30s
      test: "pg_isready -d db_prod || exit 1"
      timeout: 5s
    networks:
      - home
    restart: unless-stopped
    volumes:
      - ./storage/tandoor-db/:/var/lib/postgresql/data

  # Inventory management
  homebox:
    image: ghcr.io/hay-kot/homebox:v0.10.3 # https://github.com/hay-kot/homebox/pkgs/container/homebox
    container_name: homebox
    hostname: homebox
    deploy:
      resources:
        limits:
          cpus: "0.5"
          memory: "512M"
    environment:
      # Logging config
      HBOX_LOG_FORMAT: "text"
      HBOX_LOG_LEVEL: "info"
      # UI config
      HBOX_OPTIONS_ALLOW_REGISTRATION: "false"
      HBOX_WEB_MAX_UPLOAD_SIZE: "10"
    healthcheck:
      interval: 30s
      retries: 3
      start_period: 30s
      test: "wget -q --no-verbose --tries=1 --output-document - http://localhost:7745/api/v1/status | grep -q '\"health\":true' || exit 1"
      timeout: 5s
    labels:
      # Uptime-Kuma monitor config
      kuma.{{container_name}}.http.name: "Homebox (House Inventory)"
      kuma.{{container_name}}.http.url: "${HOMEBOX_MONITOR_URL:?[homebox] Monitor URL missing}"
    networks:
      - home
    ports:
      - "4011:7745"
    restart: unless-stopped
    volumes:
      - ./storage/homebox/:/data/

  # Daily journal
  dailytxt:
    image: phitux/dailytxt:1.0.14
    container_name: dailytxt
    hostname: dailytxt
    deploy:
      resources:
        limits:
          cpus: "0.5"
          memory: "256M"
    environment:
      # Base config
      USER_GID: "${PGID:?Group ID missing}"
      USER_UID: "${PUID:?User ID missing}"
      TZ: "${TIMEZONE:?Timezone missing}"
      # Secrets config
      SECRET_KEY: "${DAILYTXT_SECRET_KEY:?[dailytxt] Secret key missing}"
      # UI config
      ALLOW_REGISTRATION: "false"
      DATA_INDENT: "2"
      ENABLE_UPDATE_CHECK: "false"
      JWT_EXP_DAYS: "60"
      PORT: "8765"
    healthcheck:
      interval: 30s
      retries: 3
      start_period: 30s
      test: "wget -q --no-verbose --tries=1 -O - http://127.0.0.1:8765 | grep -q 'DailyTxT' || exit 1"
      timeout: 5s
    labels:
      # Uptime-Kuma monitor config
      kuma.{{container_name}}.http.name: "DailyTxt (Daily Journal)"
      kuma.{{container_name}}.http.url: "${DAILYTXT_MONITOR_URL:?[dailytxt] Monitor URL missing}"
    networks:
      - home
    ports:
      - "4015:8765"
    restart: unless-stopped
    volumes:
      - ./storage/dailytxt/:/app/data/

  # Docker log viewer
  dozzle:
    image: amir20/dozzle:v7.0.4
    container_name: dozzle
    hostname: dozzle
    deploy:
      resources:
        limits:
          cpus: "0.5"
          memory: "256M"
    environment:
      # Hosts config
      DOZZLE_HOSTNAME: "${MAIN_HOST_NAME:?[dozzle] Hostname missing}"
      DOZZLE_REMOTE_HOST: "${REMOTE_HOSTS}"
    healthcheck:
      interval: 30s
      retries: 3
      start_period: 30s
      test: [ "CMD", "/dozzle", "healthcheck" ]
      timeout: 5s
    labels:
      # Uptime-Kuma monitor config
      kuma.{{container_name}}.http.name: "Dozzle (Docker Log Viewer)"
      kuma.{{container_name}}.http.url: "${DOZZLE_MONITOR_URL:?[dozzle] Monitor URL missing}"
    networks:
      - home
    ports:
      - "4014:8080"
    restart: unless-stopped
    volumes:
      # Docker socket mount from host system
      - "${DOCKER_SOCKET}:/var/run/docker.sock"

  # Documents file browser
  filebrowser:
    image: filebrowser/filebrowser:v2.30.0
    container_name: filebrowser
    hostname: filebrowser
    deploy:
      resources:
        limits:
          cpus: "0.25"
          memory: "256M"
    environment:
      FB_NOAUTH: noauth
    labels:
      # Uptime-Kuma monitor config
      kuma.{{container_name}}.http.name: "FileBrowser (Document Viewer)"
      kuma.{{container_name}}.http.url: "${FILEBROWSER_MONITOR_URL:?[filebrowser] Monitor URL missing}"
    networks:
      - home
    ports:
      - "4007:80"
    restart: unless-stopped
    security_opt:
      - no-new-privileges:true
    user: "${PUID:?User ID missing}:${PGID:?Group ID missing}"
    volumes:
      # Volume mounts from host system
      - "${DOCUMENTS_DIRECTORY}:/srv:ro"
      # Persistent volumes
      - ./storage/filebrowser/:/app/data/

  # Automatic Uptime-Kuma monitor generator
  autokuma:
    build:
      context: .
      dockerfile: docker/autokuma/Dockerfile
      args:
        # Docker image versions
        AUTOKUMA_VERSION: "0.6.0" # https://github.com/BigBoot/AutoKuma/pkgs/container/autokuma
        # Build arguments
        # Host Monitor URLs
        EMBY_MONITOR_URL: "${EMBY_MONITOR_URL:?[autokuma] Emby static monitor URL missing}"
    container_name: autokuma
    hostname: autokuma
    deploy:
      resources:
        limits:
          cpus: "0.5"
          memory: "256M"
    environment:
      AUTOKUMA__DEFAULT_SETTINGS: |-
        http.accepted_statuscodes:  [ "200" ]
        http.expiry_notification: true
        http.interval: 60
        http.max_redirects: 1
        http.max_retries: 5
        http.retry_interval: 60
        http.timeout: 30
      AUTOKUMA__KUMA__URL: "${UPTIME_KUMA_URL:?[autokuma] UptimeKuma URL missing}"
      AUTOKUMA__KUMA__USERNAME: "${ADMIN_USERNAME:?[autokuma] UptimeKuma username missing}"
      AUTOKUMA__KUMA__PASSWORD: "${UPTIME_KUMA_PASSWORD:?[autokuma] UptimeKuma password missing}"
      AUTOKUMA__ON_DELETE: "keep" # Keep monitors even if the service goes down
      AUTOKUMA__SYNC_INTERVAL: 300 # Sync every 5 minutes
      AUTOKUMA__STATIC_MONITORS: "${AUTO_KUMA_STATIC_MONITOR_DIRECTORY:?[autokuma] Static monitor directory missing}"
      AUTOKUMA__TAG_NAME: "${AUTO_KUMA_TAG_NAME:?[autokuma] Tag name missing}"
      AUTOKUMA__TAG_COLOR: "${AUTO_KUMA_TAG_COLOUR:?[autokuma] Tag colour missing}"
      # Status page envs
      STATUS_PAGE_SLUG: "main"
    healthcheck:
      interval: 30s
      retries: 3
      start_period: 30s
      test: [ "CMD", "pidof", "autokuma" ]
      timeout: 5s
    networks:
      - home
    restart: unless-stopped
    volumes:
      # Docker socket mount from host system
      - "${DOCKER_SOCKET}:/var/run/docker.sock"

  # Automatic Uptime-Kuma monitor generator
  # This is a near-duplicate of the main one above, but connects to the RaspberryPi Docker host.
  # It also uses the published docker image, rather than building one for any static monitors.
  autokuma-pi:
    image: ghcr.io/bigboot/autokuma:0.6.0 # https://github.com/BigBoot/AutoKuma/pkgs/container/autokuma
    container_name: autokuma-pi
    hostname: autokuma-pi
    environment:
      AUTOKUMA__DEFAULT_SETTINGS: |-
        http.accepted_statuscodes:  [ "200" ]
        http.tags: [{"tag_id": 3 }]
        http.expiry_notification: true
        http.interval: 60
        http.max_redirects: 1
        http.max_retries: 5
        http.retry_interval: 60
        http.timeout: 30
      AUTOKUMA__KUMA__URL: "${UPTIME_KUMA_URL:?[autokuma] UptimeKuma URL missing}"
      AUTOKUMA__KUMA__USERNAME: "${ADMIN_USERNAME:?[autokuma] UptimeKuma username missing}"
      AUTOKUMA__KUMA__PASSWORD: "${UPTIME_KUMA_PASSWORD:?[autokuma] UptimeKuma password missing}"
      AUTOKUMA__ON_DELETE: "keep" # Keep monitors even if the service goes down
      AUTOKUMA__SYNC_INTERVAL: 600 # Sync every 10 minutes
      AUTOKUMA__TAG_NAME: "${AUTO_KUMA_TAG_NAME:?[autokuma] Tag name missing}"
      AUTOKUMA__TAG_COLOR: "${AUTO_KUMA_TAG_COLOUR:?[autokuma] Tag colour missing}"
      DOCKER_HOST: "${AUTO_KUMA_REMOTE_HOST}"
    healthcheck:
      interval: 30s
      retries: 3
      start_period: 30s
      test: [ "CMD", "pidof", "autokuma" ]
      timeout: 5s
    networks:
      - home
    restart: unless-stopped
    volumes:
      # Docker socket mount from host system
      - "${DOCKER_SOCKET}:/var/run/docker.sock"

volumes:
  # Volume required for romm-db, due to issues when using a Windows mount
  romm-db_data:
    name: romm-db_data
